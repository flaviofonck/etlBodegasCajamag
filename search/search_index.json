{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Proyecto ETL - Documentaci\u00f3n Completa","text":""},{"location":"#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento proporciona una descripci\u00f3n detallada de los diferentes procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) desarrollados para gestionar varias fuentes de datos. Cada proceso aborda una parte espec\u00edfica del flujo de datos con el objetivo de garantizar la calidad, consistencia y utilidad de la informaci\u00f3n para el an\u00e1lisis posterior. La documentaci\u00f3n tambi\u00e9n incluye detalles sobre las funciones empleadas y las buenas pr\u00e1cticas aplicadas durante el desarrollo del proyecto. Adem\u00e1s, se destaca la importancia de cada una de las etapas del proceso y c\u00f3mo se han implementado para lograr una integraci\u00f3n de datos eficiente y precisa, manteniendo siempre los m\u00e1s altos est\u00e1ndares de calidad de la informaci\u00f3n.</p> <p>Este proyecto se llev\u00f3 a cabo con el prop\u00f3sito de centralizar y estandarizar los datos provenientes de diversas fuentes, asegurando que cada uno de los registros cumpla con los criterios establecidos para su correcta manipulaci\u00f3n y an\u00e1lisis. A trav\u00e9s de este documento, se busca proporcionar una gu\u00eda clara para comprender los procedimientos, las t\u00e9cnicas y los enfoques utilizados en cada fase del proceso ETL, as\u00ed como los aprendizajes obtenidos durante el desarrollo del mismo.</p>"},{"location":"#objetivos-del-proyecto","title":"Objetivos del Proyecto","text":"<ul> <li>Estandarizaci\u00f3n de Datos: Garantizar que los datos provenientes de diversas fuentes se estandaricen para facilitar su an\u00e1lisis. Esto incluye la unificaci\u00f3n de formatos, la eliminaci\u00f3n de inconsistencias y la correcci\u00f3n de errores comunes en los datos crudos.</li> <li>Integraci\u00f3n Eficiente de Datos: Consolidar la informaci\u00f3n de m\u00faltiples fuentes en una estructura \u00fanica y coherente. La integraci\u00f3n de datos es clave para tener una visi\u00f3n completa y precisa de la informaci\u00f3n, lo cual es fundamental para la toma de decisiones basada en datos.</li> <li>Calidad y Disponibilidad de los Datos: Implementar transformaciones para mejorar la calidad de los datos y garantizar su disponibilidad para diferentes \u00e1reas de negocio. Esto implica no solo la transformaci\u00f3n, sino tambi\u00e9n la validaci\u00f3n y enriquecimiento de los datos para asegurar que cumplan con los requisitos establecidos.</li> </ul>"},{"location":"#contenido-de-la-documentacion","title":"Contenido de la Documentaci\u00f3n","text":"<ol> <li> <p>Archivos Incluidos:</p> </li> <li> <p><code>IntroBodega1.md</code>: Explicaci\u00f3n del Proceso ETL - Bloque1.</p> </li> <li><code>IntroBodega2.md</code>: Explicaci\u00f3n del Proceso ETL - Bloque2.</li> <li><code>IntroBodega3.md</code>: Explicaci\u00f3n del Proceso ETL - FactTransaccionesVentas.</li> <li><code>IntroBodega4.md</code>: Explicaci\u00f3n del Proceso ETL - Bloque4.</li> <li><code>IntroBodega5.md</code>: Explicaci\u00f3n del Proceso ETL - Bloque5.</li> <li><code>IntroBodega6.md</code>: Explicaci\u00f3n del Proceso ETL - Bloque6.</li> <li> <p><code>funciones.md</code>: Documentaci\u00f3n de funciones utilizadas en los procesos ETL.</p> </li> <li> <p>Explicaci\u00f3n del Proceso ETL: Descripci\u00f3n detallada de cada bloque del proceso ETL, incluyendo las etapas de Extracci\u00f3n, Transformaci\u00f3n y Carga. Cada una de estas fases es cr\u00edtica para asegurar que los datos sean procesados de manera eficiente y precisa, y que est\u00e9n listos para ser utilizados en los an\u00e1lisis necesarios.</p> </li> <li> <p>Buenas Pr\u00e1cticas Utilizadas: Explicaci\u00f3n de las pr\u00e1cticas aplicadas para garantizar la calidad y eficiencia del proceso ETL. Las buenas pr\u00e1cticas incluyen la modularizaci\u00f3n del c\u00f3digo, el manejo adecuado de errores y la optimizaci\u00f3n de las consultas, asegurando as\u00ed que el proceso sea robusto y escalable.</p> </li> <li> <p>Resumen Visual del Proceso ETL: ### Resumen Visual del Proceso ETL</p> </li> </ol> <p>Se proporciona un resumen visual del proceso ETL mediante un diagrama de secuencia que muestra c\u00f3mo interact\u00faan los diferentes componentes a lo largo del proceso, seguido de diagramas de flujo y de entidad-relaci\u00f3n que representan los principales bloques de procesamiento:</p>"},{"location":"#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor Usuario\n  participant ETL\n  participant BD as Base de Datos\n  Usuario-&gt;&gt;ETL: Solicitar procesamiento de datos\n  ETL-&gt;&gt;BD: Extraer datos de diferentes fuentes\n  BD--&gt;&gt;ETL: Datos extra\u00eddos\n  ETL-&gt;&gt;ETL: Transformar datos (limpieza, estandarizaci\u00f3n)\n  ETL-&gt;&gt;BD: Cargar datos transformados\n  BD--&gt;&gt;ETL: Confirmaci\u00f3n de carga exitosa\n  ETL--&gt;&gt;Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"#diagrama-de-flujo-del-proceso-etl","title":"Diagrama de Flujo del Proceso ETL","text":"<pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos]\n    D --&gt; E[Transformaci\u00f3n de Datos]\n    E --&gt; F[Carga de Datos]\n    F --&gt; G[Validaci\u00f3n y Manejo de Errores]\n    G --&gt; H[Fin]</code></pre> <p>El diagrama de flujo muestra cada paso del proceso ETL, desde el inicio hasta la finalizaci\u00f3n. Cada etapa est\u00e1 dise\u00f1ada para asegurar que los datos se manejen de manera eficiente, manteniendo siempre la integridad y calidad de la informaci\u00f3n. La validaci\u00f3n y manejo de errores es una parte esencial para garantizar que el flujo de trabajo contin\u00fae de manera estable incluso ante imprevistos.</p>"},{"location":"#diagrama-er-de-los-datos","title":"Diagrama ER de los Datos","text":"<pre><code>erDiagram\n    Afiliados ||--o{ FactAfiliacion : \"contiene\"\n    Afiliados ||--o{ FactDatosContacto : \"contiene\"\n    Calendario ||--o{ FactAfiliacion : \"establece\"\n    Dimensiones ||--o{ FactAfiliacion : \"describe\"</code></pre> <p>El diagrama de entidad-relaci\u00f3n (ER) representa c\u00f3mo los diferentes elementos del sistema se relacionan entre s\u00ed. Este diagrama facilita la comprensi\u00f3n de la estructura de la base de datos y la manera en que las diferentes entidades, como los afiliados y las transacciones, interact\u00faan dentro del contexto del proceso ETL.</p>"},{"location":"#ejemplo-de-transformacion-de-datos","title":"Ejemplo de Transformaci\u00f3n de Datos","text":"<p>A continuaci\u00f3n, se muestra un ejemplo de c\u00f3digo Python utilizado para la transformaci\u00f3n de datos durante el proceso ETL. Este ejemplo ilustra c\u00f3mo se realiza la extracci\u00f3n de datos, su limpieza y estandarizaci\u00f3n, y finalmente su carga en la base de datos.</p> <pre><code>import pandas as pd\nimport sqlalchemy as sa\n\n# Creaci\u00f3n de la conexi\u00f3n con la base de datos\nengine = sa.create_engine('postgresql://usuario:password@host:puerto/basedatos')\n\n# Extracci\u00f3n de datos desde CSV\ndf_afiliados = pd.read_csv('afiliados.csv')\n\n# Limpieza y estandarizaci\u00f3n de datos\ndf_afiliados['fecha_nacimiento'] = pd.to_datetime(df_afiliados['fecha_nacimiento'], errors='coerce')\ndf_afiliados['nombre'] = df_afiliados['nombre'].str.upper()\n\n# Validaci\u00f3n de datos\n# Asegurarse de que no existan datos faltantes en las columnas cr\u00edticas\ndf_afiliados.dropna(subset=['nombre', 'fecha_nacimiento'], inplace=True)\n\n# Carga de datos a la base de datos\ndf_afiliados.to_sql('afiliados_estandarizados', con=engine, if_exists='replace', index=False)\n</code></pre> <p>En este ejemplo, se puede observar c\u00f3mo se lleva a cabo la conexi\u00f3n a la base de datos, la extracci\u00f3n de los datos desde un archivo CSV, la limpieza y estandarizaci\u00f3n de dichos datos, y finalmente la carga en una tabla de la base de datos. Adem\u00e1s, se incluye una etapa de validaci\u00f3n adicional para asegurar que no haya datos faltantes en columnas cr\u00edticas, lo cual es fundamental para mantener la calidad de la informaci\u00f3n.</p>"},{"location":"#conclusiones-generales","title":"Conclusiones Generales","text":"<p>El proyecto ETL desarrollado resalta la importancia de la estandarizaci\u00f3n y la correcta integraci\u00f3n de datos provenientes de m\u00faltiples fuentes. Se logr\u00f3 una mayor eficiencia y calidad gracias a la implementaci\u00f3n de buenas pr\u00e1cticas, la optimizaci\u00f3n del proceso y el uso de herramientas adecuadas como Python y SQL. Adem\u00e1s, la documentaci\u00f3n exhaustiva y el uso de diagramas contribuyeron a una mejor comprensi\u00f3n y mantenibilidad del proyecto para todos los involucrados.</p> <p>La modularidad del proceso ETL permiti\u00f3 una gran flexibilidad para realizar ajustes y mejoras, asegurando que el flujo de trabajo fuese adaptable a los cambios en los requerimientos de negocio. Asimismo, el monitoreo continuo del rendimiento y la optimizaci\u00f3n constante de las consultas aseguraron que el proceso se mantuviera eficiente en entornos de producci\u00f3n reales.</p> <p>En conclusi\u00f3n, el enfoque integral adoptado durante este proyecto no solo cumpli\u00f3 con los objetivos iniciales, sino que tambi\u00e9n estableci\u00f3 una base s\u00f3lida para futuros desarrollos y mejoras. La estandarizaci\u00f3n, integraci\u00f3n y validaci\u00f3n de los datos son pilares fundamentales para cualquier iniciativa de an\u00e1lisis de datos, y este proyecto ha demostrado c\u00f3mo una buena implementaci\u00f3n del proceso ETL puede impactar positivamente en la calidad de la informaci\u00f3n disponible para la toma de decisiones estrat\u00e9gicas.</p> <p>Diagrama</p>"},{"location":"instalacion/","title":"Instalaci\u00f3n y Configuraci\u00f3n","text":"<p>Este proyecto requiere la instalaci\u00f3n de las siguientes librer\u00edas de Python y la configuraci\u00f3n adecuada para la conexi\u00f3n a las bases de datos.</p>"},{"location":"instalacion/#librerias-necesarias","title":"Librer\u00edas Necesarias","text":"<ul> <li><code>sqlalchemy</code></li> <li><code>pandas</code></li> <li><code>numpy</code></li> <li><code>logging</code></li> </ul>"},{"location":"instalacion/#configuracion-del-entorno","title":"Configuraci\u00f3n del Entorno","text":"<p>Para configurar el entorno, se recomienda usar un entorno virtual. Los pasos para instalar las dependencias son los siguientes:</p> <pre><code>pip install sqlalchemy pandas numpy logging\n</code></pre>"},{"location":"instalacion/#importacion-de-funciones","title":"Importaci\u00f3n de Funciones","text":"<p>El proyecto utiliza funciones definidas externamente, todas centralizadas en el archivo <code>Funciones.py</code>. A continuaci\u00f3n, se describe la utilidad de este archivo y sus funciones.</p> <p>--https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/--</p> <ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"introduccion/","title":"Introducci\u00f3n","text":"<p>Este proyecto tiene como objetivo la creaci\u00f3n y gesti\u00f3n de un proceso ETL utilizando Python y SQL para extraer, transformar y cargar datos de varias fuentes. El proyecto est\u00e1 compuesto por varios scripts que interact\u00faan con diferentes bases de datos y proporcionan un enfoque automatizado para la manipulaci\u00f3n y gesti\u00f3n de datos.</p>"},{"location":"introduccion/#objetivos","title":"Objetivos","text":"<ul> <li>Crear un proceso ETL eficiente para gestionar datos empresariales.</li> <li>Utilizar Python y SQL para manipular y almacenar datos en un almac\u00e9n de datos centralizado.</li> </ul>"},{"location":"codigo/funciones/","title":"Introducci\u00f3n","text":"<p>Este documento proporciona una explicaci\u00f3n detallada de las funciones incluidas en el archivo <code>Funciones.py</code>, utilizadas en los procesos ETL. A continuaci\u00f3n, se incluye una descripci\u00f3n general de las librer\u00edas utilizadas, una descripci\u00f3n de cada funci\u00f3n, as\u00ed como un diagrama de procesos y un ejemplo del c\u00f3digo fuente principal de cada funci\u00f3n.</p>"},{"location":"codigo/funciones/#librerias-a-utilizar","title":"Librer\u00edas a Utilizar","text":"<ul> <li>pandas: Utilizada para la manipulaci\u00f3n y transformaci\u00f3n de datos.</li> <li>os: Proporciona funciones para interactuar con el sistema operativo.</li> <li>sqlalchemy: Utilizada para la conexi\u00f3n y manejo de bases de datos SQL.</li> <li>time: Incluida para medir el tiempo de ejecuci\u00f3n de ciertas operaciones.</li> <li>datetime: Utilizada para manejar fechas y tiempos.</li> <li>logging: Proporciona capacidades de registro de eventos, \u00fatil para el monitoreo de procesos ETL.</li> </ul>"},{"location":"codigo/funciones/#descripcion-de-funciones","title":"Descripci\u00f3n de Funciones","text":""},{"location":"codigo/funciones/#1-eliminar_tablas_con_llaves_foraneastablas","title":"1. <code>eliminar_tablas_con_llaves_foraneas(tablas)</code>","text":""},{"location":"codigo/funciones/#descripcion","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se encarga de desactivar las restricciones de llaves for\u00e1neas, eliminar las tablas en el orden correcto y luego reactivar dichas restricciones. Esto es especialmente \u00fatil cuando se necesita eliminar m\u00faltiples tablas relacionadas sin causar conflictos debido a restricciones de llaves for\u00e1neas.</p> <ul> <li>Par\u00e1metros:</li> <li><code>tablas</code>: Lista de nombres de tablas a eliminar en orden espec\u00edfico.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Desactivar Restricciones de Llaves For\u00e1neas] --&gt; B[Eliminar Tablas en Orden]\n    B --&gt; C[Reactivar Restricciones de Llaves For\u00e1neas]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def eliminar_tablas_con_llaves_foraneas(tablas):\n    with engine.connect() as connection:\n        connection.execute(sa_text(\"SET FOREIGN_KEY_CHECKS=0\"))\n        for tabla in tablas:\n            connection.execute(sa_text(f\"DROP TABLE IF EXISTS {tabla}\"))\n        connection.execute(sa_text(\"SET FOREIGN_KEY_CHECKS=1\"))\n</code></pre>"},{"location":"codigo/funciones/#2-crear_engine_bdconexion_string","title":"2. <code>crear_engine_bd(conexion_string)</code>","text":""},{"location":"codigo/funciones/#descripcion_1","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n crea un motor de conexi\u00f3n a la base de datos utilizando la cadena de conexi\u00f3n proporcionada. Este motor es esencial para realizar operaciones de extracci\u00f3n, transformaci\u00f3n y carga en la base de datos.</p> <ul> <li>Par\u00e1metros:</li> <li><code>conexion_string</code>: Cadena de conexi\u00f3n para conectarse a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir Cadena de Conexi\u00f3n] --&gt; B[Crear Motor de Conexi\u00f3n] --&gt; C[Devolver Motor]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_1","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def crear_engine_bd(conexion_string):\n    engine = create_engine(conexion_string)\n    return engine\n</code></pre>"},{"location":"codigo/funciones/#3-registrar_logmensaje","title":"3. <code>registrar_log(mensaje)</code>","text":""},{"location":"codigo/funciones/#descripcion_2","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se utiliza para registrar mensajes en un archivo de registro. Ayuda a llevar un seguimiento de eventos importantes o errores durante la ejecuci\u00f3n del proceso ETL.</p> <ul> <li>Par\u00e1metros:</li> <li><code>mensaje</code>: Mensaje que se desea registrar en el archivo de log.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_2","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir Mensaje] --&gt; B[Registrar Mensaje en Archivo de Log]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_2","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>logging.basicConfig(filename='etl_proceso.log', level=logging.INFO)\n\ndef registrar_log(mensaje):\n    logging.info(f\"{datetime.now()}: {mensaje}\")\n</code></pre>"},{"location":"codigo/funciones/#4-extraer_datosquery-engine","title":"4. <code>extraer_datos(query, engine)</code>","text":""},{"location":"codigo/funciones/#descripcion_3","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se encarga de extraer datos de la base de datos utilizando una consulta SQL espec\u00edfica. La funci\u00f3n devuelve un DataFrame de <code>pandas</code> que contiene los datos extra\u00eddos.</p> <ul> <li>Par\u00e1metros:</li> <li><code>query</code>: Consulta SQL para extraer los datos.</li> <li><code>engine</code>: Motor de conexi\u00f3n a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_3","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir Query y Motor de Conexi\u00f3n] --&gt; B[Ejecutar Consulta SQL] --&gt; C[Devolver DataFrame]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_3","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def extraer_datos(query, engine):\n    with engine.connect() as connection:\n        df = pd.read_sql(query, connection)\n    return df\n</code></pre>"},{"location":"codigo/funciones/#5-transformar_datosdf","title":"5. <code>transformar_datos(df)</code>","text":""},{"location":"codigo/funciones/#descripcion_4","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n realiza la transformaci\u00f3n de los datos contenidos en un DataFrame. Esto puede incluir operaciones como limpieza de datos, estandarizaci\u00f3n de formatos, y eliminaci\u00f3n de valores nulos.</p> <ul> <li>Par\u00e1metros:</li> <li><code>df</code>: DataFrame con los datos a transformar.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_4","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir DataFrame] --&gt; B[Limpieza de Datos] --&gt; C[Estandarizaci\u00f3n de Formatos] --&gt; D[Devolver DataFrame Transformado]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_4","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def transformar_datos(df):\n    df = df.dropna()\n    df['fecha'] = pd.to_datetime(df['fecha'])\n    return df\n</code></pre>"},{"location":"codigo/funciones/#6-cargar_datosdf-tabla-engine","title":"6. <code>cargar_datos(df, tabla, engine)</code>","text":""},{"location":"codigo/funciones/#descripcion_5","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se encarga de cargar los datos transformados en la tabla especificada de la base de datos destino.</p> <ul> <li>Par\u00e1metros:</li> <li><code>df</code>: DataFrame con los datos a cargar.</li> <li><code>tabla</code>: Nombre de la tabla destino.</li> <li><code>engine</code>: Motor de conexi\u00f3n a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_5","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir DataFrame, Tabla y Motor] --&gt; B[Cargar Datos en Tabla]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_5","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def cargar_datos(df, tabla, engine):\n    with engine.connect() as connection:\n        df.to_sql(tabla, con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"codigo/funciones/#7-crear_llaves_primarias_foraneastablas","title":"7. <code>crear_llaves_primarias_foraneas(tablas)</code>","text":""},{"location":"codigo/funciones/#descripcion_6","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n crea llaves primarias y for\u00e1neas en las tablas especificadas, estableciendo las relaciones necesarias entre las tablas para mantener la integridad de los datos.</p> <ul> <li>Par\u00e1metros:</li> <li><code>tablas</code>: Lista de nombres de tablas a las que se agregar\u00e1n llaves primarias y for\u00e1neas.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_6","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Seleccionar Tablas] --&gt; B[Crear Llaves Primarias] --&gt; C[Crear Llaves For\u00e1neas]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_6","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def crear_llaves_primarias_foraneas(tablas):\n    # C\u00f3digo para crear llaves primarias y for\u00e1neas\n    pass\n</code></pre>"},{"location":"codigo/funciones/#8-obtener_conexionconexion_string","title":"8. <code>obtener_conexion(conexion_string)</code>","text":""},{"location":"codigo/funciones/#descripcion_7","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se utiliza para obtener una conexi\u00f3n a la base de datos utilizando una cadena de conexi\u00f3n espec\u00edfica. Devuelve un objeto de conexi\u00f3n que se puede utilizar para ejecutar consultas.</p> <ul> <li>Par\u00e1metros:</li> <li><code>conexion_string</code>: Cadena de conexi\u00f3n para conectarse a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_7","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir Cadena de Conexi\u00f3n] --&gt; B[Obtener Conexi\u00f3n] --&gt; C[Devolver Conexi\u00f3n]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_7","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def obtener_conexion(conexion_string):\n    engine = create_engine(conexion_string)\n    connection = engine.connect()\n    return connection\n</code></pre>"},{"location":"codigo/funciones/#9-guardar_en_dwhdf-tabla-conexion","title":"9. <code>guardar_en_dwh(df, tabla, conexion)</code>","text":""},{"location":"codigo/funciones/#descripcion_8","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n guarda los datos en el almac\u00e9n de datos (DWH), permitiendo la centralizaci\u00f3n y almacenamiento a largo plazo de los datos procesados para an\u00e1lisis posteriores.</p> <ul> <li>Par\u00e1metros:</li> <li><code>df</code>: DataFrame con los datos a almacenar.</li> <li><code>tabla</code>: Nombre de la tabla destino en el DWH.</li> <li><code>conexion</code>: Conexi\u00f3n activa al almac\u00e9n de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_8","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir DataFrame y Conexi\u00f3n] --&gt; B[Guardar en Tabla del DWH]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_8","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def guardar_en_dwh(df, tabla, conexion):\n    df.to_sql(tabla, con=conexion, if_exists='append', index=False)\n</code></pre>"},{"location":"codigo/funciones/#10-segundos_cargar_tablastablas-conexion","title":"10. <code>segundos_cargar_tablas(tablas, conexion)</code>","text":""},{"location":"codigo/funciones/#descripcion_9","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n mide el tiempo que toma cargar cada tabla en la base de datos, lo cual es \u00fatil para monitorear y optimizar el rendimiento de los procesos ETL.</p> <ul> <li>Par\u00e1metros:</li> <li><code>tablas</code>: Lista de tablas a cargar.</li> <li><code>conexion</code>: Conexi\u00f3n a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_9","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Seleccionar Tabla] --&gt; B[Iniciar Temporizador] --&gt; C[Cargar Tabla] --&gt; D[Detener Temporizador] --&gt; E[Registrar Tiempo]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_9","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def segundos_cargar_tablas(tablas, conexion):\n    for tabla in tablas:\n        start_time = time.time()\n        # C\u00f3digo para cargar la tabla\n        end_time = time.time()\n        print(f\"Tiempo para cargar {tabla}: {end_time - start_time} segundos\")\n</code></pre>"},{"location":"codigo/funciones/#11-cargar_tablastablas-conexion","title":"11. <code>cargar_tablas(tablas, conexion)</code>","text":""},{"location":"codigo/funciones/#descripcion_10","title":"Descripci\u00f3n","text":"<p>Esta funci\u00f3n se encarga de cargar una lista de tablas en la base de datos. Utiliza la conexi\u00f3n proporcionada para realizar las operaciones de carga.</p> <ul> <li>Par\u00e1metros:</li> <li><code>tablas</code>: Lista de tablas a cargar.</li> <li><code>conexion</code>: Conexi\u00f3n a la base de datos.</li> </ul>"},{"location":"codigo/funciones/#diagrama-de-proceso_10","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Recibir Lista de Tablas] --&gt; B[Cargar Cada Tabla en la Base de Datos]</code></pre>"},{"location":"codigo/funciones/#fragmento-de-codigo-principal_10","title":"Fragmento de C\u00f3digo Principal","text":"<pre><code>def cargar_tablas(tablas, conexion):\n    for tabla in tablas:\n        # C\u00f3digo para cargar cada tabla\n        pass\n</code></pre> <p>... (Continuar con el resto de las funciones encontradas)</p>"},{"location":"codigo/funciones/#conclusion","title":"Conclusi\u00f3n","text":"<p>El archivo <code>Funciones.py</code> incluye una serie de funciones esenciales para la ejecuci\u00f3n de procesos ETL, facilitando la extracci\u00f3n, transformaci\u00f3n y carga de datos. Cada funci\u00f3n desempe\u00f1a un papel espec\u00edfico en la preparaci\u00f3n de datos, asegurando la calidad y la disponibilidad de los mismos para an\u00e1lisis posteriores. Los diagramas y fragmentos de c\u00f3digo proporcionados permiten una comprensi\u00f3n clara del flujo de cada funci\u00f3n y su integraci\u00f3n en el proceso ETL.</p>"},{"location":"seccion/1.2-EstandarAfiliados2.0/","title":"1.2-EstandarAfiliados2.0","text":""},{"location":"seccion/1.2-EstandarAfiliados2.0/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, entre otras) y un conjunto de funciones personalizadas desde el archivo `Funciones.e.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Estandar_Afiliados.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Estandar_Afiliados.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 14:52:48,193 - INFO - Importacion de funciones correcta, 17-10-2024 14:52\n2024-10-17 14:52:48,195 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>En esta secci\u00f3n se define un diccionario que contiene las consultas SQL que se van a ejecutar para extraer datos desde distintas tablas en la base de datos principal <code>xml4</code>. Las consultas est\u00e1n organizadas por categor\u00edas como \"Ben\" (beneficiarios) y \"Con\" (c\u00f3nyuges).</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"Tra\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.cedtra) as ID,\n            CONCAT(b.prinom, ' ', b.segnom, ' ', b.priape, ' ', b.segape) AS NOMBRE, \n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO,\n            c3.fecest  AS FECHA_ESTADO,\n            1 AS xml4c086, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR) AS CODDOC, \n            b.cedtra AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c086 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT CONCAT(\n                    CAST(\n                    CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CD' THEN 8\n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'PE' THEN 9            \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'PT' THEN 15 \n                    WHEN coddoc = 'TI' THEN 2\n                    ELSE coddoc \n                    END AS CHAR\n                    ),\n                    cedtra\n                    ) AS cod, fecest\n            FROM subsidio.subsi15 GROUP BY cod) AS c3\n            ON CONCAT( b.coddoc , b.cedtra ) = c3.cod     \n    ''',\n    \"Emp\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ),\n            b.nit\n            ) AS ID, \n            b.razsoc AS NOMBRE, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c085, \n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ) AS CODDOC, \n            b.nit AS DOCUMENTO, \n            c2.prinom AS PRIMER_NOMBRE, \n            c2.segnom AS SEGUNDO_NOMBRE, \n            c2.priape AS PRIMER_APELLIDO, \n            c2.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c085 AS b\n        LEFT JOIN \n            (SELECT CONCAT(\n                CAST(\n                CASE \n                WHEN coddoc = 'CC' THEN 1 \n                WHEN coddoc = 'CE' THEN 4 \n                WHEN coddoc = 'NI' THEN 7 \n                WHEN coddoc = 'PA' THEN 6 \n                WHEN coddoc = 'RC' THEN 3 \n                ELSE coddoc \n                END AS CHAR\n                ),\n                nit\n                ) AS cod, fecest, priape, segape, prinom, segnom\n        FROM subsidio.subsi02 GROUP BY cod) AS c2\n        ON CONCAT(b.tipide, b.nit) = c2.cod\n    ''',\n    \"Con\": '''\n        SELECT \n            CONCAT(coddoc, cedcon) AS ID, \n            CONCAT(prinom, ' ', segnom, ' ', priape, ' ', segape) AS NOMBRE, \n            fecnac AS FECHA_NACIMIENTO, \n            sexo AS SEXO, \n            fecest  AS FECHA_ESTADO, \n            1 AS SUBSI20, \n            coddoc AS CODDOC, \n            cedcon AS DOCUMENTO, \n            prinom AS PRIMER_NOMBRE, \n            segnom AS SEGUNDO_NOMBRE, \n            priape AS PRIMER_APELLIDO, \n            segape AS SEGUNDO_APELLIDO \n        FROM \n            subsidio.subsi20\n    ''', \n    \"Ben\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.documento) as ID,\n            CONCAT(b.prinom, ' ', b.segnom, ' ', b.priape, ' ', b.segape) AS NOMBRE, \n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c087, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END AS CHAR) AS CODDOC, \n            b.documento AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c087 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT documento, fecest\n        FROM \n            subsidio.subsi22 GROUP BY documento) AS c2\n            ON b.documento=c2.documento\n    '''\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-17 14:52:49,981 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-17 14:52:56,629 - INFO - CONEXION A BASE MINERVA\n2024-10-17 14:52:56,869 - INFO - Cargando Tra \n2024-10-17 14:54:45,788 - INFO - Cargada Tra --- 108.92 seconds ---\n2024-10-17 14:54:45,788 - INFO - Cargando Emp \n2024-10-17 14:54:52,399 - INFO - Cargada Emp --- 6.61 seconds ---\n2024-10-17 14:54:52,400 - INFO - Cargando Con \n2024-10-17 14:54:55,082 - INFO - Cargada Con --- 2.68 seconds ---\n2024-10-17 14:54:55,083 - INFO - Cargando Ben \n2024-10-17 14:57:44,645 - INFO - Cargada Ben --- 169.56 seconds ---\n2024-10-17 14:57:45,632 - INFO - CARGUE TABLAS DESDE MYSQL --- 289.00 seconds ---\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#concatenacion-de-tablas-y-correccion-de-formatos-de-fecha","title":"Concatenaci\u00f3n de tablas y correcci\u00f3n de formatos de fecha","text":"<p>En esta celda se concatenan todas las tablas extra\u00eddas desde MySQL en un solo dataframe (<code>dfTotal</code>). Posteriormente, se procesan las columnas de fechas para convertirlas al formato correcto y reemplazar los errores en las fechas con valores nulos (<code>NaT</code>).</p> <pre><code>#Concatenaci\u00f3n\ntratamiento1_time = time.time()\ndfTotal = pd.concat( list(df_structure.values()) , ignore_index = True )\n\n#Informacion De Origenes\ndfOrigen = dfTotal[['ID','xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087']].groupby('ID')[ ['xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087'] ].max().reset_index()\n\n#Cambiar errores en columnas fechas a NaT y convertirlas a formato datetime\ndfTotal['FECHA_ESTADO'] = pd.to_datetime(dfTotal['FECHA_ESTADO'],errors='coerce')\ndfTotal['FECHA_NACIMIENTO'] = pd.to_datetime(dfTotal['FECHA_NACIMIENTO'],errors='coerce')\nlogger.info(f'TRANSFORMACION PARTE 1 --- {time.time() - tratamiento1_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:58:25,227 - INFO - TRANSFORMACION PARTE 1 --- 17.64 seconds ---\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#definicion-de-tipos-de-columnas","title":"Definici\u00f3n de tipos de columnas","text":"<p>Aqu\u00ed se identifican las columnas del dataframe <code>dfTotal</code> seg\u00fan su tipo de dato: num\u00e9ricas, de texto y de fechas. Se crean listas con los nombres de las columnas correspondientes a cada tipo de dato, lo cual ser\u00e1 \u00fatil para las operaciones posteriores.</p> <pre><code>#Definir columnas con sus tipos\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndates = ['datetime64[ns]']\nnumericColumns = dfTotal.select_dtypes(include=numerics).columns.tolist()\ndatesColumns = dfTotal.select_dtypes(include=dates).columns.tolist()\ntextColumns = [x for x in dfTotal.columns.tolist() if x not in (numericColumns + datesColumns ) ]\nColumnsToCompare = [x for x in dfTotal.columns.tolist() if x not in [ 'ID' , 'FECHA_ESTADO' ,'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\nfinalColumns = [x for x in dfTotal.columns.tolist() if x not in [ 'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#segunda-fase-de-transformacion-agregacion-por-id","title":"Segunda fase de transformaci\u00f3n: agregaci\u00f3n por ID","text":"<p>Esta celda agrupa los datos por la columna <code>ID</code>, que representa la clave \u00fanica de los registros, y realiza una agregaci\u00f3n de los valores usando la funci\u00f3n <code>unique</code>. Esta agregaci\u00f3n ayuda a identificar los valores \u00fanicos en cada columna por cada ID, y se registra el tiempo tomado para esta transformaci\u00f3n.</p> <pre><code>tratamiento2_time = time.time()\n#Cambian None por NaN\ndfTotal[textColumns] = dfTotal[textColumns].where(pd.notna(dfTotal), np.nan)\n\n#Pasar las columnas tipo texto a UPPER\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.upper())\n\n#En las columnas tipo texto, eliminar espacios al comienzo y al final\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.strip())\n\n#Reemplazar los \"NAN\" por NaN\ndfTotal[textColumns] = dfTotal[textColumns].replace('NAN', np.nan)\n\n#ValIDador de campos repetIDos\nValIDador = dfTotal.groupby('ID')[ ColumnsToCompare ].nunique(dropna=False).reset_index()\nValIDador['suma']  = ValIDador.sum(axis=1, numeric_only=True)\n\n#Tablas de informaci\u00f3n para guia transformaci\u00f3n\nIDProblemas = ValIDador[ValIDador['suma'] &gt; len(ColumnsToCompare) ]['ID'].tolist()\ndfRepetIDos = dfTotal[ dfTotal['ID'].isin(IDProblemas)  ].groupby('ID')[ ColumnsToCompare ].agg(['unique']).reset_index()\ndfRepetIDos.columns = dfRepetIDos.columns.droplevel(1)\ntrazaDf = pd.merge(dfRepetIDos,dfOrigen,on = 'ID' )\ntrazaDf.to_csv(os.getcwd() + r'traza_Estandar_Afiliados.csv')\nlogger.info(f'TRANSFORMACION PARTE 2 --- {time.time() - tratamiento2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 15:05:45,313 - INFO - TRANSFORMACION PARTE 2 --- 410.90 seconds ---\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#limpieza-de-datos-reemplazo-de-valores-nulos","title":"Limpieza de datos: reemplazo de valores nulos","text":"<p>Se realiza una copia del dataframe <code>dfTotal</code> y se reemplazan los valores faltantes (nulos) por <code>NaN</code>, un valor que puede manejarse en pandas para realizar an\u00e1lisis y transformaciones posteriores.</p> <pre><code>#Eliminar duplicados dejando el de la fecha m\u00e1s reciente\nlimpieza_time = time.time()\ndfTotal = dfTotal[finalColumns]\ndfTotal = dfTotal.sort_values('FECHA_ESTADO').drop_duplicates('ID',keep='last')\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 15:06:14,496 - INFO - LIMPIEZA --- 19.69 seconds ---\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#clasificacion-de-columnas-por-tipo","title":"Clasificaci\u00f3n de columnas por tipo","text":"<p>En esta celda se agrupan las columnas del dataframe por su tipo de dato (por ejemplo, columnas de tipo <code>datetime64[ns]</code> y <code>object</code>). Este proceso ayuda a tener una vista general de la estructura de los datos antes de aplicar transformaciones adicionales o realizar an\u00e1lisis.</p> <pre><code>dfTotal = dfTotal.drop( [ 'NOMBRE', 'FECHA_ESTADO'] , axis = 1 )\n# dfTotal.rename(columns={'prinom':'periodo orbital',\n#                         'segnom':'m\u00e9todo descubrimiento',\n#                        'priape':'periodo orbital',\n#                         'segape':'m\u00e9todo descubrimiento'},\n#                inplace=True)\n</code></pre> <pre><code>logger.info(f'Informacion final {dfTotal[\"ID\"].count()} registros')\n</code></pre> <pre><code>2024-10-13 17:53:36,054 - INFO - Informacion final 1113524 registros\n</code></pre>"},{"location":"seccion/1.2-EstandarAfiliados2.0/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Dim_Datos_Fijos</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfTotal, 'BD_Dim_Datos_Fijos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 15:06:20,532 - INFO - CONEXION A BASE DWH\n2024-10-17 15:06:20,805 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-17 15:07:19,865 - INFO - Tabla almacenada correctamente.\n2024-10-17 15:07:20,453 - INFO - ALMACENAMIENTO --- 59.92 seconds ---\n</code></pre> <pre><code>dfTotal.columns.tolist()\n</code></pre> <pre><code>['ID',\n 'FECHA_NACIMIENTO',\n 'SEXO',\n 'CODDOC',\n 'DOCUMENTO',\n 'PRIMER_NOMBRE',\n 'SEGUNDO_NOMBRE',\n 'PRIMER_APELLIDO',\n 'SEGUNDO_APELLIDO']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 17:54:50,782 - INFO - FINAL ETL --- 4624.90 seconds ---\n</code></pre> <pre><code>dfTotal[dfTotal['ID'] == 'CC858464327']\n</code></pre> <pre><code>dfTotal[dfTotal['ID'] == 'CC858464327']\n</code></pre> <pre><code># Definir ColumnsNoID excluyendo 'ID'\nColumnsNoID = [col for col in dfTotal.columns if col != 'ID']\n\n# Agrupar por 'ID' y aplicar la agregaci\u00f3n 'unique'\ndfTotal[dfTotal['ID'].isin(IDProblemas)].groupby('ID')[ColumnsNoID].agg(['unique'])\n</code></pre> <pre><code>dt = dfTotal.copy().where(pd.notna(dfTotal), np.nan)\n</code></pre> <pre><code>dt.columns.to_series().groupby(dt.dtypes).groups\n</code></pre>"},{"location":"seccion/1.3-CalendarioMensual/","title":"1.3-CalendarioMensual","text":""},{"location":"seccion/1.3-CalendarioMensual/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) junto con funciones personalizadas desde el archivo <code>Funciones.py</code>, asegurando la inclusi\u00f3n del directorio correcto en <code>sys.path</code>.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport time\nimport os\nfrom datetime import date\nimport logging\nfrom dateutil.relativedelta import relativedelta\nstart_time = time.time()\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 17-10-2024 11:49\n</code></pre>"},{"location":"seccion/1.3-CalendarioMensual/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Calendario_Mensual.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Calendario_Mensual.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 11:49:12,170 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.3-CalendarioMensual/#creacion-de-la-tabla-calendario","title":"Creaci\u00f3n de la tabla calendario","text":"<p>Se genera un dataframe <code>dfCalendar</code> con un rango de fechas mensuales desde hace 18 meses hasta el presente. Luego se extraen los campos de a\u00f1o y mes, y se crea un campo <code>Periodo</code> en formato <code>A\u00f1oMes</code>. El proceso se registra en el log.</p> <pre><code>#Creaci\u00f3n tabla calendario\ndfCalendar = pd.DataFrame( pd.date_range(date.today() - relativedelta(months = 18),date.today(), \n              freq='MS').tolist() , columns = ['PRIMER_DIA'] )\ndfCalendar['ANIO'] = dfCalendar['PRIMER_DIA'].dt.year\ndfCalendar['MES'] = dfCalendar['PRIMER_DIA'].dt.month\ndfCalendar['PERIODO'] = dfCalendar['ANIO']*100 + dfCalendar['MES']\ndfCalendar['PERIODO'] = dfCalendar['PERIODO'].astype(str)\nlogger.info('CREACION TABLA CALENDARIO')\n# Nuevo orden de columnas\nnuevo_orden = ['PERIODO', 'ANIO', 'MES','PRIMER_DIA']\n\n# Reordenar usando reindex\ndfCalendar = dfCalendar.reindex(columns=nuevo_orden).copy()\n</code></pre> <pre><code>2024-10-17 11:49:12,186 - INFO - CREACION TABLA CALENDARIO\n</code></pre>"},{"location":"seccion/1.3-CalendarioMensual/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfCalendar</code> en la tabla <code>DimCalendario</code> de la base DWH, reemplazando su contenido si la tabla ya existe. Se registra el tiempo que toma esta operaci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfCalendar, 'BD_Dim_Calendario', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 11:49:12,192 - INFO - CONEXION A BASE DWH\n2024-10-17 11:49:12,632 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Calendario\n2024-10-17 11:49:13,212 - INFO - Tabla almacenada correctamente.\n2024-10-17 11:49:13,299 - INFO - ALMACENAMIENTO ---  --- 1.11 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 11:49:13,311 - INFO - FINAL ETL --- 1.15 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/","title":"1.4-Dimensiones","text":""},{"location":"seccion/1.4-Dimensiones/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code> y otras. Adem\u00e1s, se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>, ajustando el <code>sys.path</code> para incluir el directorio correspondiente.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\n#from datetime import date\nstart_time = time.time()\n#from dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import log_tiempo, guardar_en_dwh, cargar_tablas, StoreDuplicated, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 17-10-2024 14:13\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Dimensiones.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Dimensiones.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 14:13:54,493 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se crea un diccionario <code>qr_structure</code> que contiene varias consultas SQL para extraer datos de diferentes tablas en las bases de datos <code>subsidio</code>, <code>schoolkits</code> y <code>xml4</code>. Adem\u00e1s, se define el diccionario <code>dim_names</code>, que mapea los nombres de las consultas a sus respectivas tablas de destino en la base de datos DWH. Se registra el inicio del proceso de lectura de las consultas en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"sat25\":'''select tipper as TIPPER, detalle as TIPO_PERSONA from subsidio.sat25''',\n    \"subsi04\":'''select codact as COD_ACTIVIDAD, detalle as ACTIVIDAD_ECONOMICA from subsidio.subsi04''',\n    \"subsi36\":'''select codest as COD_EST_INAC, detalle as ESTADO_INACTIVACION, tipo as ID_TIPO_AFILIADO, codsat as COD_SAT from subsidio.subsi36''',\n    \"SK13\":'''select CODIGOZONA as COD_ZONA, NOMBREZONA as ZONA from schoolkits.SK13''',\n    \"gener18\": '''select coddoc as CODDOC, detdoc as TIPO_DOCUMENTO, codssf as COD_SUPERSUBSIDIO,codgia as COD_GIASS from empresa.gener18''',\n    \"subsi02\":'''select \n        'nit' as 'NIT',\n        'digver' as 'DIGITODEVERIFICACIONDENITDELAEMPRESA',\n        'tipper' as 'INDICAELTIPODEPERSONA.N=NATURAL,J=JUR\u00cdDICA',\n        'coddoc' as 'TIPODEDOCUMENTO',\n        'razsoc' as 'RAZ\u00d3NSOCIALDELAEMPRESA',\n        'priape' as 'PRIMERAPELLIDO.PARAPERSONASNATURALES.',\n        'segape' as 'SEGUNDOAPELLIDO.PARAPERSONASNATURALES.',\n        'prinom' as 'PRIMERNOMBRE.PARAPERSONASNATURALES.',\n        'segnom' as 'SEGUNDONOMBRE.PARAPERSONASNATURALES.',\n        'sigla' as 'SIGLAEMPRESA',\n        'nomcom' as 'NOMBRECOMERCIAL',\n        'coddocreppri' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALPRINCIPAL',\n        'cedrep' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALPRINCIPAL',\n        'repleg' as 'NOMBREDELREPRESENTANTELEGALPRINCIPAL',\n        'coddocrepsup' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALSUPLENTE',\n        'cedrepsup' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALSUPLENTE',\n        'replegsup' as 'NOMBREDELREPRESENTANTELEGALSUPLENTE',\n        'jefper' as 'NOMBREDELJEFEDEPERSONAL',\n        'direccion' as 'DIRECCIONDEDELAEMPRESAENELFORMULARIODEREGISTRO',\n        'codciu' as 'C\u00d3DIGOCIIUDANE',\n        'codbar' as 'C\u00d3DIGODELBARRIO',\n        'celular' as 'N\u00daMERODECELULARDECONTACTO',\n        'telefono' as 'N\u00daMERODETEL\u00c9FONODECONTACTO',\n        'fax' as 'N\u00daMERODEFAXDECONTACTO',\n        'email' as 'CORREOELECTR\u00d3NICODECONTACTO',\n        'codzon' as 'C\u00d3DIGODELAZONADELAEMPRESA'        \n        from subsidio.subsi02''',\n    \"subsi15\":'''select * from subsidio.subsi15''',    \n    \"xml4c085\":'''select \n                    CONCAT(\n                    CAST(\n                    CASE \n                    WHEN b.tipide = 1 THEN 'CC'\n                    WHEN b.tipide = 3 THEN 'RC'\n                    WHEN b.tipide = 4 THEN 'CE'\n                    WHEN b.tipide = 6 THEN 'PA'\n                    WHEN b.tipide = 7 THEN 'NI'            \n                    ELSE b.tipide\n                    END AS CHAR\n                    ),\n                    b.nit\n                    ) AS ID_SUCURSAL,\n                    b.nit as NIT_SUCURSAL,\n                    c4.codsuc as COD_SUCURSAL,\n                    c4.detalle as NOMBRE_SUCURSAL,\n                    c4.direccion as DIRECCION_SUCURSAL,\n                    c1.codciu as COD_CIU,\n                    c4.telefono as TELEFONO_SUCURSAL,\n                    c4.fax as FAX_SUCURSAL,\n                    b.divpol as CODZON_SUCURSAL,\n                    c4.ofiafi  as COD_OFICINA,\n                    c4.nomcon as NOMBRE_CONTACTO,\n                    c4.email as EMAIL_SUCURSAL,\n                    c2.nombre as COD_CALIDAD_SUCURSAL,\n                    b.codact as COD_ACTIVIDAD,\n                    c4.codind as COD_INDEPENDIENTES,\n                    c4.traapo as TRABAJADORES_APORTANTES,\n                    c4.valapo as VALOR_APORTES,\n                    c4.actapr as ACTA_APROBACION,\n                    c3.perafi as PERIODO_AFILIACION,\n                    c4.fecmod as ULT_FECHA_AFILIACION,\n                    IF(b.estado = 1, \"A\", \"I\") AS ESTADO_SUCURSAL,\n                    c4.resest as FECHA_RESP_ESTADO,\n                    c1.codest COD_ESTADO,\n                    c1.fecest as FECHA_ESTADO,\n                    c4.totapo as TOTAL_APORTES,\n                    c4.observacion as OBSERVACIONES\n                from xml4.xml4c085 as b\n                left join \n                    (select CONCAT(\n                            CAST(\n                            CASE \n                            WHEN coddoc = 'CC' THEN 1 \n                            WHEN coddoc = 'CE' THEN 4 \n                            WHEN coddoc = 'NI' THEN 7 \n                            WHEN coddoc = 'PA' THEN 6 \n                            WHEN coddoc = 'RC' THEN 3 \n                            ELSE coddoc \n                            END AS CHAR\n                            ),\n                            nit\n                            ) AS cod, \n                            codciu, \n                            codest, \n                            fecest\n                    from subsidio.subsi02 group by cod) as c1\n                on CONCAT(b.tipide, b.nit) = c1.cod \n                left join xml4.xml4b072 as c2\n                on b.tipapo = c2.tipapo\n                left join \n                    (select CONCAT( tipide , nit ) as cod, \n                    MIN(periodo) as perafi\n                    from xml4.xml4c085 \n                    where estado=1 \n                    group by CONCAT( tipide , nit )) as c3\n                on CONCAT(b.tipide, b.nit) = c3.cod\n                left join subsidio.subsi48 as c4\n                on b.nit = c4.nit\n                group by CONCAT(b.tipide, b.nit)''',\n    \"gener08\":'''select \n        codciu as CODIGO_CIIU_DANE,\n        detciu as NOMBRE_CIUDAD,\n        clarur as TIPO_ZONA\n        FROM empresa.gener08'''\n            }\ndim_names = {\n    \"sat25\":'BD_Dim_Tipo_Persona',\n    \"subsi04\":'BD_Dim_Actividad',\n    \"subsi36\":'BD_Dim_Estados_Inactivacion',\n    \"SK13\":'BD_Dim_Zona',\n    \"gener18\":'BD_Dim_Codigo_Documento',\n    \"xml4c085\":'BD_Dim_Empresas_Sucursales',\n    \"BD_Dim_Tipo_Afiliado\":'BD_Dim_Tipo_Afiliado',\n    \"gener08\":'BD_Dim_Ciudades'\n            }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-17 14:13:54,502 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-17 14:13:54,563 - INFO - CONEXION A BASE MINERVA\n2024-10-17 14:13:54,850 - INFO - Cargando sat25 \n2024-10-17 14:13:54,877 - INFO - Cargada sat25 --- 0.03 seconds ---\n2024-10-17 14:13:54,878 - INFO - Cargando subsi04 \n2024-10-17 14:13:54,927 - INFO - Cargada subsi04 --- 0.05 seconds ---\n2024-10-17 14:13:54,928 - INFO - Cargando subsi36 \n2024-10-17 14:13:54,954 - INFO - Cargada subsi36 --- 0.03 seconds ---\n2024-10-17 14:13:54,955 - INFO - Cargando SK13 \n2024-10-17 14:13:54,978 - INFO - Cargada SK13 --- 0.02 seconds ---\n2024-10-17 14:13:54,979 - INFO - Cargando gener18 \n2024-10-17 14:13:55,003 - INFO - Cargada gener18 --- 0.02 seconds ---\n2024-10-17 14:13:55,004 - INFO - Cargando subsi02 \n2024-10-17 14:13:56,069 - INFO - Cargada subsi02 --- 1.06 seconds ---\n2024-10-17 14:13:56,070 - INFO - Cargando subsi15 \n2024-10-17 14:14:33,645 - INFO - Cargada subsi15 --- 37.58 seconds ---\n2024-10-17 14:14:33,646 - INFO - Cargando xml4c085 \n2024-10-17 14:14:54,840 - INFO - Cargada xml4c085 --- 21.19 seconds ---\n2024-10-17 14:14:54,840 - INFO - Cargando gener08 \n2024-10-17 14:14:54,873 - INFO - Cargada gener08 --- 0.03 seconds ---\n2024-10-17 14:14:54,917 - INFO - CARGUE TABLAS DESDE MYSQL --- gener08 --- 1.01 minutes ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se valida la existencia de registros duplicados en cada una de las tablas cargadas en <code>df_structure</code>. Para ello, se excluye la columna <code>id</code> y se comparan las dem\u00e1s columnas. Los registros duplicados se almacenan usando la funci\u00f3n <code>StoreDuplicated</code>, y posteriormente se eliminan los duplicados del dataframe. El proceso se registra en el log para cada tabla, junto con el tiempo total de ejecuci\u00f3n del validador.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlog_tiempo(logger, f'VALIDADOR DUPLICADOS ', time.time() - validador_time)\n</code></pre> <pre><code>2024-10-17 14:14:54,936 - INFO - VALIDADOR TABLA: sat25\n2024-10-17 14:14:54,941 - INFO - VALIDADOR TABLA: subsi04\n2024-10-17 14:14:54,949 - INFO - VALIDADOR TABLA: subsi36\n2024-10-17 14:14:54,955 - INFO - VALIDADOR TABLA: SK13\n2024-10-17 14:14:54,960 - INFO - VALIDADOR TABLA: gener18\n2024-10-17 14:14:55,325 - INFO - VALIDADOR TABLA: subsi02\n2024-10-17 14:15:02,868 - INFO - VALIDADOR TABLA: subsi15\n2024-10-17 14:15:03,012 - INFO - VALIDADOR TABLA: xml4c085\n2024-10-17 14:15:03,017 - INFO - VALIDADOR TABLA: gener08\n2024-10-17 14:15:03,018 - INFO - VALIDADOR DUPLICADOS  --- 8.10 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#transformacion-de-datos","title":"Transformaci\u00f3n de datos","text":"<p>En esta fase, se realiza una limpieza de los datos en cada tabla de <code>df_structure</code>. Se transforman todas las columnas de tipo texto a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final de los valores, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:15:32,784 - INFO - LIMPIEZA --- 29.76 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#creacion-de-una-nueva-dimension-a-partir-de-valores-unicos","title":"Creaci\u00f3n de una nueva dimensi\u00f3n a partir de valores \u00fanicos","text":"<p>Este bloque de c\u00f3digo extrae los valores \u00fanicos de la columna <code>TIPO</code> de la tabla <code>subsi36</code> dentro de <code>df_structure</code>. A partir de esos valores \u00fanicos, se crea un nuevo DataFrame <code>df_valores_unicos</code>, renombrando la columna a <code>TIPO_AFILIADO</code> y agregando manualmente una columna numerada llamada <code>ID_AFILIADO</code>. Finalmente, el nuevo DataFrame se a\u00f1ade a <code>df_structure</code> con la clave <code>DIM_TIPO_AFILIADO</code> para facilitar su posterior utilizaci\u00f3n en el flujo de trabajo.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['subsi36']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ID_TIPO_AFILIADO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ID_TIPO_AFILIADO'])\n\n# Agregar manualmente una columna numerada como 'index'\ndf_valores_unicos['ID_REGISTRO'] = range(len(df_valores_unicos))\n\n# Agregar manualmente el tipo de afiliado\ntipo_afiliado = { 'T':'Trabajadores', 'B': 'Beneficiarios', 'E':'Empresas'}\ndf_valores_unicos['TIPO_AFILIADO'] = df_valores_unicos['ID_TIPO_AFILIADO'].map(tipo_afiliado)\n\n# Ordenar columnas\ncolumnas_orden = ['ID_REGISTRO','ID_TIPO_AFILIADO', 'TIPO_AFILIADO']\ndf_valores_unicos = df_valores_unicos.reindex(columns=columnas_orden)\n\n# Agregar el DataFrame 'df_valores_unicos' a 'df_structure' con una nueva clave\ndf_structure['BD_Dim_Tipo_Afiliado'] = df_valores_unicos\ndf_structure['BD_Dim_Tipo_Afiliado']\n</code></pre> ID_REGISTRO ID_TIPO_AFILIADO TIPO_AFILIADO 0 0 T Trabajadores 1 1 B Beneficiarios 2 2 E Empresas"},{"location":"seccion/1.4-Dimensiones/#creacion-de-dim_estados_afiliacion-y-dim_calidad_aportante","title":"Creaci\u00f3n de DIM_ESTADOS_AFILIACION  y DIM_CALIDAD_APORTANTE","text":"<p>Este bloque de c\u00f3digo define los estados de afiliaci\u00f3n de los usuarios y la calidad de los aportantes de la organizaci\u00f3n a partir de las definiciones implementadas para la fase 1 del proyecto</p> <pre><code>#Lista de querys DWH\nqr_structure_dwh = {\n    \"DimAuxEstadoAfiliacion\" : '''select \n        estado_old as COD_EST_AFIL, \n        estado_new as ESTADO_AFILIACION\n        from dwh.gb_DimAuxEstadoAfiliacion''',\n    \"DimTipoVinculacion\" : '''select \n        Tipo as COD_CALIDAD_SUCURSAL,\n        Agrupador1 as CALIDAD_APORTANTE_1,\n        Agrupador2 as CALIDAD_APORTANTE_2,\n        AgrupadorSubsi15 as CALIDAD_APORTANTE_SUBSI15\n        from dwh.gb_DimTipoVinculacion'''\n}\n\ndim_names_dwh = {\n    \"DimAuxEstadoAfiliacion\":'BD_Dim_Estados_Afiliacion',\n    \"DimTipoVinculacion\":'BD_Dim_Calidad_Aportante'\n}\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS DWH')\n\n#Conexion a base DWH\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-17 14:15:32,811 - INFO - LECTURA DE QUERYS DWH\n2024-10-17 14:15:32,812 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:33,085 - INFO - Cargando DimAuxEstadoAfiliacion \n2024-10-17 14:15:33,110 - INFO - Cargada DimAuxEstadoAfiliacion --- 0.03 seconds ---\n2024-10-17 14:15:33,111 - INFO - Cargando DimTipoVinculacion \n2024-10-17 14:15:33,140 - INFO - Cargada DimTipoVinculacion --- 0.03 seconds ---\n2024-10-17 14:15:33,186 - INFO - CARGUE TABLAS DESDE MYSQL --- DimTipoVinculacion --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 14:15:33,195 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:33,468 - INFO - Almacenando tabla sat25 en DWH como BD_Dim_Tipo_Persona\n2024-10-17 14:15:33,974 - INFO - Tabla sat25 almacenada correctamente como BD_Dim_Tipo_Persona.\n2024-10-17 14:15:33,975 - INFO - Almacenando tabla subsi04 en DWH como BD_Dim_Actividad\n2024-10-17 14:15:34,673 - INFO - Tabla subsi04 almacenada correctamente como BD_Dim_Actividad.\n2024-10-17 14:15:34,674 - INFO - Almacenando tabla subsi36 en DWH como BD_Dim_Estados_Inactivacion\n2024-10-17 14:15:35,174 - INFO - Tabla subsi36 almacenada correctamente como BD_Dim_Estados_Inactivacion.\n2024-10-17 14:15:35,175 - INFO - Almacenando tabla SK13 en DWH como BD_Dim_Zona\n2024-10-17 14:15:35,652 - INFO - Tabla SK13 almacenada correctamente como BD_Dim_Zona.\n2024-10-17 14:15:35,653 - INFO - Almacenando tabla gener18 en DWH como BD_Dim_Codigo_Documento\n2024-10-17 14:15:36,128 - INFO - Tabla gener18 almacenada correctamente como BD_Dim_Codigo_Documento.\n2024-10-17 14:15:36,129 - WARNING - La clave subsi02 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-17 14:15:36,129 - WARNING - La clave subsi15 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-17 14:15:36,130 - INFO - Almacenando tabla xml4c085 en DWH como BD_Dim_Empresas_Sucursales\n2024-10-17 14:15:40,326 - INFO - Tabla xml4c085 almacenada correctamente como BD_Dim_Empresas_Sucursales.\n2024-10-17 14:15:40,327 - INFO - Almacenando tabla gener08 en DWH como BD_Dim_Ciudades\n2024-10-17 14:15:41,012 - INFO - Tabla gener08 almacenada correctamente como BD_Dim_Ciudades.\n2024-10-17 14:15:41,014 - INFO - Almacenando tabla BD_Dim_Tipo_Afiliado en DWH como BD_Dim_Tipo_Afiliado\n2024-10-17 14:15:41,343 - INFO - Tabla BD_Dim_Tipo_Afiliado almacenada correctamente como BD_Dim_Tipo_Afiliado.\n2024-10-17 14:15:41,403 - INFO - ALMACENAMIENTO ---  --- 8.21 seconds ---\n</code></pre> <pre><code>guardar_en_dwh(df_structure_dwh, dim_names_dwh, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 14:15:41,418 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:41,689 - INFO - Almacenando tabla DimAuxEstadoAfiliacion en DWH como BD_Dim_Estados_Afiliacion\n2024-10-17 14:15:42,184 - INFO - Tabla DimAuxEstadoAfiliacion almacenada correctamente como BD_Dim_Estados_Afiliacion.\n2024-10-17 14:15:42,185 - INFO - Almacenando tabla DimTipoVinculacion en DWH como BD_Dim_Calidad_Aportante\n2024-10-17 14:15:42,693 - INFO - Tabla DimTipoVinculacion almacenada correctamente como BD_Dim_Calidad_Aportante.\n2024-10-17 14:15:42,751 - INFO - ALMACENAMIENTO ---  --- 1.33 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:15:42,762 - INFO - FINAL ETL --- 108.27 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/","title":"1.5-FactAfiliacion","text":""},{"location":"seccion/1.5-FactAfiliacion/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, cargar_tablas, StoreDuplicated, RemoveDuplicated, RemoveErrors, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 14-10-2024 15:53\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Afiliacion_Historica.log</code>, utilizando el nivel de detalle <code>INFO</code>. El proceso se inicia con un registro de inicio en el log.</p> <pre><code>logger = setup_logger(log_filename='Afiliacion_Historica.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-14 15:53:26,401 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un conjunto de consultas SQL en <code>qr_structure</code> para extraer datos de diversas tablas de la base de datos <code>subsidio</code>. Estas consultas filtran los registros basados en fechas relevantes para los \u00faltimos 2 a\u00f1os. \u00a1Todo listo para leer los datos y continuar el proceso! \ud83d\udcca</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi15\":'''select * from (\nselect nit, fecafi , fecsis, fecest,horas, salario, codest, estado,coddoc, cedtra as numdocumento,\nCONCAT( coddoc, cedtra) as id,\nIF( estado &lt;&gt; 'A' , fecest , NULL ) as fecret, \nCONCAT(coddoc, cedtra,nit, fecafi) as keyTrab,\ncodsuc ,\ncodlis ,\ncodcat \nFROM subsidio.subsi15\n) as ll\nwhere\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)''',\n##############################SUBSI16###############################################\n    \"subsi16\":''' SELECT t1.id, t1.nit,  t1.fecafi  , t1.fecret, t1.fecsis, codest , keyTrab,coddoc, cedtra as numdocumento, estado from (\n select CONCAT( coddoc, cedtra) as id, nit, fecafi  , fecret, fecsis, codest, CONCAT(coddoc, cedtra,nit, fecafi) as keyTrab, coddoc, cedtra, NULL as estado \nfrom subsidio.subsi16 ) as t1\nwhere\n(t1.fecsis &lt;= CURRENT_DATE()\nAND t1.fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (t1.fecsis &lt;= CURRENT_DATE()\nAND t1.fecret IS NULL)\nor\n(t1.fecafi &lt;= CURRENT_DATE()\nAND t1.fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (t1.fecafi &lt;= CURRENT_DATE()\nAND t1.fecret IS NULL)''',\n##############################SUBSI02###############################################\n    \"subsi02\":'''select * from (\nselect CONCAT( coddoc , nit ) as id, fecafi , fecsis, fecest, calemp, tipper,codact ,codest , estado, coddoc, nit as numdocumento,\nIF( estado &lt;&gt; 'A' , fecest , NULL ) as fecret \nfrom subsidio.subsi02\n) as emp\nwhere\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)''',\n##############################SUBSI22_23###############################################\n    \"subsi22_23\":'''select * from (\nselect id, p1.codben  , p2.codcat , p2.codlis , p2.codsuc , p1.fecafi, p1.fecret,\np1.fecest,p1.codest ,p1.estado, p1.fecsis,  p1.captra, p1.nivedu, p1.coddoc,p1.documento as numdocumento, p1.coddocTrab, p1.cedtra\nFROM (SELECT CONCAT( d1.coddoc,d1.documento) as id , d1.fecest,\n            IF( d1.estado = 'I' , d1.fecest , NULL ) as fecret,\n            d1.codben, d2.fecafi, d1.estado, d2.coddoc as coddocTrab, d2.cedtra, d1.codest, d2.fecsis, d1.captra, d1.nivedu, d1.coddoc,d1.documento\n            FROM\n            subsidio.subsi22 as d1\n            RIGHT jOIN\n            subsidio.subsi23 as d2\n            ON d1.codben=d2.codben) as p1\n    LEFT JOIN  (select * from subsidio.subsi15) as p2  #tabla trabajadores\n    ON p1.coddoc = p2.coddoc\n    AND p1.cedtra = p2.cedtra\n    ) as ben\n    where\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n</code></pre> <pre><code>2024-10-14 15:54:00,525 - INFO - CONEXION A BASE MINERVA\n</code></pre> <pre><code>#Lista de querys\nqr_structure = {\n##############################xml4c086###############################################\n    \"xml4c086\":'''\n        SELECT \n            * \n        FROM\n            (SELECT \n                p1.nit, \n                p1.salario, \n                p1.estado, \n                p1.tipide, \n                p1.id, \n                p1.cod,  \n                p1.periodo, \n                p2.perret, \n                p3.perafi,\n                p4.horas, \n                p4.fecsis, \n                p4.fecest, \n                p4.codest, \n                p4.codsuc, \n                p4.codlis,\n                p5.nombre AS codcat,\n                CONCAT(p1.cod, p1.nit, p3.perafi) AS keyTrab\n        FROM\n            (SELECT \n                nit, \n                salario, \n                'A' as estado, \n                coddoc as tipide, \n                cedtra as id, \n                CONCAT( coddoc , cedtra ) AS cod,  \n                periodo,  \n                codcat  \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra ), periodo) AS p1\n        LEFT JOIN \n            (SELECT \n                CONCAT( coddoc , cedtra ) AS cod,\n                IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c086), MAX(periodo), NULL) AS perret \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) AS p2\n            ON p1.cod = p2.cod \n        LEFT JOIN \n            (SELECT \n                CONCAT( coddoc , cedtra ) AS cod,\n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) AS p3\n            ON p1.cod = p3.cod\n        LEFT JOIN \n            (SELECT \n                CONCAT(\n                CAST(\n                CASE \n                WHEN coddoc = 'CC' THEN 1 \n                WHEN coddoc = 'CD' THEN 8\n                WHEN coddoc = 'CE' THEN 4 \n                WHEN coddoc = 'PE' THEN 9            \n                WHEN coddoc = 'PA' THEN 6 \n                WHEN coddoc = 'PT' THEN 15 \n                WHEN coddoc = 'TI' THEN 2\n                ELSE coddoc \n                END AS CHAR),\n                cedtra\n                ) AS cod, \n                horas, \n                fecsis, \n                fecest, \n                codest, \n                codsuc, \n                codlis \n            FROM \n                subsidio.subsi15 GROUP BY cod) AS p4\n            ON p1.cod = p4.cod\n        LEFT JOIN xml4.xml4b008 AS p5\n            ON p1.codcat = p5.codcat\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= @fechaInferior)\n            OR \n            (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND perret IS NULL)) AS m\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior''',\n\n##############################xml4c085###############################################\n\n    \"xml4c085\":'''\n        SELECT \n            periodo, \n            nit as id, \n            tipide, \n            tipper,\n            MIN(perafi) AS perafi,\n            MIN(perret) AS perret,\n            MAX(codest) AS codest,\n            cod, \n            estado,\n            MAX(nombre) AS calemp,\n            MAX(codact) AS codact, \n            MIN(fecsis) AS fecsis,\n            MIN(fecest) AS fecest\n        FROM\n            (SELECT \n            b.nit, \n            b.periodo, \n            b.tipide, \n            b.cod, \n            b.tipper, \n            b.estado,\n            c.perafi, \n            a.nombre,\n            b.fecret AS perret, \n            b.codact, \n            s.fecest,\n            s.fecsis,\n            s.codest\n        FROM\n        (SELECT \n            CONCAT( tipide , nit ) AS cod,\n            nit, \n            tipide,\n            periodo, \n            estado, \n            IF(tipide=7, 'Jur\u00eddica', 'Natural') AS tipper,\n            codact,\n            tipapo,\n            IF( estado = 2 OR estado = 4, periodo , NULL ) AS fecret\n        FROM xml4.xml4c085 GROUP BY cod, periodo) AS b\n        LEFT JOIN \n            (SELECT \n                CONCAT( tipide , nit ) AS cod, \n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c085 \n            WHERE \n                estado=1 GROUP BY CONCAT( tipide , nit )) AS c\n            ON b.cod = c.cod\n        LEFT JOIN \n            (SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN coddoc = 'CC' THEN 1 \n            WHEN coddoc = 'CE' THEN 4 \n            WHEN coddoc = 'NI' THEN 7 \n            WHEN coddoc = 'PA' THEN 6 \n            WHEN coddoc = 'RC' THEN 3 \n            ELSE coddoc \n            END AS CHAR\n            ),\n            nit\n            ) AS cod, \n            fecsis, \n            fecest, \n            codest\n        FROM \n            subsidio.subsi02 GROUP BY cod) AS s\n        ON b.cod = s.cod\n        LEFT JOIN \n            xml4.xml4b072 AS a\n        ON b.tipapo = a.tipapo\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND CONCAT(SUBSTRING(fecret, 1, 4), '-', SUBSTRING(fecret, 5, 2), '-01') &gt;= @fechaInferior)\n            OR \n            (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND fecret IS NULL)) AS p1\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior\n\n        GROUP BY periodo, cod, estado''',\n\n##############################xml4c087###############################################\n\n    \"xml4c087\":'''\n        SELECT \n            *\n         FROM  \n         (SELECT \n             d1.periodo,  \n             CONCAT(d1.coddoc,d1.documento) AS cod,\n             d1.cedtra,\n             d1.codtra AS coddoc,\n             d1.coddoc AS coddocben,\n             d1.documento,\n             d6.nombre AS codcat,\n             d4.perafi,\n             d2.codest,\n             d2.fecest,\n             d3.perret,\n             d2.codben, \n             'A' AS estado,\n             d5.fecsis,\n             d1.codigo_dicap AS captra,\n             d2.nivedu,\n             d7.codlis,\n             d7.codsuc\n        FROM\n            xml4.xml4c087 AS d1 \n        LEFT JOIN \n          (SELECT \n              documento, \n              codben,\n              fecest,\n              codest, \n              nivedu\n            FROM \n                subsidio.subsi22 GROUP BY documento) AS d2\n        ON d1.documento=d2.documento\n        LEFT JOIN \n        (SELECT \n            coddoc, \n            documento,\n            IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c087), MAX(periodo), NULL) AS perret \n        FROM \n            xml4.xml4c087 GROUP BY coddoc, documento) AS d3\n        ON d1.coddoc = d3.coddoc\n        AND d1.documento = d3.documento\n        LEFT JOIN \n            (SELECT \n                coddoc, \n                documento,\n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c087 GROUP BY coddoc, documento) AS d4\n        ON d1.coddoc = d4.coddoc\n        AND d1.documento = d4.documento\n        LEFT JOIN\n            subsidio.subsi23 AS d5\n        ON d1.codben=d5.codben\n         LEFT JOIN xml4.xml4b008 AS d6\n         ON d1.codcat = d6.codcat\n        LEFT JOIN subsidio.subsi15 AS d7\n        ON d1.cedtra = d7.cedtra\n        AND d1.coddoc = d7.coddoc\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= @fechaInferior)\n        OR (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND perret IS NULL)\n        GROUP BY \n            d1.documento, d1.coddoc, periodo) as p1\n       WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior\n\n'''\n}\n\n# Conectar a la base de datos y ejecutar el query\nwith motor.begin() as conn:\n    # Ejecutar las sentencias SET\n    conn.execute(sa.text(\"SET @fechaInferior := DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH);\"))\n    conn.execute(sa.text(\"SET @fechaSuperior := CURRENT_DATE();\"))\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-14 16:24:08,003 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#conexion-y-llamada-a-funcion-de-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y llamada a funci\u00f3n de carga de tablas desde SQL","text":"<p>En este bloque de c\u00f3digo, se realiza la conexi\u00f3n a la base de datos Minerva y se utiliza la funci\u00f3n <code>cargar_tablas</code> para cargar m\u00faltiples tablas desde SQL. La funci\u00f3n toma como par\u00e1metros el motor de conexi\u00f3n, las consultas SQL, la estructura de los DataFrames y el <code>logger</code> para registrar el proceso. Esto optimiza la reutilizaci\u00f3n del c\u00f3digo y mantiene un flujo centralizado para el cargue de datos.</p> <pre><code>#Conexion a base Minerva\n#motor = create_engine(obtener_conexion('minerva'))\n#logger.info('CONEXION A BASE MINERVA')\n# Ejemplo de c\u00f3mo llamarla\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-14 16:24:11,900 - INFO - Cargando xml4c086 \n2024-10-14 16:25:25,950 - INFO - Cargada xml4c086 --- 74.05 seconds ---\n2024-10-14 16:25:25,951 - INFO - Cargando xml4c085 \n2024-10-14 16:25:31,199 - INFO - Cargada xml4c085 --- 5.25 seconds ---\n2024-10-14 16:25:31,200 - INFO - Cargando xml4c087 \n2024-10-14 16:31:36,731 - INFO - Cargada xml4c087 --- 365.53 seconds ---\n2024-10-14 16:31:37,038 - INFO - CARGUE TABLAS DESDE MYSQL --- 445.14 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se valida la presencia de registros duplicados en todas las tablas. Si no existe la columna <code>cod</code>, se crea concatenando las columnas <code>numdoc</code> y <code>coddoc</code>. A continuaci\u00f3n, se comparan las columnas relevantes para detectar duplicados y se almacenan los resultados usando <code>StoreDuplicated</code>. Luego, se eliminan los duplicados usando <code>RemoveDuplicated</code>, y se registra el proceso en el log. Cualquier error se maneja de manera adecuada, registrando tambi\u00e9n los casos en que no se puede crear la columna <code>cod</code>.</p> <pre><code># Validador de campos repetidos\nvalidador_time = time.time()\n\n# Iterar sobre todas las tablas\nfor ky in [x for x in df_structure.keys()]:\n    df = df_structure[ky]\n\n    # Validar si la columna 'cod' existe; si no, crearla\n    if 'cod' not in df.columns:\n        if 'numdoc' in df.columns and 'coddoc' in df.columns:\n            # Crear la columna 'cod' concatenando 'numdoc' y 'coddoc'\n            df = df.assign(cod=df['numdoc'].astype(str) + df['coddoc'].astype(str))\n            logger.info(f\"Columna 'id' creada en el DataFrame: {ky}\")\n        else:\n            # Si no existen las columnas necesarias para crear 'cod', saltar la tabla\n            logger.error(f\"No se encontraron las columnas 'numdoc' y 'coddoc' necesarias para crear 'id' en el DataFrame: {ky}\")\n            continue  # Saltar a la siguiente tabla si no se pueden crear los valores de 'cod'\n\n    # Asegurarse de que la columna 'cod' existe antes de proceder\n    if 'cod' in df.columns:\n        # Definir las columnas para comparar, excluyendo 'id'\n        ColumnsToCompare = [x for x in df.columns if x != 'cod']\n\n        # Imprimir el nombre de la tabla en proceso\n        print(f\"Procesando tabla: {ky}\")\n\n        # Almacenar duplicados\n        try:\n            StoreDuplicated('cod', ColumnsToCompare, df, f'trazaDuplicados_{ky}')\n        except KeyError as e:\n            logger.error(f\"Error en StoreDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Limpieza inicial para quitar duplicados\n        try:\n            df_structure[ky] = RemoveDuplicated('cod', 'fecest', df)\n        except KeyError as e:\n            logger.error(f\"Error en RemoveDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Registrar la validaci\u00f3n en el log\n        logger.info(f'VALIDADOR TABLA: {ky}')\n    else:\n        logger.error(f\"La columna 'cod' no fue creada correctamente en el DataFrame: {ky}\")\n\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>Procesando tabla: xml4c086\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c086.csv\n\n\n2024-10-14 16:34:50,428 - INFO - VALIDADOR TABLA: xml4c086\n\n\nProcesando tabla: xml4c085\nGuardando duplicados\n\n\n2024-10-14 16:35:00,031 - INFO - VALIDADOR TABLA: xml4c085\n\n\nDuplicados guardados en: trazaDuplicados_xml4c085.csv\nProcesando tabla: xml4c087\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c087.csv\n\n\n2024-10-14 16:40:35,024 - INFO - VALIDADOR TABLA: xml4c087\n2024-10-14 16:40:35,025 - INFO - VALIDADOR DUPLICADOS --- 510.98 seconds ---\n</code></pre> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list( [x for x in df_structure.keys()]):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'cod' ] ]\n    print([ky])\n    StoreDuplicated('cod' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = RemoveDuplicated('cod', 'fecest', df_structure[ky])\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>['xml4c086']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c086.csv\n\n\n2024-10-14 16:43:13,922 - INFO - VALIDADOR TABLA: xml4c086\n2024-10-14 16:43:14,009 - INFO - VALIDADOR TABLA: xml4c085\n\n\n['xml4c085']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c085.csv\n['xml4c087']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c087.csv\n\n\n2024-10-14 16:43:16,813 - INFO - VALIDADOR TABLA: xml4c087\n2024-10-14 16:43:16,815 - INFO - VALIDADOR DUPLICADOS --- 4.17 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#definicion-de-columnas-con-sus-tipos","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas de cada tabla en num\u00e9ricas, fechas y texto. Las columnas que contienen \"fec\" en su nombre se convierten al tipo fecha, y se crean los periodos de afiliaci\u00f3n y retiro en formato <code>A\u00f1oMes</code>. El proceso se registra en el log, incluyendo el tiempo total de ejecuci\u00f3n.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list( [x for x in df_structure.keys() ] ):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\n    #Se crean Los periodo para afiliaci\u00f3n y retiro (correcci\u00f3n cuando hay NAN en afi o en ret)\n    df_structure[ky]['perafi'] = (df_structure[ky]['perafi']).fillna(188001).astype(int).astype(str)\n    df_structure[ky]['perret'] = (df_structure[ky]['perret']).fillna(300001).astype(int).astype(str)\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:23:42,901 - INFO - TRANSFORMACION 2 --- 0.44 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#transformacion-de-texto-y-limpieza","title":"Transformaci\u00f3n de texto y limpieza","text":"<p>Se realiza una limpieza de las columnas de tipo texto en todas las tablas. Las columnas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El proceso utiliza <code>.loc</code> para evitar advertencias de asignaci\u00f3n, y el tiempo total de la limpieza se registra en el log.</p> <pre><code># Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in [x for x in df_structure.keys()]:\n    print(ky)\n    # Usar .loc para modificar las columnas seleccionadas y evitar SettingWithCopyWarning\n    # Pasar las columnas tipo texto a UPPER\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.upper())\n\n    # En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.strip())\n\n    # Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>xml4c086\nxml4c085\nxml4c087\n\n\nC:\\Users\\Equipo\\AppData\\Local\\Temp\\ipykernel_780\\2321073015.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n2024-10-14 17:24:06,921 - INFO - LIMPIEZA --- 6.07 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#concatenacion-de-tablas","title":"Concatenaci\u00f3n de tablas","text":"<p>Se concatenan las tablas en un \u00fanico dataframe <code>df_struc_Total</code>, ignorando los \u00edndices previos.</p> <pre><code>#Concatenacion Tablas\ndf_struc_Total = pd.concat( list(df_structure.values()) , ignore_index = True )\n</code></pre> <pre><code>BaseXmlTotal = df_struc_Total\n</code></pre> <pre><code>BaseXmlTotal\n</code></pre> nit salario estado tipide id cod periodo perret perafi horas ... tipper calemp codact cedtra coddoc coddocben documento codben captra nivedu 0 900280342 1300000.0 A 1.0 7142937 17142937 202402 202402 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 819005979 535600.0 A 1.0 70091318 170091318 202401 202401 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 71376503 1300000.0 A 15.0 7425498 157425498 202406 NaN 202309 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 900668068 1000000.0 A 1.0 1082968618 11082968618 202306 202307 202001 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 900117086 1950000.0 A 1.0 85455480 185455480 202401 202402 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 585110 NaN NaN A NaN NaN 149597057 202409 NaN 202408 NaN ... NaN NaN NaN 9910609 1 1 49597057 NaN 2.0 NaN 585111 NaN NaN A NaN NaN 21082877639 202409 NaN 202303 NaN ... NaN NaN NaN 9910731 1 2 1082877639 314997.0 2.0 01 585112 NaN NaN A NaN NaN 11082939020 202409 NaN 201912 NaN ... NaN NaN NaN 9910731 1 1 1082939020 NaN 2.0 NaN 585113 NaN NaN A NaN NaN 21083018512 202409 NaN 202308 NaN ... NaN NaN NaN 9910731 1 2 1083018512 310091.0 2.0 01 585114 NaN NaN A NaN NaN 11221984582 202409 NaN 202112 NaN ... NaN NaN NaN 997043231071997 9 1 1221984582 NaN 2.0 NaN <p>585115 rows \u00d7 29 columns</p>"},{"location":"seccion/1.5-FactAfiliacion/#registro-de-afiliacion","title":"Registro de afiliaci\u00f3n","text":"<p>Se a\u00f1ade una nueva columna <code>Afiliado</code> en el dataframe <code>BaseXmlTotal</code>. Esta columna indica si un registro estaba afiliado en un periodo espec\u00edfico, basado en las fechas de afiliaci\u00f3n (<code>periodoAfi</code>) y retiro (<code>periodoRet</code>). Si el periodo actual (<code>Periodo</code>) se encuentra entre las fechas de afiliaci\u00f3n y retiro, el valor es <code>'si'</code>, de lo contrario, es <code>'no'</code>.</p> <pre><code>#registro de afiliaci\u00f3n\nBaseXmlTotal['Afiliado'] = np.where( \n   ( ( BaseXmlTotal['perafi'] &lt;= BaseXmlTotal['periodo'] ) &amp; ( BaseXmlTotal['periodo'] &lt;= BaseXmlTotal['perret'] ) )    \n    , 'si', 'no' )\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#organizacion-de-columnas","title":"Organizaci\u00f3n de columnas","text":"<p>Se reorganizan las columnas del dataframe <code>BaseXmlTotal</code>. La columna <code>periodo</code> se mueve a la primera posici\u00f3n y la columna <code>Afiliado</code> se coloca en la tercera posici\u00f3n del dataframe.</p> <pre><code>#Organizar Columnas\nf_column = BaseXmlTotal.pop('periodo')\nBaseXmlTotal.insert(0,'periodo', f_column )\nf_column = BaseXmlTotal.pop('Afiliado')\nBaseXmlTotal.insert(2,'Afiliado', f_column )\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#agregacion-de-datos","title":"Agregaci\u00f3n de datos","text":"<p>Se agrupan los datos de <code>BaseXmlTotal</code> por las columnas <code>cod</code> y <code>periodo</code>, donde el periodo se agrupa en bloques trimestrales. Las columnas clave se agregan utilizando funciones como <code>last</code>, <code>first</code>, <code>sum</code> y <code>count</code>, dependiendo de la naturaleza de los datos. Despu\u00e9s de la agregaci\u00f3n, el dataframe resultante se asigna nuevamente a <code>BaseXmlTotal</code> y se registra el tiempo total de la operaci\u00f3n en el log.</p> <pre><code># Iniciar la agregaci\u00f3n\n# tagreg_start_time = time.time()\n# logger.info('INICIO DE AGREGACI\u00d3N DE DATOS')\n\n# # Agrupar BaseXmlTotal\n# # Se agrupa por 'periodo' y 'cod' para reducir la cantidad de registros finales\n# # Se agregan columnas clave seg\u00fan las instrucciones proporcionadas\n# BaseXmlTotal['periodo'] = BaseXmlTotal['periodo'].astype(str).str[:6]  # Extraer el formato yyyymm\n# #Comentar la siguiente linea en caso de necesitar continuar con la agregacion original y solo estructurar las columnas\n# BaseXmlTotal['periodo'] = (BaseXmlTotal['periodo'].astype(int) // 3 * 3).astype(str)  # Agrupar cada 3 meses\n# BaseXmlTotal_agg = BaseXmlTotal.groupby(['cod','periodo'], as_index=False).agg({\n#     'periodo':'last',\n#     'Afiliado': 'last',\n#     'perafi': 'first',\n#     'fecsis': 'last',\n#     'fecest': 'last',\n#     'calemp': 'last',\n#     'tipper': 'last',\n#     'codact': 'last',\n#     'codest': 'last',\n#     'estado': 'last',\n#     'tipide': 'last',\n#     'id': 'last',\n#     'documento': 'last',\n#     'perret': 'last',\n#     'codben': 'count',\n#     'codcat': 'last',\n#     'codlis': 'last',\n#     'codsuc': 'last',\n#     'captra': 'last',\n#     'nivedu': 'last',\n#     'coddoc': 'last',\n#     'cedtra': 'last',\n#     'nit': 'last',\n#     'horas': 'sum',\n#     'salario': 'last'\n# })\n\n# # Opcional: Mostrar las primeras filas para verificar\n# # print(BaseXmlTotal_agg.head())\n\n\n# # Asignar el DataFrame agrupado a BaseXmlTotal para la siguiente etapa\n# BaseXmlTotal = BaseXmlTotal_agg\n\n\n# # Finalizar la agregaci\u00f3n\n# logger.info(f'AGREGACI\u00d3N FINALIZADA --- {time.time() - tagreg_start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:25:28,867 - INFO - INICIO DE AGREGACI\u00d3N DE DATOS\n2024-10-14 17:25:31,591 - INFO - AGREGACI\u00d3N FINALIZADA --- 2.72 seconds ---\n</code></pre> <pre><code># Renombrar las columnas\nBaseXmlTotal = BaseXmlTotal.rename(columns={\n     'cod':'ID_REGISTRO',\n     'periodo': 'PERIODO',\n     'Afiliado':'ID_AFILIADO',\n     'calemp':'COD_CALIDAD_SUCURSAL',\n     'tipper':'TIPPER',\n     'codact':'COD_ACT',\n     'codest':'COD_EST_INAC',\n     'codcat':'CATEGORIA',\n     'estado':'COD_EST_AFIL',\n     'codsuc':'ID_SUCURSAL', #Ajustar \n     'perafi':'PERIODO_AFILIACION',\n     'perret':'PERIODO_RETIRO',\n     'fecsis':'FECHA_SISTEMA',\n     'fecest':'FECHA_ESTADO',\n     'codben':'COD_BENEFICIARIO',\n     'tipide':'TIPO_DOCUMENTO',\n     'id':'',\n     'coddocben':'',\n     'documento':'',\n\n\n\n#     'codlis':'',\n#     'captra':'',\n#     'nivedu':'',\n#     'coddoc':'',\n#     'cedtra':'',\n#     'nit':'',\n#     'horas':'',\n#     'salario':'' \n# })\n# BaseXmlTotal.columns.tolist()\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>BaseXmlTotal</code> en la tabla <code>BD_FactAfiliacion</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(BaseXmlTotal, 'BD_Fact_Afiliacion', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-14 17:34:15,493 - INFO - CONEXION A BASE DWH\n2024-10-14 17:34:15,768 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactAfiliacion\n2024-10-14 17:35:17,055 - INFO - Tabla almacenada correctamente.\n2024-10-14 17:35:17,723 - INFO - ALMACENAMIENTO --- 62.23 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactAfiliacion/#registro-de-finalizacion-del-etl","title":"Registro de finalizaci\u00f3n del ETL","text":"<p>Se registra el tiempo total de ejecuci\u00f3n del proceso ETL desde su inicio hasta su finalizaci\u00f3n en el log, indicando que el proceso ha concluido con \u00e9xito.</p> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:36:48,552 - INFO - FINAL ETL --- 6204.20 seconds ---\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/","title":"1.6-FactDatosContacto","text":""},{"location":"seccion/1.6-FactDatosContacto/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, y <code>dateutil</code>, junto con funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, lo que asegura la correcta importaci\u00f3n de las funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, RemoveDuplicated, obtener_conexion, cargar_tablas, guardar_en_dwh, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 14-10-2024 09:07\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>FactDatosContacto.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactDatosContacto.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-14 09:07:13,360 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un diccionario <code>qr_structure</code> con consultas SQL para extraer datos de las tablas <code>subsi15</code>, <code>subsi02</code>, y <code>subsi22</code> de la base de datos <code>subsidio</code>. Estas consultas generan un identificador \u00fanico <code>id</code> y extraen diversas columnas clave para los trabajadores, empresas y beneficiarios. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi15_dc\":'''select \nCONCAT( coddoc, cedtra) as id,codsuc,codlis,cedtra,coddoc,\ndireccion,codciu,codbar,telefono,email,codzon,rural,agro,captra,\ntipdis,horas,salario,tipsal,fecsal,sexo,estciv,tipcon,feccon,trasin,\nvivienda,cabhog,nivedu,tipcot,vendedor,empleador,codcue,ofides,codban,\nnumcue,tipcue,fecemi,feccar,codcat,fecnac,ciunac,fecing,fecpre,fecafi,\nfecsis,fecmod,usumod,usuario,giro,codgir,estado,codest,fecest,carnet,benef,\nruaf,ruasub,fecrua,nota,habdat,notcor,fecexp,codfonpen,numafipen,orisex,\ncodocu,facvul,codetn,codpai,celular1,celular2,tiptar,cedant,pueblo,resguardo, 1 as Trabajador\nfrom subsidio.subsi15''',\n\n##############################SUBSI02###############################################\n    \"subsi02_dc\":'''select \nCONCAT( coddoc , nit ) as id,nit,digver,tipper,coddoc,razsoc,sigla,nomcom,\ncoddocreppri,cedrep,repleg,coddocrepsup,cedrepsup,replegsup,jefper,direccion,\ncodciu,codbar,celular,telefono,fax,email,codzon,dirpri,ciupri,celpri,telpri,\nfaxpri,emailpri,nomcon,ofiafi,codase,calemp,tipemp,tipsoc,tipapo,forpre,pymes,\ncontratista,colegio,habdat,notcor,matmer,codact,codind,feccer,actapr,fecapr,\nfecafi,fecsis,fecmod,estado,resest,codest,fecest,totapo,nomini,coddocrepleg,\npriaperepleg,segaperepleg,prinomrepleg,segnomrepleg,codcaj,pagweb,dirnot,emailnot, 1 as Empresa\nfrom subsidio.subsi02''',\n##############################SUBSI22_23###############################################\n    \"subsi22_dc\":'''select \nCONCAT( coddoc,documento) as id, codben,documento,fecexp,coddoc,priape,segape,prinom,segnom,parent,huerfano,tiphij,\ncaptra,tipdis,nivedu,sexo,fecnac,ciunac,calendario,giro,codgir,estado,codest,fecest,\nregimen,tipocotizante,created_at,updated_at, 1 as Beneficiario\nfrom subsidio.subsi22'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-14 09:07:15,635 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n# Ejemplo de c\u00f3mo llamarla\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-14 09:07:15,656 - INFO - CONEXION A BASE MINERVA\n2024-10-14 09:07:16,177 - INFO - Cargando subsi15_dc \n2024-10-14 09:08:10,668 - INFO - Cargada subsi15_dc --- 54.49 seconds ---\n2024-10-14 09:08:10,669 - INFO - Cargando subsi02_dc \n2024-10-14 09:08:20,958 - INFO - Cargada subsi02_dc --- 10.29 seconds ---\n2024-10-14 09:08:20,960 - INFO - Cargando subsi22_dc \n2024-10-14 09:08:51,585 - INFO - Cargada subsi22_dc --- 30.63 seconds ---\n2024-10-14 09:08:51,768 - INFO - CARGUE TABLAS DESDE MYSQL --- 96.11 seconds ---\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas del diccionario <code>df_structure</code>. Si no existe la columna <code>id</code>, se crea concatenando las columnas <code>numdoc</code> y <code>coddoc</code>. Luego, se compara el resto de las columnas para detectar duplicados. Los registros duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y se eliminan los duplicados con <code>RemoveDuplicated</code>. Se registran en el log los resultados de la validaci\u00f3n, y se manejan adecuadamente los posibles errores.</p> <pre><code># Validador de campos repetidos\nvalidador_time = time.time()\n\n# Iterar sobre todas las tablas excepto 'subsi16'\nfor ky in [x for x in df_structure.keys()]:\n    df = df_structure[ky]\n\n    # Validar si la columna 'id' existe; si no, crearla\n    if 'id' not in df.columns:\n        if 'numdoc' in df.columns and 'coddoc' in df.columns:\n            # Crear la columna 'id' concatenando 'numdoc' y 'coddoc'\n            df = df.assign(id=df['numdoc'].astype(str) + df['coddoc'].astype(str))\n            logger.info(f\"Columna 'id' creada en el DataFrame: {ky}\")\n        else:\n            # Si no existen las columnas necesarias para crear 'id', saltar la tabla\n            logger.error(f\"No se encontraron las columnas 'numdoc' y 'coddoc' necesarias para crear 'id' en el DataFrame: {ky}\")\n            continue  # Saltar a la siguiente tabla si no se pueden crear los valores de 'id'\n\n    # Asegurarse de que la columna 'id' existe antes de proceder\n    if 'id' in df.columns:\n        # Definir las columnas para comparar, excluyendo 'id'\n        ColumnsToCompare = [x for x in df.columns if x != 'id']\n\n        # Imprimir el nombre de la tabla en proceso\n        print(f\"Procesando tabla: {ky}\")\n\n        # Almacenar duplicados\n        try:\n            StoreDuplicated('id', ColumnsToCompare, df, f'trazaDuplicados_{ky}')\n        except KeyError as e:\n            logger.error(f\"Error en StoreDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Limpieza inicial para quitar duplicados\n        try:\n            df_structure[ky] = RemoveDuplicated('id', 'fecest', df)\n        except KeyError as e:\n            logger.error(f\"Error en RemoveDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Registrar la validaci\u00f3n en el log\n        logger.info(f'VALIDADOR TABLA: {ky}')\n    else:\n        logger.error(f\"La columna 'id' no fue creada correctamente en el DataFrame: {ky}\")\n\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>Procesando tabla: subsi15_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi15_dc.csv\n\n\n2024-10-14 09:09:16,105 - INFO - VALIDADOR TABLA: subsi15_dc\n\n\nProcesando tabla: subsi02_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi02_dc.csv\n\n\n2024-10-14 09:09:18,542 - INFO - VALIDADOR TABLA: subsi02_dc\n\n\nProcesando tabla: subsi22_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi22_dc.csv\n\n\n2024-10-14 09:09:25,954 - INFO - VALIDADOR TABLA: subsi22_dc\n2024-10-14 09:09:25,956 - INFO - VALIDADOR DUPLICADOS --- 34.17 seconds ---\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#definicion-de-columnas-con-sus-tipos","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas de cada tabla en el diccionario <code>df_structure</code> en tres tipos: num\u00e9ricas, de fechas y de texto. Las columnas de fechas, identificadas por el prefijo \"fec\", se convierten al formato de fecha utilizando <code>pd.to_datetime()</code>. Se registran los tipos de columnas en tres diccionarios (<code>numericColumns</code>, <code>datesColumns</code>, <code>textColumns</code>) y se asegura que las transformaciones se apliquen correctamente. El tiempo total de este proceso se registra en el log.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 09:09:29,742 - INFO - TRANSFORMACION 2 --- 3.75 seconds ---\n</code></pre> <pre><code>df_structure['subsi15_dc'].columns.to_series().groupby(df_structure['subsi15_dc'].dtypes).groups\n</code></pre> <pre><code>{int64: ['codbar', 'horas', 'salario', 'usuario', 'orisex', 'facvul', 'codetn', 'Trabajador'], datetime64[ns]: ['fecsal', 'feccon', 'fecemi', 'feccar', 'fecnac', 'fecing', 'fecpre', 'fecafi', 'fecsis', 'fecmod', 'fecest', 'fecrua', 'fecexp'], float64: ['usumod', 'ruasub'], object: ['id', 'codsuc', 'codlis', 'cedtra', 'coddoc', 'direccion', 'codciu', 'telefono', 'email', 'codzon', 'rural', 'agro', 'captra', 'tipdis', 'tipsal', 'sexo', 'estciv', 'tipcon', 'trasin', 'vivienda', 'cabhog', 'nivedu', 'tipcot', 'vendedor', 'empleador', 'codcue', 'ofides', 'codban', 'numcue', 'tipcue', 'codcat', 'ciunac', 'giro', 'codgir', 'estado', 'codest', 'carnet', 'benef', 'ruaf', 'nota', 'habdat', 'notcor', 'codfonpen', 'numafipen', 'codocu', 'codpai', 'celular1', 'celular2', 'tiptar', 'cedant', 'pueblo', 'resguardo']}\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#transformacion-y-limpieza-de-datos","title":"Transformaci\u00f3n y limpieza de datos","text":"<p>Se realiza una limpieza de las columnas de tipo texto en todas las tablas del diccionario <code>df_structure</code>, excepto en <code>subsi15</code> y <code>subsi16</code>. Las columnas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list( [x for x in df_structure.keys() if  x not in [ 'subsi15' , 'subsi16' ] ]   ):\n    print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NAN', np.nan)\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>subsi15_dc\n\n\nC:\\Users\\flavi\\AppData\\Local\\Temp\\ipykernel_9096\\136011473.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\n\n\nsubsi02_dc\n\n\nC:\\Users\\flavi\\AppData\\Local\\Temp\\ipykernel_9096\\136011473.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\n\n\nsubsi22_dc\n\n\n2024-10-14 09:10:20,666 - INFO - LIMPIEZA --- 50.87 seconds ---\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#concatenacion-de-tablas","title":"Concatenaci\u00f3n de tablas","text":"<p>Se concatenan todas las tablas presentes en el diccionario <code>df_structure</code> en un \u00fanico dataframe llamado <code>df_struc_Total</code>. La concatenaci\u00f3n se realiza ignorando los \u00edndices previos, lo que crea un dataframe consolidado con todos los registros.</p> <pre><code>#Concatenacion Tablas\ndf_struc_Total = pd.concat( list(df_structure.values()) , ignore_index = True )\n</code></pre> <pre><code>#df_struc_Total = df_struc_Total.drop_duplicates(subset='id', keep='last')\n#df_struc_Total.columns.tolist()\n</code></pre>"},{"location":"seccion/1.6-FactDatosContacto/#agregacion-de-datos-por-periodo-trimestral","title":"Agregaci\u00f3n de datos por periodo trimestral","text":"<p>Se realiza una agregaci\u00f3n del dataframe <code>df_struc_Total</code> por <code>id</code> y por trimestre, utilizando la columna <code>fecafi</code> para generar los periodos trimestrales. Se aplican diferentes funciones de agregaci\u00f3n como <code>last</code>, <code>first</code>, <code>sum</code>, y <code>nunique</code> seg\u00fan corresponda a cada columna. Despu\u00e9s de la agregaci\u00f3n, se elimina la columna <code>Trimestre</code> si no es necesaria en el dataframe final. El proceso se registra en el log, indicando tanto el inicio como la finalizaci\u00f3n de la agregaci\u00f3n.</p> <pre><code># Iniciar la agregaci\u00f3n en el log\nlogger.info('INICIO DE AGREGACI\u00d3N POR PERIODO')\n\n# Convertir 'fecafi' a un formato trimestral para la agrupaci\u00f3n y evitar fragmentaci\u00f3n\ndf_struc_Total = pd.concat([df_struc_Total, pd.to_datetime(df_struc_Total['fecafi'], errors='coerce').dt.to_period('Q').rename('Trimestre')], axis=1)\n\n# Agrupar por 'id' y 'Trimestre'\ndf_agg = df_struc_Total.groupby(['id', 'Trimestre'], as_index=False).agg({\n    'codsuc': 'last', 'codlis': 'last',  'cedtra': 'last',  'coddoc': 'last',  'direccion': 'last',  'codciu': 'last',  'codbar': 'last',  'telefono': 'last',\n    'email': 'last',  'codzon': 'last',  'rural': 'last',  'agro': 'last',  'captra': 'last',  'tipdis': 'last',  'horas': 'sum',              # Sumar las horas \n    'salario': 'last',           # Mantener el \u00faltimo salario registrado\n    'tipsal': 'last',  'fecsal': 'last',  'sexo': 'last',  'estciv': 'last',  'tipcon': 'last',  'feccon': 'last',  'trasin': 'last',  'vivienda': 'last',\n    'cabhog': 'last',  'nivedu': 'last',  'tipcot': 'last',  'vendedor': 'last',  'empleador': 'last',  'codcue': 'last',  'ofides': 'last',  'codban': 'last',\n    'numcue': 'last',  'tipcue': 'last',  'fecemi': 'last',  'feccar': 'last',  'codcat': 'last',  'fecnac': 'last',  'ciunac': 'last',  'fecing': 'last',\n    'fecpre': 'last',  'fecafi': 'first',           # Mantener la primera fecha \n    'fecsis': 'last',  'fecmod': 'last',  'usumod': 'last',  'usuario': 'last',  'giro': 'last',  'codgir': 'last',  'estado': 'last',  'codest': 'last',\n    'fecest': 'last',  'carnet': 'last',  'benef': 'last',  'ruaf': 'last',  'ruasub': 'last',  'fecrua': 'last',  'nota': 'last',  'habdat': 'last',\n    'notcor': 'last',  'fecexp': 'last',  'codfonpen': 'last',  'numafipen': 'last',  'orisex': 'last',  'codocu': 'last',  'facvul': 'last',  'codetn': 'last',\n    'codpai': 'last',  'celular1': 'last',  'celular2': 'last',  'tiptar': 'last',  'cedant': 'last',  'pueblo': 'last',  'resguardo': 'last',  'Trabajador': 'last',\n    'nit': 'last',   'digver': 'last',  'tipper': 'last',  'razsoc': 'last',  'sigla': 'last',  'nomcom': 'last',  'coddocreppri': 'last',  'cedrep': 'last',\n    'repleg': 'last',  'coddocrepsup': 'last',  'cedrepsup': 'last',  'replegsup': 'last',  'jefper': 'last',  'celular': 'last',  'fax': 'last',  'dirpri': 'last',\n    'ciupri': 'last',  'celpri': 'last',  'telpri': 'last',  'faxpri': 'last',  'emailpri': 'last',  'nomcon': 'last',  'ofiafi': 'last',  'codase': 'last',\n    'calemp': 'last',  'tipemp': 'last',  'tipsoc': 'last',  'tipapo': 'last',  'forpre': 'last',  'pymes': 'last',  'contratista': 'last',  'colegio': 'last',\n    'matmer': 'last',  'codact': 'last',  'codind': 'last',  'feccer': 'last',  'actapr': 'last',  'fecapr': 'last',  'resest': 'last',  'totapo': 'last',\n    'nomini': 'last',  'coddocrepleg': 'last',  'priaperepleg': 'last',  'segaperepleg': 'last',  'prinomrepleg': 'last',  'segnomrepleg': 'last',  'codcaj': 'last',  'pagweb': 'last',\n    'dirnot': 'last',  'emailnot': 'last',  'Empresa': 'last',  'codben': 'nunique',          # N\u00famero de beneficiarios \u00fanicos\n    'documento': 'last',  'priape': 'last',  'segape': 'last',  'prinom': 'last',   'segnom': 'last',  'parent': 'last',  'huerfano': 'last',  'tiphij': 'last',\n    'calendario': 'last',  'regimen': 'last',  'tipocotizante': 'last',  'created_at': 'last',  'updated_at': 'last',  'Beneficiario': 'last'\n})\n\n# Eliminar columna 'Trimestre' si no se necesita en el DataFrame final\ndf_agg.drop(columns=['Trimestre'], inplace=True)\n\n# Finalizar la agregaci\u00f3n en el log\nlogger.info('AGREGACI\u00d3N POR PERIODO FINALIZADA')\n\n# Asignar el DataFrame agrupado a df_struc_Total para la siguiente etapa\ndf_struc_Total = df_agg\n</code></pre> <pre><code>2024-10-14 09:10:22,167 - INFO - INICIO DE AGREGACI\u00d3N POR PERIODO\n2024-10-14 09:10:46,655 - INFO - AGREGACI\u00d3N POR PERIODO FINALIZADA\n</code></pre> <pre><code>df_struc_Total = df_agg\n# Convertir todos los nombres de las columnas de df_struc_Total a may\u00fasculas\ndf_struc_Total.columns = df_struc_Total.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf_struc_Total = df_struc_Total.rename(columns={\n    'ID': 'ID_AFILIADO', \n    'CODCIU': 'COD_CIU',\n    'CODSUC':'COD_SUCURSAL',\n    'CODZON':'COD_ZON',\n    'ESTCIV':'ESTADO_CIVIL',\n    'NUMCUE':'NUMERO_CUENTA',\n    'TIPCUE':'TIPO_CUENTA',\n    'CODBAN':'COD_BANCO'\n    })\n\n# Seleccionar solo las columnas que deseas conservar\ndf_struc_Total = df_struc_Total[[\n    'ID_AFILIADO', 'COD_CIU', 'COD_SUCURSAL', 'COD_ZON', 'ESTADO_CIVIL', 'NUMERO_CUENTA', 'TIPO_CUENTA', 'COD_BANCO',\n    'TELEFONO','EMAIL','DIRECCION'\n    ]]\n</code></pre> <pre><code>df_struc_Total\n</code></pre> ID_AFILIADO COD_CIU COD_SUCURSAL COD_ZON ESTADO_CIVIL NUMERO_CUENTA TIPO_CUENTA COD_BANCO TELEFONO EMAIL DIRECCION 0 CC04619 47001 001 47001 1 None E NaN None None CRA 36 31-60 1 CC05715 47001 001 47001 1 None E NaN 4311683 None CALLE 13 2 27 2 CC09039 47001 001 47001 1 None E NaN None None None 3 CC090913 47001 001 47001 1 None E NaN 4502857 None MZ 33 CASA 20 PARQUE 4 CC091013 47001 001 47001 1 None E NaN None None MZ A CASA 15 ... ... ... ... ... ... ... ... ... ... ... ... 463467 TI98120770829 47001 001 47001 1 None E NaN None None MZ 125 CS2 CIUDADELA 463468 TI99012615902 47001 001 47001 1 None E NaN 4201354 TIENDASAMARIASAS@GAMAIL.COM CL 12 N 17-21 463469 TI99022410527 47001 001 47001 1 None E NaN None None CRA 50 N.24A-150 463470 TI99032003377 47001 001 47001 1 None E NaN None None MZ 14 CS 6 URB EL CISNE 463471 TI99040317160 47001 001 47001 1 None E NaN None CAMILO036@GMAIL.COM CL 32 N31-28 COREA <p>463472 rows \u00d7 11 columns</p>"},{"location":"seccion/1.6-FactDatosContacto/#guardar-en-la-base-de-datos-dwh-con-manejo-de-errores","title":"Guardar en la base de datos DWH con manejo de errores","text":"<p>Se intenta guardar los primeros 1000 registros del dataframe <code>df_struc_Total</code> en la tabla <code>BD_FactDatosContactos</code> en la base de datos DWH. Si el proceso es exitoso, se registra el tiempo de almacenamiento en el log. En caso de error, se realiza un rollback para evitar transacciones pendientes y se registra el error en el log.</p> <pre><code># Llamada a la funci\u00f3n para un \u00fanico DataFrame\nguardar_en_dwh(df_struc_Total, 'BD_FactDatosContactos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-14 09:10:50,097 - INFO - CONEXION A BASE DWH\n2024-10-14 09:10:50,662 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactDatosContactos\n2024-10-14 09:11:44,758 - INFO - Tabla almacenada correctamente.\n2024-10-14 09:11:45,308 - INFO - ALMACENAMIENTO --- 55.21 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 09:11:45,346 - INFO - FINAL ETL --- 272.06 seconds ---\n</code></pre>"},{"location":"seccion/2.1-FactAportes/","title":"2.1-FactAportes","text":""},{"location":"seccion/2.1-FactAportes/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, y <code>dateutil</code>, junto con funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, lo que asegura la correcta importaci\u00f3n de las funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport time\nimport os\nimport logging\nstart_time = time.time()\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 12-10-2024 23:19\n</code></pre>"},{"location":"seccion/2.1-FactAportes/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>FactAportes.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactAportes.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-12 23:19:33,037 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.1-FactAportes/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando <code>create_engine</code> y la cadena de conexi\u00f3n generada por <code>Conexion_dwh()</code>. Se registra en el log la confirmaci\u00f3n de la conexi\u00f3n.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n</code></pre> <pre><code>2024-10-12 23:19:33,547 - INFO - CONEXION A BASE MINERVA\n</code></pre>"},{"location":"seccion/2.1-FactAportes/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL para la tabla <code>subsi11</code> en el diccionario <code>qr_structure</code>, que extrae todas las columnas de la tabla y genera una nueva columna <code>id_no_codoc</code> basada en el campo <code>nit</code>. El proceso de lectura de las consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi11\":'''select *, nit as id_no_codoc from subsidio.subsi11'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-12 23:19:33,555 - INFO - LECTURA DE QUERYS\n</code></pre> <pre><code># Cargue de tablas desde sql\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-12 23:20:49,036 - INFO - Carga subsi11 --- 74.97 seconds ---\n2024-10-12 23:20:49,427 - INFO - CARGUE TABLAS DESDE MYSQL --- 75.86 seconds ---\n</code></pre>"},{"location":"seccion/2.1-FactAportes/#definicion-de-columnas-con-sus-tipos","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas del diccionario <code>df_structure</code> en tres categor\u00edas: columnas num\u00e9ricas, columnas de fechas (aquellas que comienzan con \"fec\") y columnas de texto. Las columnas de fechas se convierten al formato de fecha usando <code>pd.to_datetime()</code> con manejo de errores. Las listas de columnas se almacenan en tres diccionarios: <code>numericColumns</code>, <code>datesColumns</code> y <code>textColumns</code>. El tiempo total del proceso se registra en el log.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 23:20:50,697 - INFO - TRANSFORMACION 2 --- 1.26 seconds ---\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/","title":"2.2-FactEntrega","text":""},{"location":"seccion/2.2-FactEntrega/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport os\nimport logging\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 12:10\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Fact_entrega.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code># Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='Fact_entrega.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\nstart_time = time.time()\n</code></pre> <pre><code>2024-10-13 12:10:32,172 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL en el diccionario <code>qr_structure</code> que extrae datos de las tablas <code>subsi09</code> y <code>subsi146</code> a trav\u00e9s de un <code>INNER JOIN</code>. La consulta selecciona varias columnas clave relacionadas con fechas, valores y documentos. El proceso de lectura de las consultas se registra en el log.</p> <pre><code>qr_structure = {\n    \"fact_entrega\": f\"SELECT s09.id AS id, s09.fecanu AS f_anu, s09.fecasi AS f_asig_cuota, s09.fecent AS f_disp_benef, s09.feccon AS f_consign, s09.fecche AS f_cheque, s09.fecfos AS f_fosfec, s09.valcre AS val_credito, s09.valaju AS val_ajuste, s09.documento AS doc_contable, s146.fecenv AS f_envio, s146.feccon AS f_conciliac, s146.fecrec AS f_rechazo, s146.fecpre AS f_prescrip, s146.fecifz AS f_interfaz, s146.fecpto AS f_reemplazo, s146.valor AS val_cuota, s146.mempag AS num_memorando, s146.doccon AS doc_conciliac, s146.docpre AS doc_prescrip FROM subsidio.subsi09 s09 INNER JOIN subsidio.subsi146 s146 ON s09.id = s146.id09\"\n}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:10:32,186 - INFO - LECTURA DE QUERYS\n</code></pre> <pre><code>qr_structure\n</code></pre> <pre><code>{'fact_entrega': 'SELECT s09.id AS id, s09.fecanu AS f_anu, s09.fecasi AS f_asig_cuota, s09.fecent AS f_disp_benef, s09.feccon AS f_consign, s09.fecche AS f_cheque, s09.fecfos AS f_fosfec, s09.valcre AS val_credito, s09.valaju AS val_ajuste, s09.documento AS doc_contable, s146.fecenv AS f_envio, s146.feccon AS f_conciliac, s146.fecrec AS f_rechazo, s146.fecpre AS f_prescrip, s146.fecifz AS f_interfaz, s146.fecpto AS f_reemplazo, s146.valor AS val_cuota, s146.mempag AS num_memorando, s146.doccon AS doc_conciliac, s146.docpre AS doc_prescrip FROM subsidio.subsi09 s09 INNER JOIN subsidio.subsi146 s146 ON s09.id = s146.id09'}\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:10:32,843 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:10:33,357 - INFO - Cargando fact_entrega \n2024-10-13 12:10:34,590 - INFO - Cargada fact_entrega --- 1.23 seconds ---\n2024-10-13 12:10:34,679 - INFO - CARGUE TABLAS DESDE MYSQL --- 1.83 seconds ---\n</code></pre> <pre><code>df_structure['fact_entrega']\n</code></pre> id f_anu f_asig_cuota f_disp_benef f_consign f_cheque f_fosfec val_credito val_ajuste doc_contable f_envio f_conciliac f_rechazo f_prescrip f_interfaz f_reemplazo val_cuota num_memorando doc_conciliac doc_prescrip 0 20985911 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 1 20985912 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 2 20985913 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 3 20929805 None 2022-02-21 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 34994 1.0 None None 4 20979008 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-05-26 2022-06-01 2022-02-23 None None None 34994 2.0 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 39438 25809207 None 2024-09-25 2024-09-27 None None None 0 0 None None None 2024-09-27 None None None 44491 NaN None None 39439 25778144 None 2024-09-18 2024-09-24 None None None 0 0 None None None 2024-09-24 None None None 44491 NaN None None 39440 25691627 None 2024-09-19 2024-09-24 None None None 0 0 None 2024-10-11 None 2024-09-24 None None None 44491 102.0 None None 39441 25691628 None 2024-09-19 2024-09-24 None None None 0 0 None 2024-10-11 None 2024-09-24 None None None 44491 102.0 None None 39442 25802299 None 2024-09-25 2024-09-27 None None None 0 0 None None None 2024-09-27 None None None 51165 NaN None None <p>39443 rows \u00d7 20 columns</p>"},{"location":"seccion/2.2-FactEntrega/#proceso-de-limpieza-y-validacion-de-duplicados","title":"Proceso de limpieza y validaci\u00f3n de duplicados","text":"<p>Se define un proceso para comparar y limpiar los datos de cada tabla en <code>df_structure</code>. Se comparan las columnas listadas en <code>ColumnsToCompare</code> para identificar duplicados y eliminarlos. Luego, se aplica una limpieza adicional para detectar y corregir errores en los datos utilizando las funciones <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, y <code>RemoveErrors</code>. Los duplicados y errores se registran en archivos espec\u00edficos para cada tabla y se registra el proceso en el log.</p> <pre><code># Columnas que deseas comparar y limpiar\ncolumId = 'id'  # Ajusta seg\u00fan tu identificador \u00fanico\nColumnsToCompare = ['f_anu', 'f_asig_cuota', 'f_disp_benef', 'f_consign', 'f_cheque', 'f_fosfec', \n                    'val_credito', 'val_ajuste', 'doc_contable', 'f_envio', 'f_conciliac', \n                    'f_rechazo', 'f_prescrip', 'f_interfaz', 'f_reemplazo', 'val_cuota', \n                    'num_memorando', 'doc_conciliac', 'doc_prescrip']\n\n# Iterar sobre los DataFrames en df_structure\nfor key, df in df_structure.items():\n    print(f\"Procesando tabla: {key}\")\n\n    # Guardar duplicados\n    StoreDuplicated(columId, ColumnsToCompare,df, f'duplicados_fact_entrega_{key}')\n    logger.info(f'Duplicados guardados: {key}')\n\n    # Eliminar duplicados por fecha\n    df_cleaned = RemoveDuplicated(columId, 'f_disp_benef', df)\n    logger.info(f'Duplicados por fecha eliminados: {key}')\n\n    # Limpiar errores\n    df_final = RemoveErrors(df_cleaned, f'errores_fact_entrega_{key}')\n\n    # Registrar informaci\u00f3n de la limpieza\n    logger.info(f'Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: {key}')\n</code></pre> <pre><code>Procesando tabla: fact_entrega\nGuardando duplicados\n\n\n2024-10-13 12:10:34,996 - INFO - Duplicados guardados: fact_entrega\n2024-10-13 12:10:35,044 - INFO - Duplicados por fecha eliminados: fact_entrega\n2024-10-13 12:10:35,046 - INFO - Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: fact_entrega\n\n\nDuplicados guardados en: duplicados_fact_entrega_fact_entrega.csv\n</code></pre> <pre><code>df_final\n</code></pre> id f_anu f_asig_cuota f_disp_benef f_consign f_cheque f_fosfec val_credito val_ajuste doc_contable f_envio f_conciliac f_rechazo f_prescrip f_interfaz f_reemplazo val_cuota num_memorando doc_conciliac doc_prescrip 33616 7011580 None 2011-04-19 2011-04-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-04-25 None None None 20346 78.0 None None 33615 7005252 None 2011-04-19 2011-04-25 None None None 0 0 None None None 2011-04-25 None 2024-02-29 2024-02-29 20346 NaN None 0011070 33587 6974521 None 2011-04-19 2011-04-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-04-25 None None None 23397 78.0 None None 33586 7236763 None 2011-07-18 2011-07-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-07-25 None None None 23397 78.0 None None 33585 7203942 None 2011-07-18 2011-07-25 None None None 0 0 None 2024-02-14 2024-02-15 2011-07-25 None None None 23397 77.0 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 28003 17431466 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28004 17431470 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28005 17431471 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28006 17431490 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28007 17431507 None 0000-00-00 NaT None None None 0 0 None 2023-08-25 2023-08-25 2023-07-29 None None None 31450 54.0 None None <p>39443 rows \u00d7 20 columns</p>"},{"location":"seccion/2.2-FactEntrega/#definicion-y-transformacion-de-columnas","title":"Definici\u00f3n y transformaci\u00f3n de columnas","text":"<p>Se identifican las columnas num\u00e9ricas, de texto y de fecha en cada tabla dentro de <code>df_structure</code>. Las columnas de fecha, cuyo nombre comienza con \"fec\", se convierten a formato <code>datetime</code>. Adem\u00e1s, se crean las columnas <code>periodoAfi</code> y <code>periodoRet</code> basadas en las columnas de fechas de afiliaci\u00f3n (<code>fecafi</code>) y retiro (<code>fecret</code>), formateando los periodos como <code>A\u00f1oMes</code>. Si alguna de estas columnas no se encuentra, se registra una advertencia en el log. El tiempo total del proceso de transformaci\u00f3n se registra en el log.</p> <pre><code># Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    # Obtener columnas num\u00e9ricas\n    numericColumns[ky] = df_structure[ky].select_dtypes(include=numerics).columns.tolist()\n\n    # Obtener columnas de fecha\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec')]\n\n    # Obtener columnas de texto\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in (numericColumns[ky] + datesColumns[ky])] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('cod')] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('ced')] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('num')]\n\n    textColumns[ky] = list(set(textColumns[ky]))\n\n    # Convertir columnas de fecha a datetime\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n\n    # Crear nuevas columnas 'periodoAfi' y 'periodoRet'\n    if 'fecafi' in df_structure[ky].columns and 'fecret' in df_structure[ky].columns:\n        df_structure[ky]['periodoAfi'] = (df_structure[ky]['fecafi'].dt.year * 100 + df_structure[ky]['fecafi'].dt.month).fillna(188001).astype(int).astype(str)\n        df_structure[ky]['periodoRet'] = (df_structure[ky]['fecret'].dt.year * 100 + df_structure[ky]['fecret'].dt.month).fillna(300001).astype(int).astype(str)\n    else:\n        logger.warning(f'Columnas \"fecafi\" y/o \"fecret\" no encontradas en la tabla para {ky}.')\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:35,141 - WARNING - Columnas \"fecafi\" y/o \"fecret\" no encontradas en la tabla para fact_entrega.\n2024-10-13 12:10:35,143 - INFO - TRANSFORMACION 2 --- 0.02 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#tratamiento-de-errores-afiliacion-mayor-que-retiro","title":"Tratamiento de errores: afiliaci\u00f3n mayor que retiro","text":"<p>Se filtran los registros en cada tabla de <code>df_structure</code> donde el periodo de afiliaci\u00f3n (<code>periodoAfi</code>) es menor o igual al periodo de retiro (<code>periodoRet</code>). Si las columnas <code>periodoAfi</code> o <code>periodoRet</code> no est\u00e1n presentes en alguna tabla, se genera una advertencia en el log. El tiempo total de este proceso de correcci\u00f3n de errores se registra en el log.</p> <pre><code># Tratamiento de errores afi &gt; ret\ntrataerro_time = time.time()\nfor ky in list(df_structure.keys()):\n    # Filtrar filas donde periodoAfi es menor o igual a periodoRet\n    if 'periodoAfi' in df_structure[ky].columns and 'periodoRet' in df_structure[ky].columns:\n        df_structure[ky] = df_structure[ky][df_structure[ky]['periodoAfi'] &lt;= df_structure[ky]['periodoRet']]\n    else:\n        logger.warning(f'Columnas \"periodoAfi\" y/o \"periodoRet\" no encontradas en la tabla para {ky}.')\n\nlogger.info(f'TRATAMIENTO ERRORES --- {time.time() - trataerro_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:35,156 - WARNING - Columnas \"periodoAfi\" y/o \"periodoRet\" no encontradas en la tabla para fact_entrega.\n2024-10-13 12:10:35,161 - INFO - TRATAMIENTO ERRORES --- 0.00 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#limpieza-de-texto-y-eliminacion-de-nan","title":"Limpieza de texto y eliminaci\u00f3n de NaN","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las cadenas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code>. El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code># Limpieza de texto y eliminaci\u00f3n de NaN\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.upper())\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.strip())\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_6048\\3248070240.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n2024-10-13 12:10:35,812 - INFO - LIMPIEZA --- 0.63 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#creacion-de-tabla-de-calendario-y-cross-join","title":"Creaci\u00f3n de tabla de calendario y <code>Cross Join</code>","text":"<p>Se genera un dataframe de calendario que contiene periodos mensuales de los \u00faltimos 18 meses. Luego, se realiza un <code>cross join</code> entre el dataframe <code>df_struc_Total</code> (que contiene los datos de la consulta <code>fact_entrega</code>) y el calendario utilizando una clave auxiliar <code>kp</code>. Este proceso permite combinar todos los periodos con los registros existentes. El tiempo total de la operaci\u00f3n se registra en el log.</p> <pre><code># Concatenaci\u00f3n de tablas (si hubiese m\u00e1s de una tabla, en este caso se omitir\u00e1 ya que solo tenemos subsi11)\n# df_struc_Total = pd.concat(list(df_structure.values()), ignore_index=True)\n\n# Creaci\u00f3n tabla calendario y cross join\ncross_time = time.time()\ndfCalendar = pd.DataFrame(pd.date_range(date.today() - relativedelta(months=18), date.today(), freq='MS').tolist(), columns=['Fechas'])\ndfCalendar['Periodo'] = (dfCalendar['Fechas'].dt.year * 100 + dfCalendar['Fechas'].dt.month).astype(str)\ndfCalendar = dfCalendar[['Periodo']]\n\n# Cross join\ndf_struc_Total = df_structure['fact_entrega']\ndf_struc_Total['kp'] = 0\ndfCalendar['kp'] = 0\nFactEntregaTotal = df_struc_Total.merge(dfCalendar, on='kp', how='outer')\n\nlogger.info(f'TRANSFORMACION 3 CROSS JOIN --- {time.time() - cross_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:36,184 - INFO - TRANSFORMACION 3 CROSS JOIN --- 0.36 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#creacion-de-la-columna-entrega-y-organizacion-de-columnas","title":"Creaci\u00f3n de la columna <code>Entrega</code> y organizaci\u00f3n de columnas","text":"<p>Se verifica si la columna <code>periodoRet</code> est\u00e1 presente en el dataframe <code>FactEntregaTotal</code>. Si existe, se crea la columna <code>Entrega</code> indicando <code>'si'</code> si el periodo actual est\u00e1 dentro del periodo de retiro, y <code>'no'</code> en caso contrario. Si no est\u00e1 presente la columna <code>periodoRet</code>, se asigna <code>'no'</code> por defecto. Luego, se eliminan las columnas innecesarias (como <code>kp</code>) y se reorganizan las columnas colocando <code>Periodo</code> en la primera posici\u00f3n y <code>Entrega</code> en la tercera.</p> <pre><code># Verificar que 'periodoRet' existe en el DataFrame\nif 'periodoRet' in FactEntregaTotal.columns:\n    # Creaci\u00f3n de la columna 'Entrega'\n    FactEntregaTotal['Entrega'] = np.where(\n        (FactEntregaTotal['Periodo'] &lt;= FactEntregaTotal['periodoRet']),\n        'si', 'no'\n    )\nelse:\n    logger.warning(\"'periodoRet' no encontrado en FactEntregaTotal.\")\n    # Si 'periodoRet' no est\u00e1 disponible, podr\u00edas definir una l\u00f3gica alternativa o asignar 'no' por defecto\n    FactEntregaTotal['Entrega'] = 'no'\n\n# Eliminar columnas innecesarias\nFactEntregaTotal = FactEntregaTotal.drop(['kp'], axis=1)\n\n# Organizar Columnas\nf_column = FactEntregaTotal.pop('Periodo')\nFactEntregaTotal.insert(0, 'Periodo', f_column)\nf_column = FactEntregaTotal.pop('Entrega')\nFactEntregaTotal.insert(2, 'Entrega', f_column)\n</code></pre> <pre><code>2024-10-13 12:10:36,196 - WARNING - 'periodoRet' no encontrado en FactEntregaTotal.\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 12:10:36,343 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/2.2-FactEntrega/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>FactEntregaTotal</code> en la tabla <code>BD_FactEntrega</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>guardar_en_dwh(FactEntregaTotal, 'BD_FactEntrega', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:10:36,354 - INFO - CONEXION A BASE DWH\n2024-10-13 12:10:36,852 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactEntrega\n2024-10-13 12:11:51,844 - INFO - Tabla almacenada correctamente.\n2024-10-13 12:11:52,636 - INFO - ALMACENAMIENTO --- 76.28 seconds ---\n</code></pre> <pre><code>FactEntregaTotal.columns.tolist()\n</code></pre> <pre><code>['Periodo',\n 'id',\n 'Entrega',\n 'f_anu',\n 'f_asig_cuota',\n 'f_disp_benef',\n 'f_consign',\n 'f_cheque',\n 'f_fosfec',\n 'val_credito',\n 'val_ajuste',\n 'doc_contable',\n 'f_envio',\n 'f_conciliac',\n 'f_rechazo',\n 'f_prescrip',\n 'f_interfaz',\n 'f_reemplazo',\n 'val_cuota',\n 'num_memorando',\n 'doc_conciliac',\n 'doc_prescrip']\n</code></pre> <pre><code># Iniciar la agregaci\u00f3n\ntagreg_start_time = time.time()\nlogger.info('INICIO DE AGREGACI\u00d3N DE DATOS')\n\n# Agrupar FactEntregaTotal\n# Se agrupa por 'Periodo' y 'id' para reducir la cantidad de registros finales\n# Se agregan columnas clave seg\u00fan las instrucciones proporcionadas\nFactEntregaTotal['Periodo'] = FactEntregaTotal['Periodo'].astype(str).str[:6]  # Extraer el formato yyyymm\n#Comentar la siguiente linea en caso de necesitar continuar con la agregacion original y solo estructurar las columnas\n#FactEntregaTotal['Periodo'] = (FactEntregaTotal['Periodo'].astype(int) // 3 * 3).astype(str)  # Agrupar cada 3 meses\nFactEntregaAgg = FactEntregaTotal.groupby(['id','Periodo'], as_index=False).agg({\n    'Periodo':'last',\n    'id':'last',\n    'Entrega':'count',\n    'f_anu':'last',\n    'f_asig_cuota':'last',\n    'f_disp_benef':'last',\n    'f_consign':'count',\n    'f_cheque':'count',\n    'f_fosfec':'last',\n    'val_credito':'sum',\n    'val_ajuste':'sum',\n    'doc_contable':'last',\n    'f_envio':'last',\n    'f_conciliac':'last',\n    'f_rechazo':'last',\n    'f_prescrip':'last',\n    'f_interfaz':'last',\n    'f_reemplazo':'last',\n    'val_cuota':'last',\n    'num_memorando':'count',\n    'doc_conciliac':'last',\n    'doc_prescrip':'last'\n})\n\n# Asignar el DataFrame agrupado a FactEntregaTotal para la siguiente etapa\n#FactEntregaTotal = FactEntregaAgg\n\n\n# Finalizar la agregaci\u00f3n\nlogger.info(f'AGREGACI\u00d3N FINALIZADA --- {time.time() - tagreg_start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:11:52,655 - INFO - INICIO DE AGREGACI\u00d3N DE DATOS\n2024-10-13 12:11:53,719 - INFO - AGREGACI\u00d3N FINALIZADA --- 1.06 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/","title":"2.3-FactSubsidioVivienda","text":""},{"location":"seccion/2.3-FactSubsidioVivienda/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 12:13\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>SubsidioVivienda.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code># Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='SubsidioVivienda.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 12:13:18,582 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un conjunto de consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de diferentes tablas de la base de datos <code>vivienda</code>. Cada consulta selecciona columnas clave relacionadas con postulaciones, beneficiarios, informaci\u00f3n financiera, y resultados de solicitud de subsidio. El diccionario <code>dim_names</code> se utiliza para mapear el nombre de cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"subvi02\":'''SELECT \n                CONCAT(coddoc, cedpos) as id,\n                documento,numrad,cedpos,coddoc,direccion,\n                codciu,telefono,celular,codzon,email,fecnac,sexo,estciv,nit,codsuc,telemp,\n                email_empre,codact,salario,porapo,tippob,tippos,discap,cabhog,numasi,tipmad,\n                prioridad,estado,fecest,fecdig,usuario,periodo,fecmod,usumod,modsol,fecact,nota\n                FROM vivienda.subvi02''',\n    \"subvi03\":'''SELECT \n                documento,modsol,tippro,coddep,codciu,tipviv,valviv,\n                valaho,valces,fecter,totapo,fecini,totsub,totfin\n                FROM vivienda.subvi03''',\n    \"subvi04\":'''SELECT\n                numpos,documento,priape,segape,prinom,segnom,fecnac,numdoc,\n                coddoc,sexo,parent,reemplazo,estciv,discap,codocu,ingres\n                FROM vivienda.subvi04''',\n    \"subvi05\":'''select \n                documento,credito,cueaho,codban,ciuban,\n                fecaho,fonces,fecces,fecini,ciufon\n                FROM vivienda.subvi05''',\n    \"subvi06\":'''select \n                documento,periodo,numasi,fecasi,valaju,valsub,nota,\n                estado,fecven,fecpag,fecleg,ruaf,ruasub,feccom\n                FROM vivienda.subvi06'''\n               }\ndim_names = {\n    \"subvi02\":'BD_Datos_Postulacion',\n    \"subvi03\":'BD_Detalle_Postulacion',\n    \"subvi04\":'BD_Beneficiario_Postulacion',\n    \"subvi05\":'BD_Financiero_Postulacion',\n    \"subvi06\":'BD_Resultado_Solicitud_Subsidio'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:13:18,605 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:13:18,649 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:13:21,312 - INFO - Cargando subvi02 \n2024-10-13 12:13:33,944 - INFO - Cargada subvi02 --- 12.63 seconds ---\n2024-10-13 12:13:33,945 - INFO - Cargando subvi03 \n2024-10-13 12:13:35,751 - INFO - Cargada subvi03 --- 1.81 seconds ---\n2024-10-13 12:13:35,753 - INFO - Cargando subvi04 \n2024-10-13 12:13:36,504 - INFO - Cargada subvi04 --- 0.75 seconds ---\n2024-10-13 12:13:36,507 - INFO - Cargando subvi05 \n2024-10-13 12:13:36,747 - INFO - Cargada subvi05 --- 0.24 seconds ---\n2024-10-13 12:13:36,749 - INFO - Cargando subvi06 \n2024-10-13 12:13:37,016 - INFO - Cargada subvi06 --- 0.27 seconds ---\n2024-10-13 12:13:37,100 - INFO - CARGUE TABLAS DESDE MYSQL --- 18.45 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la presencia de registros duplicados en todas las tablas del diccionario <code>df_structure</code>. Las columnas utilizadas para comparar duplicados excluyen la columna <code>id</code>. Los duplicados detectados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:37,261 - INFO - VALIDADOR TABLA: subvi02\n2024-10-13 12:13:37,307 - INFO - VALIDADOR TABLA: subvi03\n2024-10-13 12:13:37,403 - INFO - VALIDADOR TABLA: subvi04\n2024-10-13 12:13:37,443 - INFO - VALIDADOR TABLA: subvi05\n2024-10-13 12:13:37,486 - INFO - VALIDADOR TABLA: subvi06\n2024-10-13 12:13:37,487 - INFO - VALIDADOR DUPLICADOS --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla dentro de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total de la operaci\u00f3n se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:38,477 - INFO - LIMPIEZA --- 0.98 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 12:13:38,489 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. Si las tablas ya existen en la base de datos, se reemplazan con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:13:38,503 - INFO - CONEXION A BASE DWH\n2024-10-13 12:13:39,027 - INFO - Almacenando tabla subvi02 en DWH como BD_Datos_Postulacion\n2024-10-13 12:13:43,943 - INFO - Tabla subvi02 almacenada correctamente.\n2024-10-13 12:13:43,944 - INFO - Almacenando tabla subvi03 en DWH como BD_Detalle_Postulacion\n2024-10-13 12:13:46,613 - INFO - Tabla subvi03 almacenada correctamente.\n2024-10-13 12:13:46,615 - INFO - Almacenando tabla subvi04 en DWH como BD_Beneficiario_Postulacion\n2024-10-13 12:13:50,163 - INFO - Tabla subvi04 almacenada correctamente.\n2024-10-13 12:13:50,165 - INFO - Almacenando tabla subvi05 en DWH como BD_Financiero_Postulacion\n2024-10-13 12:13:52,874 - INFO - Tabla subvi05 almacenada correctamente.\n2024-10-13 12:13:52,876 - INFO - Almacenando tabla subvi06 en DWH como BD_Resultado_Solicitud_Subsidio\n2024-10-13 12:13:54,956 - INFO - Tabla subvi06 almacenada correctamente.\n2024-10-13 12:13:55,062 - INFO - ALMACENAMIENTO --- 16.56 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:55,073 - INFO - FINAL ETL --- 36.52 seconds ---\n</code></pre> <pre><code># \ntabla_consulta = 'subvi06'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['documento', 'periodo']).agg({\n    'numasi': 'first',  # Mantener el primer valor de asignaci\u00f3n\n    'fecasi': 'first',  # Fecha de asignaci\u00f3n m\u00e1s temprana\n    'valaju': 'sum',  # Sumar valores ajustados si es que cambian en el mismo periodo\n    'valsub': 'sum',  # Sumar valores del subsidio\n    'nota': 'last',  # Mantener la \u00faltima nota registrada\n    'estado': 'last',  # \u00daltimo estado registrado\n    'fecven': 'last',  # \u00daltima fecha de vencimiento registrada\n    'fecpag': 'last',  # \u00daltima fecha de pago registrada\n    'fecleg': 'last',  # \u00daltima fecha de legalizaci\u00f3n\n    'ruaf': 'last',  # \u00daltimo valor de ruaf\n    'ruasub': 'count',  \n    'feccom': 'last'  # \u00daltima fecha de comentario registrada\n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>BD_Resultado_Solicitud_Subsidio\nNumero de registros 4174\n\n\n\n\n\n(4174, 4174)\n</code></pre> <pre><code># \ntabla_consulta = 'subvi05'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Financiero_Postulacion\nNumero de registros 10046\n\n\n\n\n\n['documento',\n 'credito',\n 'cueaho',\n 'codban',\n 'ciuban',\n 'fecaho',\n 'fonces',\n 'fecces',\n 'fecini',\n 'ciufon']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi04'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Beneficiario_Postulacion\nNumero de registros 22172\n\n\n\n\n\n['numpos',\n 'documento',\n 'priape',\n 'segape',\n 'prinom',\n 'segnom',\n 'fecnac',\n 'numdoc',\n 'coddoc',\n 'sexo',\n 'parent',\n 'reemplazo',\n 'estciv',\n 'discap',\n 'codocu',\n 'ingres']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi03'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'No existen campos para agrupar, numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Detalle_Postulacion\nNo existen campos para agrupar, numero de registros 12793\n\n\n\n\n\n['documento',\n 'modsol',\n 'tippro',\n 'coddep',\n 'codciu',\n 'tipviv',\n 'valviv',\n 'valaho',\n 'valces',\n 'fecter',\n 'totapo',\n 'fecini',\n 'totsub',\n 'totfin']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi02'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\ntabla.columns.tolist()\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['documento', 'periodo']).agg({\n    'salario': 'last',  # Promedio de salario si es que cambia\n    'prioridad': 'count',  # Promedio de prioridad si tiene m\u00faltiples registros\n    'estado': 'last',  # Conservamos el \u00faltimo estado registrado\n    'fecmod': 'last',  # \u00daltima fecha de modificaci\u00f3n\n    'codciu': 'last',  # Primera ciudad asociada\n    'telefono': 'last',  # El primer n\u00famero de tel\u00e9fono registrado\n    'email': 'last',  # El primer email registrado\n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>BD_Datos_Postulacion\n\n\n\n\n\n(13131, 13131)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/2.4-CrearFactFosfec/","title":"2.4-CrearFactFosfec","text":""},{"location":"seccion/2.4-CrearFactFosfec/#importacion-de-bibliotecas-y-configuracion-inicial","title":"Importaci\u00f3n de bibliotecas y configuraci\u00f3n inicial","text":"<p>Se importan las bibliotecas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>pymysql</code>, y <code>dateutil</code>. Adem\u00e1s, se importa un m\u00f3dulo de funciones personalizadas desde un archivo externo <code>Funciones.py</code>, incluyendo funciones como <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, <code>RemoveErrors</code>, <code>Conexion_Minerva</code>, y <code>Conexion_dwh</code>. Tambi\u00e9n se configura el logger para registrar eventos en un archivo <code>FactFosFec.log</code>, y se inicia el proceso ETL, registrando el evento de inicio en el log.</p> <pre><code># Importar bibliotecas necesarias\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport logging\n\n\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, cargar_tablas, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n\n# Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='FactFosFec.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 12:15:50,105 - INFO - COMIENZO ETL\n\n\nImportacion de funciones correcta, 13-10-2024 12:15\n</code></pre>"},{"location":"seccion/2.4-CrearFactFosfec/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL en el diccionario <code>qr_structure</code> que extrae datos de las tablas <code>fosfec160</code>, <code>fosfec07</code>, <code>fosfec09</code>, y <code>fosfec21</code> de la base de datos <code>fosfec</code>, utilizando varias combinaciones de <code>INNER JOIN</code> y <code>LEFT JOIN</code>. La consulta selecciona columnas clave relacionadas con fechas, documentos y salarios. El proceso de lectura de consultas se registra en el log.</p> <pre><code># Consultas SQL\n\nqr_structure = {\n    \"fact_foscec\": '''\n    SELECT \n        fosfec160.id,\n        fosfec160.cedtra,\n        fosfec160.tipdoc,\n        fosfec160.fecexp,\n        fosfec160.fecsis,\n        fosfec07.fecasi,\n        fosfec07.fecest,\n        fosfec09.fecnac,\n        fosfec160.fecfin,\n        fosfec09.fecdig,\n        fosfec09.fecsal,\n        fosfec160.fecenv,\n        fosfec160.fecterm,\n        fosfec160.salario,\n        fosfec09.ultsal,\n        fosfec160.bonopen,\n        fosfec07.recsub,\n        fosfec07.cuosub,\n        fosfec160.numdaviplata,\n        fosfec07.documento,\n        fosfec160.numres,\n        fosfec160.codcat,\n        fosfec160.codpen,\n        fosfec07.codces,\n        fosfec160.codcaj\n    FROM fosfec.fosfec160 \n    INNER JOIN fosfec.fosfec07 \n        ON CONVERT(fosfec.fosfec160.cedtra USING utf8) COLLATE utf8_unicode_ci = CONVERT(fosfec.fosfec07.cedtra USING utf8) COLLATE utf8_unicode_ci \n    INNER JOIN fosfec.fosfec09 \n        ON fosfec.fosfec07.cedtra = fosfec.fosfec09.cedtra \n    LEFT JOIN fosfec.fosfec21 \n        ON CONVERT(fosfec.fosfec21.cedcon USING utf8) COLLATE utf8_unicode_ci = fosfec.fosfec160.cedtra\n    '''\n}\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:15:50,127 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.4-CrearFactFosfec/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:15:50,282 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:15:50,815 - INFO - Cargando fact_foscec \n2024-10-13 12:20:07,867 - INFO - Cargada fact_foscec --- 257.05 seconds ---\n2024-10-13 12:20:07,954 - INFO - CARGUE TABLAS DESDE MYSQL --- 257.67 seconds ---\n</code></pre>"},{"location":"seccion/2.4-CrearFactFosfec/#definicion-de-funciones-de-transformacion-y-limpieza","title":"Definici\u00f3n de funciones de transformaci\u00f3n y limpieza","text":"<p>Se define el identificador \u00fanico <code>columId</code> como <code>'id'</code>, y se crea una lista llamada <code>ColumnsToCompare</code> que contiene las columnas clave del dataframe que se utilizar\u00e1n en los procesos de transformaci\u00f3n y limpieza. Estas columnas incluyen datos sobre fechas, salarios, documentos, y c\u00f3digos. Esta lista es esencial para las operaciones posteriores, como la identificaci\u00f3n de duplicados y la validaci\u00f3n de datos.</p> <pre><code># Definici\u00f3n de funciones de transformaci\u00f3n y limpieza\ncolumId = 'id'  # Ajusta seg\u00fan tu identificador \u00fanico\n\n# Lista actualizada de columnas que cumplen con las restricciones\nColumnsToCompare = [\n    'id', 'cedtra', 'tipdoc', 'fecexp', 'fecsis', 'fecasi', 'fecest', 'fecnac', \n    'fecfin', 'fecdig', 'fecsal', 'fecenv', 'fecterm', 'salario', 'ultsal', \n    'bonopen', 'recsub', 'cuosub', 'numdaviplata', 'documento', 'numres', \n    'codcat', 'codpen', 'codces', 'codcaj'\n]\n</code></pre>"},{"location":"seccion/2.4-CrearFactFosfec/#proceso-de-limpieza-y-validacion-de-duplicados","title":"Proceso de limpieza y validaci\u00f3n de duplicados","text":"<p>Se itera sobre los dataframes en <code>df_structure</code>, guardando los duplicados mediante la funci\u00f3n <code>StoreDuplicated</code> usando las columnas especificadas en <code>ColumnsToCompare</code>. Si la columna <code>f_disp_benef</code> est\u00e1 presente, se eliminan duplicados en funci\u00f3n de esa fecha; en caso contrario, se omite esa operaci\u00f3n y se registra una advertencia en el log. Posteriormente, se limpia el dataframe de errores utilizando la funci\u00f3n <code>RemoveErrors</code>. Cada tabla procesada se registra en el log al completar la validaci\u00f3n y limpieza de duplicados. </p> <pre><code># Iterar sobre los DataFrames para realizar transformaciones\nfor key, df in df_structure.items():\n    print(f\"Procesando tabla: {key}\")\n\n    # Guardar duplicados\n    StoreDuplicated('N/A', ColumnsToCompare, df, 'duplicados_fact_fosfec_' + key)\n\n    # Eliminar duplicados por fecha (aseg\u00farate de tener una columna de fecha adecuada en tu DataFrame)\n    if 'f_disp_benef' in df.columns:\n        df_cleaned = RemoveDuplicated(columId, 'f_disp_benef', df)\n    else:\n        logger.warning(f'Columna de fecha \"f_disp_benef\" no encontrada en {key}. Se omite la eliminaci\u00f3n de duplicados por fecha.')\n        df_cleaned = df\n\n    # Limpiar errores (si fuera aplicable a tu contexto)\n    df_final = RemoveErrors(df_cleaned, 'errores_fact_foscec_' + key)\n\n    logger.info('Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: ' + key)\n</code></pre> <pre><code>2024-10-13 12:20:08,077 - WARNING - Columna de fecha \"f_disp_benef\" no encontrada en fact_foscec. Se omite la eliminaci\u00f3n de duplicados por fecha.\n2024-10-13 12:20:08,079 - INFO - Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: fact_foscec\n\n\nProcesando tabla: fact_foscec\n</code></pre> <pre><code>df_structure['fact_foscec']\n</code></pre> id cedtra tipdoc fecexp fecsis fecasi fecest fecnac fecfin fecdig ... bonopen recsub cuosub numdaviplata documento numres codcat codpen codces codcaj 0 31914 1004271963 CC 2012-11-21 2024-04-09 2024-05-16 2024-04-10 1994-01-09 None 2024-04-09 ... N S 2 3005770663 0023812 1 B 230201 None None 1 25102 1004344943 CC 2005-10-24 2022-12-03 None 2022-12-16 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017310 1 A 230301 None None 2 25102 1004344943 CC 2005-10-24 2022-12-03 2023-02-15 2023-01-11 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017493 1 A 230301 None None 3 25285 1004344943 CC 2005-10-24 2023-01-05 None 2022-12-16 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017310 1 A 230301 None None 4 25285 1004344943 CC 2005-10-24 2023-01-05 2023-02-15 2023-01-11 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017493 1 A 230301 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17118 34281 19604571 CC 2004-03-01 2024-10-04 None 2022-10-10 1985-01-25 None 2022-09-10 ... N S 4 3128019398 0016479 1 None 230301 None None 17119 34285 1082968559 CC 2011-06-21 2024-10-08 None 2024-09-17 1993-04-03 None 2024-09-10 ... N S 2 3242483705 0025692 1 None 230301 None None 17120 34287 1081786023 CC 2004-07-09 2024-10-08 2017-04-20 2020-02-06 1986-05-18 None 2018-06-13 ... N S 2 3113203794 0005686 1 None 230301 SINAFP None 17121 34293 1082841439 CC 2004-07-09 2024-10-10 None 2023-10-03 1983-02-21 None 2024-02-01 ... N S 1 3187737349 0020743 1 None 231001 None 07 17122 34293 1082841439 CC 2004-07-09 2024-10-10 None 2024-03-11 1983-02-21 None 2024-02-01 ... N S 1 3187737349 0022497 1 None 231001 None 07 <p>17123 rows \u00d7 25 columns</p>"},{"location":"seccion/2.4-CrearFactFosfec/#guardar-en-la-base-de-datos-dwh","title":"Guardar en la base de datos DWH","text":"<p>Se guarda el dataframe <code>fact_foscec</code> en la tabla <code>BD_FactFosfec</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log, seguido del registro de finalizaci\u00f3n del proceso ETL, incluyendo el tiempo total desde el inicio del proceso.</p> <pre><code>guardar_en_dwh(df_structure['fact_foscec'], 'BD_FactFosfec', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:21:05,422 - INFO - CONEXION A BASE DWH\n2024-10-13 12:21:05,934 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactFosfec\n2024-10-13 12:21:11,117 - INFO - Tabla almacenada correctamente.\n2024-10-13 12:21:11,227 - INFO - ALMACENAMIENTO --- 5.80 seconds ---\n</code></pre> <pre><code># Tiempo total de ejecuci\u00f3n\nlogger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 23:45:03,410 - INFO - FINAL ETL --- 243.36 seconds ---\n</code></pre> <pre><code># \ntabla = df_structure['fact_foscec']\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['fecasi', 'documento']).agg({\n    'cedtra': 'last',  \n    'tipdoc': 'last',  \n    'fecest': 'last',  \n    'salario': 'last', \n    'ultsal': 'last', \n    'bonopen': 'last', \n    'recsub': 'last',  \n    'cuosub': 'sum',   \n    'numdaviplata': 'last',  \n    'codcat': 'last',  \n    'codpen': 'last',  \n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>Numero de registros 17123\n\n\n\n\n\n(17123, 9460)\n</code></pre> <pre><code>df_grouped\n</code></pre> fecasi documento cedtra tipdoc fecest salario ultsal bonopen recsub cuosub numdaviplata codcat codpen 0 2014-07-24 0000002 85473356 CC None 1.560.000 737717 None S 2 NO TENGO A 230301 1 2014-07-24 0000003 39032364 CC None 720.000 781242 None N 0 3152117593 A 25-14 2 2014-07-24 0000021 85473678 CC None 877803 1129093 None S 1 3013570721 A 231001 3 2014-07-24 0000024 39046011 CC None 3328000 1284000 None N 0 3175011658 B 230301 4 2014-07-24 0000030 7629151 CC None 1300000 1300000 N N 0 3103527803 A 230201 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9455 2024-09-18 0025288 1004272164 CC 2024-08-26 13000000 1300000 N N 0 3243202882 A 230301 9456 2024-09-18 0025289 57467186 CC 2024-09-25 1950000 3300000 N S 1 3182112369 B 230201 9457 2024-09-18 0025290 1128105347 CC 2024-08-27 1300000 1300000 N S 2 3145862495 A 231001 9458 2024-09-18 0025294 36667152 CC None 1300000 1300000 N N 0 3238985219 A 25-14 9459 2024-09-18 0025295 1082977918 CC None 1442158 1442158 N N 0 3008801927 A 230201 <p>9460 rows \u00d7 13 columns</p>"},{"location":"seccion/2.4-CrearFactFosfecold/","title":"2.4-CrearFactFosfecold","text":"<pre><code># Importar bibliotecas necesarias\nimport sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport pymysql\nimport time\nimport logging\n\n# Configuraci\u00f3n inicial\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='FactFosFec.log', encoding='utf-8', level=logging.DEBUG, format='%(asctime)s %(message)s')\nlogging.info('COMIENZO ETL')\nstart_time = time.time()\n\n# Conexion a base Minerva\nusuario = 'qcs'\ncontrase\u00f1a = 'asK/&amp;\"j,OLS187*'\nhost = '172.16.2.111'\ncadena_conexion = f\"mysql+pymysql://{usuario}:{contrase\u00f1a}@{host}\"\nmotor = create_engine(cadena_conexion)\nlogging.info('CONEXION A BASE MINERVA')\n\n# Consultas SQL\nqr_structure = {\n    \"fact_entrega\": f\"SELECT * FROM fosfec.fosfec160 INNER JOIN fosfec.fosfec07 ON CONVERT(fosfec.fosfec160.cedtra USING utf8) COLLATE utf8_unicode_ci = CONVERT(fosfec.fosfec07.cedtra USING utf8) COLLATE utf8_unicode_ci INNER JOIN fosfec.fosfec09 ON fosfec.fosfec07.cedtra = fosfec.fosfec09.cedtra LEFT JOIN fosfec.fosfec21 ON CONVERT(fosfec.fosfec21.cedcon USING utf8) COLLATE utf8_unicode_ci = fosfec.fosfec160.cedtra\"\n}\n\n\ndf_structure = dict()\nlogging.info('LECTURA DE QUERYS')\n\n# Cargue de tablas desde SQL\ncargue_time = time.time()\nfor ky in list(qr_structure.keys()):\n    try:\n        with motor.begin() as conn:\n            print(ky)\n            df_structure[ky] = pd.read_sql_query(qr_structure[ky], conn)\n        logging.info('CARGUE TABLA: ' + ky)\n    except Exception as e:\n        logging.error(f'Error al cargar la tabla {ky}: {str(e)}')\n\nlogging.info(f'CARGUE TABLAS DESDE MYSQL --- {time.time() - cargue_time:.2f} seconds ---')\n\n# Realizar las uniones (si es necesario)\n#merged_df = df_structure['fosfec160'].merge(df_structure['fosfec07'], on='cedtra', how='left', suffixes=('', '_07'))\n#merged_df = merged_df.merge(df_structure['fosfec09'], on='cedtra', how='left', suffixes=('', '_09'))\n#merged_df = merged_df.merge(df_structure['fosfec21'], left_on='cedtra', right_on='cedcon', how='left', suffixes=('', '_21'))\n\n#Guardar la tabla final como 'factFosFec' en la base de datos\n#final_df.to_sql('factFosFec', con=motor, if_exists='replace', index=False)\n# Mensaje de confirmaci\u00f3n\nlogging.info(\"Las tablas han sido creadas exitosamente en la base de datos.\")\nprint(\"Las tablas han sido creadas exitosamente en la base de datos.\")\n\n# Tiempo total de ejecuci\u00f3n\nlogging.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>fact_entrega\nLas tablas han sido creadas exitosamente en la base de datos.\n</code></pre> <pre><code>qr_structure\n</code></pre> <pre><code>{'fact_entrega': 'SELECT * FROM fosfec.fosfec160 INNER JOIN fosfec.fosfec07 ON CONVERT(fosfec.fosfec160.cedtra USING utf8) COLLATE utf8_unicode_ci = CONVERT(fosfec.fosfec07.cedtra USING utf8) COLLATE utf8_unicode_ci INNER JOIN fosfec.fosfec09 ON fosfec.fosfec07.cedtra = fosfec.fosfec09.cedtra LEFT JOIN fosfec.fosfec21 ON CONVERT(fosfec.fosfec21.cedcon USING utf8) COLLATE utf8_unicode_ci = fosfec.fosfec160.cedtra'}\n</code></pre> <pre><code>df_structure\n</code></pre> <pre><code>{'fact_entrega':           id      cedtra tipdoc   priape      segape   prinom    segnom  \\\n 0      33092  1002131608     CC    JEREZ   GUTIERREZ    LAURA   VANESSA   \n 1      32313  1003257544     CC    DURAN       DURAN     YENI             \n 2      28165  1004162766     CC  GUILLOT       ROLON   KELLYS    JOHANA   \n 3        613  1004211264     CC    ACU\u00d1A      DAVILA   ALVARO    JAVIER   \n 4      31914  1004271963     CC  VELASCO   BALMACEDA    SIXTA    DAYANA   \n ...      ...         ...    ...      ...         ...      ...       ...   \n 16678  33886    85454819     CC    GOMEZ     SANTANA    CESAR   AUGUSTO   \n 16679  33887    19591838     CC     JOSE  DEL CARMEN  CAMACHO      LARA   \n 16680  33889    36695663     CC    OLMOS    MARTINEZ     IRIS    LAURID   \n 16681  33890  1193544314     CC    NU\u00d1EZ     HUELVAS    LAURA   VANESSA   \n 16682  33891  1004381604     CC   CASTRO       EGUIS  ORLANDO  DE JESUS\n\n                     barrio ciudad    telefono  ...    telefono nivedu  \\\n 0      RESERVAS DE CURINCA  47001  3028369038  ...        None      1   \n 1             LOS LAURELES  47001  3005750230  ...  3132853204      1   \n 2           BRISAS DEL RIO  47980  3145862189  ...        None      1   \n 3           VILLA DEL RIOS  47001  3003161050  ...        None      1   \n 4           BRISAS DEL RIO  47288  3005770663  ...        None      1   \n ...                    ...    ...         ...  ...         ...    ...   \n 16678             PORVENIR  47001     1000000  ...        None   None   \n 16679          23 DE ABRIL  47189  3206898499  ...        None   None   \n 16680                YUCAL  47001  3243827150  ...        None   None   \n 16681              CHIMILA  47288  3004132837  ...        None   None   \n 16682           EL POBLADO  47189      999999  ...        None   None\n\n            fecnac codsex estciv estado fecest    salario fescal ocupacion  \n 0      1999-03-14      F      5      A   None  1200000.0   None      None  \n 1      1990-02-06      F      5      A   None        0.0   None      None  \n 2      1992-08-20      F      5      A   None  1160000.0   None      None  \n 3      1989-09-13      M      5      A   None   781242.0   None      None  \n 4      1994-01-09      F      1      A   None        0.0   None      None  \n ...           ...    ...    ...    ...    ...        ...    ...       ...  \n 16678        None   None   None   None   None        NaN   None      None  \n 16679        None   None   None   None   None        NaN   None      None  \n 16680        None   None   None   None   None        NaN   None      None  \n 16681        None   None   None   None   None        NaN   None      None  \n 16682        None   None   None   None   None        NaN   None      None\n\n [16683 rows x 148 columns]}\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/","title":"3.1-FactTransaccionesVentas","text":""},{"location":"seccion/3.1-FactTransaccionesVentas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger, guardar_en_dwh\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 18:38\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>TransaccionesVentas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='TransaccionesVentas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 18:38:20,779 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se definen dos consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de las tablas <code>servi20</code> y <code>subvi21</code> de la base de datos <code>servicios</code>. Las consultas seleccionan varias columnas clave relacionadas con ventas y coberturas. El diccionario <code>dim_names</code> se utiliza para mapear cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"servi20\":'''select \n                marca,documento,usuario,fecha,hora,codzon,codsed,codfue,codase,cedtra,nomtra,cedcon,\n                nomcon,nit,razsoc,direccion,telefono,email,credito,abono,estado,fecest,nota,usuabo,\n                codcen,ent,usuent\n                from servicios.servi20''',\n    \"subvi21\":'''select \n                marca,documento,fecha,hora,codzon,codsed,codfue,codase,cedtra,nomtra,cedcon,\n                nomcon,nit,razsoc,direccion,telefono,email,credito,abono,estado,fecest,nota,usuabo,\n                codcen,ent,usuent\n                from servicios.servi20'''\n               }\ndim_names = {\n    \"servi20\":'BD_Cobertura_Ventas_Servicios',\n    \"subvi21\":'BD_Detalle_Cobertura_Ventas_Servicios'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 18:38:20,797 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y se cargan los resultados en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 18:38:21,716 - INFO - CONEXION A BASE MINERVA\n2024-10-13 18:38:22,189 - INFO - Cargando servi20 \n2024-10-13 18:40:10,312 - INFO - Cargada servi20 --- 108.12 seconds ---\n2024-10-13 18:40:10,315 - INFO - Cargando subvi21 \n2024-10-13 18:41:18,677 - INFO - Cargada subvi21 --- 68.36 seconds ---\n2024-10-13 18:41:19,019 - INFO - CARGUE TABLAS DESDE MYSQL --- 177.30 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas de <code>df_structure</code>. Las columnas a comparar se determinan excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, as\u00ed como el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 18:41:31,126 - INFO - VALIDADOR TABLA: servi20\n2024-10-13 18:41:43,805 - INFO - VALIDADOR TABLA: subvi21\n2024-10-13 18:41:43,808 - INFO - VALIDADOR DUPLICADOS --- 24.78 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final de cada valor, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code>. El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 18:42:59,215 - INFO - LIMPIEZA --- 75.40 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH, verificando primero que la clave est\u00e9 en el diccionario <code>dim_names</code> para asignar el nombre de la tabla correspondiente. Si la clave no est\u00e1 presente, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra en el log, junto con los nombres de las tablas almacenadas correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 18:42:59,235 - INFO - CONEXION A BASE DWH\n2024-10-13 18:42:59,714 - INFO - Almacenando tabla servi20 en DWH como BD_Cobertura_Ventas_Servicios\n2024-10-13 18:47:55,534 - INFO - Tabla servi20 almacenada correctamente.\n2024-10-13 18:47:55,536 - INFO - Almacenando tabla subvi21 en DWH como BD_Detalle_Cobertura_Ventas_Servicios\n2024-10-13 18:53:20,380 - INFO - Tabla subvi21 almacenada correctamente.\n2024-10-13 18:53:23,122 - INFO - ALMACENAMIENTO --- 623.89 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 18:53:23,135 - INFO - FINAL ETL --- 902.40 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/4.1-FactColegio/","title":"4.1-FactColegio","text":""},{"location":"seccion/4.1-FactColegio/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 11:09\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Colegio.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Colegio.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 11:09:15,597 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se definen tres consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de las tablas <code>colegio12</code>, <code>colegio13</code>, y <code>colegio15</code> de la base de datos <code>colegio</code>. Cada consulta selecciona columnas clave relacionadas con estudiantes, acudientes y matr\u00edculas. El diccionario <code>dim_names</code> se utiliza para mapear cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"colegio12\":'''select \n                ano,numdoc,coddoc,ciuexp,priape,segape,prinom,segnom,rh,eps,codsex,dirnac,ciunac,fecnac,\n                dirres,barrio,estrato,codciu,telres,email,nota,observacion,docpad,docmad,docacu,codben,codcat,tipo\n                from colegio.colegio12''',\n    \"colegio13\":'''select \n                numdoc,coddoc,ciuexp,priape,segape,prinom,segnom,codpar,profesion,empresa,ocupacion,codsex,\n                direccion,codciu,telefono,email\n                from colegio.colegio13''',\n    \"colegio15\":'''select \n                ano,nummat,numdoc,codgra,codgru,fecmat,colant,numlib,numfol,codcon,codest,codcat,\n                estado,fecest,motivo,usuario,estbol\n                from colegio.colegio15'''\n               }\ndim_names = {\n    \"colegio12\":'BD_Datos_Estudiantes',\n    \"colegio13\":'BD_Datos_Acudientes',\n    \"colegio15\":'BD_Datos_Matriculas'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 11:09:15,612 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n a la base de datos de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 11:09:15,679 - INFO - CONEXION A BASE MINERVA\n2024-10-13 11:09:16,195 - INFO - Cargando colegio12 \n2024-10-13 11:09:16,800 - INFO - Cargada colegio12 --- 0.60 seconds ---\n2024-10-13 11:09:16,802 - INFO - Cargando colegio13 \n2024-10-13 11:09:16,983 - INFO - Cargada colegio13 --- 0.18 seconds ---\n2024-10-13 11:09:16,985 - INFO - Cargando colegio15 \n2024-10-13 11:09:17,167 - INFO - Cargada colegio15 --- 0.18 seconds ---\n2024-10-13 11:09:17,252 - INFO - CARGUE TABLAS DESDE MYSQL --- 1.57 seconds ---\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la presencia de registros duplicados en todas las tablas de <code>df_structure</code>. Se comparan las columnas especificadas excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla utilizando <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,353 - INFO - VALIDADOR TABLA: colegio12\n2024-10-13 11:09:17,381 - INFO - VALIDADOR TABLA: colegio13\n2024-10-13 11:09:17,409 - INFO - VALIDADOR TABLA: colegio15\n2024-10-13 11:09:17,410 - INFO - VALIDADOR DUPLICADOS --- 0.15 seconds ---\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final de cada valor, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code>. El tiempo total de la limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA  --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,538 - INFO - LIMPIEZA  --- 0.12 seconds ---\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 11:09:17,550 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/4.1-FactColegio/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code># Conexi\u00f3n a la base DWH utilizando 'with' para una conexi\u00f3n segura\nalmacenamiento_time = time.time()\nlogger.info('CONEXION A BASE DWH')\n\n# Utilizamos 'with' para asegurar que la conexi\u00f3n se cierre autom\u00e1ticamente\nwith create_engine(obtener_conexion('dwh')).begin() as conn:\n    for ky in df_structure.keys():\n        logger.info(f'Almacenando tabla {ky} en DWH como {dim_names[ky]}')\n        # Guardar cada DataFrame en su tabla correspondiente en la base DWH\n        df_structure[ky].to_sql(name=dim_names[ky], con=conn, if_exists='replace', index=False)\n        logger.info(f'Tabla {ky} almacenada correctamente.')\nlogger.info(f'ALMACENAMIENTO --- {time.time() - almacenamiento_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,563 - INFO - CONEXION A BASE DWH\n2024-10-13 11:09:18,066 - INFO - Almacenando tabla colegio12 en DWH como BD_Datos_Estudiantes\n2024-10-13 11:09:20,287 - INFO - Tabla colegio12 almacenada correctamente.\n2024-10-13 11:09:20,289 - INFO - Almacenando tabla colegio13 en DWH como BD_Datos_Acudientes\n2024-10-13 11:09:21,393 - INFO - Tabla colegio13 almacenada correctamente.\n2024-10-13 11:09:21,395 - INFO - Almacenando tabla colegio15 en DWH como BD_Datos_Matriculas\n2024-10-13 11:09:22,511 - INFO - Tabla colegio15 almacenada correctamente.\n2024-10-13 11:09:22,616 - INFO - ALMACENAMIENTO --- 5.05 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:22,627 - INFO - FINAL ETL --- 7.06 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/","title":"4.1-FactColegio_fijos","text":""},{"location":"seccion/4.1-FactColegio_fijos/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport pymysql\nimport time\nimport logging\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import setup_logger, Conexion_dwh, testfunciones\nprint(testfunciones())\n# Configuraci\u00f3n inicial\ntime_start = time.time()\nlogger = setup_logger(log_filename='Merge_Colegio_DimDatosFijos.log', log_level=logging.INFO)\nlogger.info('INICIO DE PROCESO')\n</code></pre> <pre><code>2024-10-05 10:25:55,575 - INFO - INICIO DE PROCESO\n\n\nImportacion de funciones correcta, 05-10-2024 10:25\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#definicion-de-consultas-sql-con-columnas-requeridas","title":"Definici\u00f3n de consultas SQL con columnas requeridas","text":"<p>Se definen consultas SQL en el diccionario <code>qr_structure</code> para extraer los datos de las tablas <code>BD_Datos_Estudiantes</code> y <code>BD_Datos_Acudientes</code>. Se asegura que las columnas necesarias, como <code>id</code>, <code>fecnac</code>, <code>sexo</code>, <code>coddoc</code>, <code>numdocumento</code>, <code>prinom</code>, <code>segnom</code>, <code>priape</code>, y <code>segape</code>, est\u00e9n presentes en los resultados. Se utiliza la funci\u00f3n <code>create_engine()</code> para conectar a la base de datos DWH. La conexi\u00f3n exitosa se registra en el log.</p> <pre><code># Definir las consultas SQL para cargar datos y asegurarse de crear la estructura con los campos requeridos\nrequired_columns = ['id', 'fecnac', 'sexo', 'coddoc', 'numdocumento', 'prinom', 'segnom', 'priape', 'segape']\nqr_structure = {\n    \"colegio12\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS id,\n        fecnac,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS numdocumento,\n        prinom,\n        segnom,\n        priape,\n        segape\n    FROM BD_Datos_Estudiantes''',\n    \"colegio13\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS id,\n        NULL AS fecnac,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS numdocumento,\n        prinom,\n        segnom,\n        priape,\n        segape\n    FROM BD_Datos_Acudientes'''\n}\n\n# Conectar a la base de datos DWH\nmotor = create_engine(Conexion_dwh())\nlogger.info('CONEXI\u00d3N A BASE DE DATOS DWH ESTABLECIDA')\n</code></pre> <pre><code>2024-10-05 10:26:01,809 - INFO - CONEXI\u00d3N A BASE DE DATOS DWH ESTABLECIDA\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#cargar-y-estructurar-tablas-con-las-columnas-requeridas","title":"Cargar y estructurar tablas con las columnas requeridas","text":"<p>Se cargan las tablas <code>colegio12</code> y <code>colegio13</code> desde la base de datos, garantizando que todas las columnas requeridas est\u00e9n presentes en los dataframes resultantes. Si alguna columna falta, se crea con el valor <code>\"SinDato\"</code>. Posteriormente, se concatenan ambas tablas en un solo dataframe <code>df_colegio_combined</code>. Los eventos de carga y concatenaci\u00f3n se registran en el log.</p> <pre><code># Cargar las tablas en DataFrames y asegurar la estructura requerida\ndf_structure = {}\nfor key, query in qr_structure.items():\n    try:\n        with motor.begin() as conn:\n            logger.info(f'CARGANDO TABLA: {key}')\n            df = pd.read_sql_query(sa.text(query), conn)\n\n            # Asegurarse de que las columnas requeridas est\u00e9n presentes en la estructura\n            for column in required_columns:\n                if column not in df.columns:\n                    df[column] = \"SinDato\"\n\n            df_structure[key] = df[required_columns]\n            logger.info(f'TABLA {key} CARGADA Y ESTRUCTURADA')\n    except Exception as e:\n        logger.error(f'Error al cargar la tabla {key}: {str(e)}')\n\n# Concatenar las tablas de colegio12 y colegio13\ndf_colegio_combined = pd.concat([df_structure['colegio12'], df_structure['colegio13']], ignore_index=True)\nlogger.info('TABLAS colegio12 Y colegio13 COMBINADAS')\n</code></pre> <pre><code>2024-10-05 11:00:40,892 - INFO - CARGANDO TABLA: colegio12\n2024-10-05 11:00:41,178 - INFO - TABLA colegio12 CARGADA Y ESTRUCTURADA\n2024-10-05 11:00:41,256 - INFO - CARGANDO TABLA: colegio13\n2024-10-05 11:00:41,352 - INFO - TABLA colegio13 CARGADA Y ESTRUCTURADA\n2024-10-05 11:00:41,431 - INFO - TABLAS colegio12 Y colegio13 COMBINADAS\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#filtrar-registros-unicos-por-id","title":"Filtrar registros \u00fanicos por <code>id</code>","text":"<p>Se eliminan los registros duplicados en el dataframe <code>df_colegio_combined</code> manteniendo solo los registros con <code>id</code> \u00fanicos. El dataframe resultante se almacena en <code>df_colegio_unique</code>, y el proceso se registra en el log.</p> <pre><code># Obtener solo los registros con 'id' \u00fanicos\ndf_colegio_unique = df_colegio_combined.drop_duplicates(subset='id')\nlogger.info('REGISTROS \u00daNICOS DE ID OBTENIDOS')\n</code></pre> <pre><code>2024-10-05 11:00:45,227 - INFO - REGISTROS \u00daNICOS DE ID OBTENIDOS\n</code></pre> <pre><code>df_colegio_unique\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 TI1001941478 2003-08-18 M TI 1001941478 ISAAC DAVID SEVILLA LOPEZ 2 TI1003266048 2002-12-25 M TI 1003266048 JORGE LUIS GARCIA JIMENEZ 4 TI1004358737 2002-04-24 M TI 1004358737 CARLOS DAVID GUZMAN SARMIENTO 6 TI1004367232 2003-06-10 F TI 1004367232 ARIADNA SOFIA EFFER OTERO 8 TI1004462648 2003-05-10 M TI 1004462648 SEBASTIAN ALEJANDRO JOYA MORALES ... ... ... ... ... ... ... ... ... ... 3719 CC93400713 None M CC 93400713 OSCAR ALEDT GIRALDO CORREA 3720 CC94520471 None M CC 94520471 JEAN PAUL TORRES VELEZ 3721 CC9770518 None M CC 9770518 MAURICIO None CADENA HOYOS 3722 CC98668306 None M CC 98668306 JUAN CARLOS POSADA GALLARDO 3723 CCMAGDALENA None M CC MAGDALENA FREDYS JOSE CAMPO MENDOZA <p>2428 rows \u00d7 9 columns</p>"},{"location":"seccion/4.1-FactColegio_fijos/#filtrar-registros-unicos-no-presentes-en-dimdatosfijos","title":"Filtrar registros \u00fanicos no presentes en <code>DimDatosFijos</code>","text":"<p>Se carga la tabla <code>BD_DimDatosFijos</code> desde la base de datos DWH y se filtran los registros del dataframe <code>df_colegio_combined</code> que no est\u00e1n presentes en la tabla <code>DimDatosFijos</code> basada en el campo <code>id</code>. Los registros faltantes se almacenan en el dataframe <code>df_missing_in_dwh</code>, y ambos procesos se registran en el log.</p> <pre><code># Obtener la tabla DimDatosFijos del DWH\ndf_dwh = pd.read_sql_query(\"SELECT * FROM dwh.BD_DimDatosFijos\", motor)\nlogger.info('TABLA DimDatosFijos CARGADA')\n\n# Filtrar los registros \u00fanicos que no est\u00e1n presentes en DimDatosFijos\ndf_missing_in_dwh = df_colegio_combined[~df_colegio_combined['id'].isin(df_dwh['id'])]\nlogger.info('FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos')\n</code></pre> <pre><code>2024-10-05 11:01:22,826 - INFO - TABLA DimDatosFijos CARGADA\n2024-10-05 11:01:23,375 - INFO - FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos\n</code></pre> <pre><code>df_missing_in_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 96 RC1048070119 2007-06-07 M RC 1048070119 JOSHUA None PASCAGAZA MAZENETT 136 RC1067624048 2013-12-01 M RC 1067624048 JUAN ESTEBAN RIVAS DAVILA 159 RC1081798749 2007-04-17 F RC 1081798749 JUANA VALENTINA GARCIA SANCHEZ 166 RC1081815011 2011-02-21 F RC 1081815011 KARLA SOFIA GARCIA SANCHEZ 282 RC1082866607 2005-08-30 M RC 1082866607 MAURICIO DAVID MOGOLLON DURAN ... ... ... ... ... ... ... ... ... ... 2352 TI1205965079 2015-01-15 M TI 1205965079 YEIKO JULIAN REY AGUDELO 2371 TI1205966118 2015-10-09 F TI 1205966118 SOFIA VALENTINA CABALLERO PEREZ 2374 TI1205966119 2015-10-12 F TI 1205966119 SALOME None RAMIREZ JIMENEZ 2381 NP1205966853 2016-04-05 M NP 1205966853 SIMON JOSE ARANGO LAFAURIE 2384 TI1205967151 2016-06-13 F TI 1205967151 TANIA ROSA CANTILLO RUEDA <p>169 rows \u00d7 9 columns</p> <pre><code>df_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 RC1025772327 2016-03-13 M RC 1025772327 THIAGO ALEJANDRO VALENCIA SALDARRIAGA 1 RC1083028689 2015-09-20 F RC 1083028689 BETZAIN None BARROS JIMENEZ 2 TI1081809711 2009-10-09 M TI 1081809711 JUAN DAVID BOLA\u00d1OS SANJUAN 3 TI1128200488 2008-08-07 F TI 1128200488 LAURA MARGARITA MESA JIMENEZ 4 TI1082916077 2008-05-27 M TI 1082916077 SEBASTIAN ANDRES VILLALOBO PALENCIA ... ... ... ... ... ... ... ... ... ... 975246 CC85466094 NaT None CC None CAMILO None VARGAS SALCEDO 975247 CC85471573 NaT None CC None JUAN CARLOS SANDOVAL VIVES 975248 CC85475426 NaT None CC None ASDRUBAL ENRIQUE POMARES CORREDOR 975249 CC858464327 NaT None CC None GABRIEL ANGEL BARRETO PION 975250 CCMAGDALENA NaT None CC None FREDYS JOSE CAMPO MENDOZA <p>975251 rows \u00d7 9 columns</p>"},{"location":"seccion/4.1-FactColegio_fijos/#guardar-registros-faltantes-en-la-tabla-dimdatosfijos","title":"Guardar registros faltantes en la tabla <code>DimDatosFijos</code>","text":"<p>Se conectan los registros faltantes en la tabla <code>BD_DimDatosFijos</code> en el DWH y se almacenan los registros del dataframe <code>df_missing_in_dwh</code> en la base de datos utilizando el modo <code>append</code>. El proceso de almacenamiento se registra en el log, y en caso de error, se captura y se registra el mensaje correspondiente.</p> <pre><code># Conectar a la base de datos DWH\nmotor3 = create_engine(Conexion_dwh())\n# Guardar los registros faltantes en la tabla DimDatosFijos\nalmacenamiento_time = time.time()\ntry:\n    df_missing_in_dwh.to_sql(name='BD_Dim_datosfijos', con=motor3, if_exists='append', index=False)\n    logger.info(f'ALMACENAMIENTO DE REGISTROS FALTANTES EN DimDatosFijos --- {time.time() - almacenamiento_time:.2f} seconds ---')\nexcept Exception as e:\n    logger.error(f'Error al almacenar registros en DimDatosFijos: {str(e)}')\n</code></pre> <pre><code>2024-10-05 11:02:18,222 - INFO - ALMACENAMIENTO DE REGISTROS FALTANTES EN DimDatosFijos --- 0.97 seconds ---\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#cargar-la-tabla-dimdatosfijos-desde-el-dwh","title":"Cargar la tabla <code>DimDatosFijos</code> desde el DWH","text":"<p>Se carga la tabla <code>BD_DimDatosFijos</code> desde la base de datos DWH en el dataframe <code>df_dwh</code> utilizando <code>pd.read_sql_query</code>. El evento de carga exitosa se registra en el log.</p> <pre><code># Obtener la tabla DimDatosFijos del DWH\ndf_dwh = pd.read_sql_query(\"SELECT * FROM dwh.BD_DimDatosFijos\", motor)\nlogger.info('TABLA DimDatosFijos CARGADA')\n</code></pre> <pre><code>2024-10-05 11:02:57,473 - INFO - TABLA DimDatosFijos CARGADA\n</code></pre> <pre><code>df_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 RC1025772327 2016-03-13 M RC 1025772327 THIAGO ALEJANDRO VALENCIA SALDARRIAGA 1 RC1083028689 2015-09-20 F RC 1083028689 BETZAIN None BARROS JIMENEZ 2 TI1081809711 2009-10-09 M TI 1081809711 JUAN DAVID BOLA\u00d1OS SANJUAN 3 TI1128200488 2008-08-07 F TI 1128200488 LAURA MARGARITA MESA JIMENEZ 4 TI1082916077 2008-05-27 M TI 1082916077 SEBASTIAN ANDRES VILLALOBO PALENCIA ... ... ... ... ... ... ... ... ... ... 975246 CC85466094 NaT None CC None CAMILO None VARGAS SALCEDO 975247 CC85471573 NaT None CC None JUAN CARLOS SANDOVAL VIVES 975248 CC85475426 NaT None CC None ASDRUBAL ENRIQUE POMARES CORREDOR 975249 CC858464327 NaT None CC None GABRIEL ANGEL BARRETO PION 975250 CCMAGDALENA NaT None CC None FREDYS JOSE CAMPO MENDOZA <p>975251 rows \u00d7 9 columns</p> <pre><code>\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/","title":"5.1-FactServicioSocial copy","text":""},{"location":"seccion/5.1-FactServicioSocial%20copy/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 16-10-2024 21:35\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>ServicioSocial.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='ServicioSocial.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-16 21:35:42,419 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se definen dos consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de las tablas <code>estudiantes</code> y <code>parientes</code> de la base de datos <code>nsijec</code>. Cada consulta selecciona columnas clave relacionadas con estudiantes y sus parientes. El diccionario <code>dim_names</code> se utiliza para mapear cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"estudiantes\":'''select \n                actividad_productiva,carga_masiva,convenio,created_at,created_by,direccion,enfermedad_alergia,estado,\n                fecha_inscripcion,grado,id,infraestructura,institucion,instructor,jornada,mod_prestacion_servicio,\n                modalidad,municipio,nivel_escolar,nombre_enfermedad_alergia,nombres_medicamentos,observacion,periodo,persona,pertenencia_etnica,pob_estudiante,poblacion,proyecto,pueblo_indigena,resguardo,telefono,tipo_localidad,toma_medicamentos,trabajador_adolecente,updated_at,updated_by\n                from nsijec.estudiantes''',\n    \"colegio13\":'''select \n                acudiente,created_at,created_by,direccion,email,estrato,estudiante,id,municipio,\n                parentesco,pariente,telefono,updated_at,updated_by\n                from nsijec.parientes'''                \n               }\n\n#Lista de querys Neith\nqr_structureNeith = {\n    \"acudientes\":'''select * from saipi.acudientes''',\n    \"registros\":'''select * from saipi.registros''',\n    \"valoracion\":'''select * from saipi.valoracion'''\n    }\n\ndim_names = {\n    \"estudiantes\":'BD_Estudiantes',\n    \"colegio13\":'BD_Parientes_Estudiantes',\n    \"acudientes\":'BD_Acudientes_Estudiantes',\n    \"registros\":'BD_Registros_Estudiantes',\n    \"valoracion\":'BD_Valoracion_Estudiantes'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-16 21:35:42,439 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code># Cargar tablas desde la base 'minerva'\nmotor = create_engine(obtener_conexion('minerva'))\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-16 21:35:43,360 - INFO - Cargando estudiantes \n2024-10-16 21:35:45,336 - INFO - Cargada estudiantes --- 1.98 seconds ---\n2024-10-16 21:35:45,338 - INFO - Cargando colegio13 \n2024-10-16 21:35:45,418 - INFO - Cargada colegio13 --- 0.08 seconds ---\n2024-10-16 21:35:45,496 - INFO - CARGUE TABLAS DESDE MYSQL --- colegio13 --- 2.62 seconds ---\n</code></pre> <pre><code># Cargar tablas desde la base 'neith'\nmotorNeith = create_engine(obtener_conexion('neith'))\ncargar_tablas(motorNeith, qr_structureNeith, df_structure, logger)\n</code></pre> <pre><code>2024-10-16 21:35:46,018 - INFO - Cargando acudientes \n2024-10-16 21:35:46,542 - INFO - Cargada acudientes --- 0.52 seconds ---\n2024-10-16 21:35:46,544 - INFO - Cargando registros \n2024-10-16 21:35:47,163 - INFO - Cargada registros --- 0.62 seconds ---\n2024-10-16 21:35:47,165 - INFO - Cargando valoracion \n2024-10-16 21:35:47,737 - INFO - Cargada valoracion --- 0.57 seconds ---\n2024-10-16 21:35:47,825 - INFO - CARGUE TABLAS DESDE MYSQL --- valoracion --- 2.31 seconds ---\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas de <code>df_structure</code>, excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-16 21:35:48,125 - INFO - VALIDADOR TABLA: estudiantes\n2024-10-16 21:35:48,156 - INFO - VALIDADOR TABLA: colegio13\n2024-10-16 21:35:48,211 - INFO - VALIDADOR TABLA: acudientes\n2024-10-16 21:35:48,294 - INFO - VALIDADOR TABLA: registros\n2024-10-16 21:35:48,357 - INFO - VALIDADOR TABLA: valoracion\n2024-10-16 21:35:48,358 - INFO - VALIDADOR DUPLICADOS --- 0.52 seconds ---\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_28760\\3328129721.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nC:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_28760\\3328129721.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\n2024-10-16 21:35:49,815 - INFO - LIMPIEZA --- 1.44 seconds ---\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH utilizando los nombres definidos en <code>dim_names</code>. Si la clave de la tabla no est\u00e1 presente en <code>dim_names</code>, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra, junto con los nombres de las tablas que se almacenaron correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-16 21:35:49,828 - INFO - CONEXION A BASE DWH\n2024-10-16 21:35:50,307 - INFO - Almacenando tabla estudiantes en DWH como BD_Estudiantes\n2024-10-16 21:35:55,525 - INFO - Tabla estudiantes almacenada correctamente como BD_Estudiantes.\n2024-10-16 21:35:55,527 - INFO - Almacenando tabla colegio13 en DWH como BD_Parientes_Estudiantes\n2024-10-16 21:35:56,276 - INFO - Tabla colegio13 almacenada correctamente como BD_Parientes_Estudiantes.\n2024-10-16 21:35:56,278 - INFO - Almacenando tabla acudientes en DWH como BD_Acudientes_Estudiantes\n2024-10-16 21:35:58,477 - INFO - Tabla acudientes almacenada correctamente como BD_Acudientes_Estudiantes.\n2024-10-16 21:35:58,479 - INFO - Almacenando tabla registros en DWH como BD_Registros_Estudiantes\n2024-10-16 21:36:01,440 - INFO - Tabla registros almacenada correctamente como BD_Registros_Estudiantes.\n2024-10-16 21:36:01,443 - INFO - Almacenando tabla valoracion en DWH como BD_Valoracion_Estudiantes\n2024-10-16 21:36:04,326 - INFO - Tabla valoracion almacenada correctamente como BD_Valoracion_Estudiantes.\n2024-10-16 21:36:04,416 - INFO - ALMACENAMIENTO ---  --- 14.59 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-16 21:36:04,428 - INFO - FINAL ETL --- 22.13 seconds ---\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial%20copy/#union-de-dataframes-con-registro-en-dwh","title":"Uni\u00f3n de DataFrames con registro en DWH","text":"<p>Este c\u00f3digo identifica y elimina columnas con m\u00e1s del 95% de datos vac\u00edos en tres DataFrames: <code>valoracion</code>, <code>registros</code> y <code>acudientes</code>. Posteriormente, realiza una union entre estas tablas limpias, comenzando con la fusi\u00f3n de <code>acudientes</code> con <code>registros</code>, seguida por la uni\u00f3n con la tabla <code>valoracion</code>. El DataFrame resultante, <code>final_merged_limpio</code>, se guarda en la tabla <code>BD_FactServicioSocial</code> dentro del DWH utilizando la funci\u00f3n <code>guardar_en_dwh</code>, reemplazando los datos existentes si es necesario.</p> <pre><code># Asumiendo que valoracion_df, registros_df y acudientes_df est\u00e1n en df_structure\nvaloracion_df = df_structure['valoracion']\nregistros_df = df_structure['registros']\nacudientes_df = df_structure['acudientes']\n\n# Funci\u00f3n para calcular las columnas con m\u00e1s del threshold% de datos vac\u00edos\ndef columnas_con_muchos_nulos(df, threshold=0.95):\n    porcentaje_nulos = df.isnull().mean()\n    columnas_excluir = porcentaje_nulos[porcentaje_nulos &gt; threshold].index\n    return columnas_excluir\n\n# Verificar qu\u00e9 columnas tienen m\u00e1s del 95% de datos vac\u00edos y eliminarlas\nthreshold = 0.95\ncolumnas_acudientes_excluir = columnas_con_muchos_nulos(acudientes_df, threshold)\ncolumnas_registros_excluir = columnas_con_muchos_nulos(registros_df, threshold)\ncolumnas_valoracion_excluir = columnas_con_muchos_nulos(valoracion_df, threshold)\n\n# Excluir las columnas identificadas en cada DataFrame\nacudientes_df_limpio = acudientes_df.drop(columns=columnas_acudientes_excluir)\nregistros_df_limpio = registros_df.drop(columns=columnas_registros_excluir)\nvaloracion_df_limpio = valoracion_df.drop(columns=columnas_valoracion_excluir)\n\n# Realizar las uniones con las tablas ya filtradas\n# Paso 1: Unir la tabla de acudientes con registros\nacudientes_registros_merged = pd.merge(acudientes_df_limpio, registros_df_limpio, left_on='registro_id', right_on='id', suffixes=('_acudiente', '_registro'))\n\n# Paso 2: Unir el resultado anterior con la tabla de valoraciones\nfinal_merged_limpio = pd.merge(acudientes_registros_merged, valoracion_df_limpio, left_on='registro_id', right_on='registro_id', suffixes=('', '_valoracion'))\n\nguardar_en_dwh(final_merged_limpio, 'BD_FactServicioSocial', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-16 21:36:04,572 - INFO - CONEXION A BASE DWH\n2024-10-16 21:36:05,085 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactServicioSocial\n2024-10-16 21:36:10,499 - INFO - Tabla almacenada correctamente.\n2024-10-16 21:36:10,609 - INFO - ALMACENAMIENTO ---  --- 6.04 seconds ---\n</code></pre> <pre><code>def df_columnas(df):\n    # Obtener los nombres de las columnas\n    nombres_columnas = df.columns.tolist()\n\n    # Crear un nuevo DataFrame con los nombres de las columnas\n    df_nombres = pd.DataFrame(nombres_columnas, columns=['Nombre_Columnas'])\n\n    return df_nombres\n\ntabla_columnas = df_columnas(final_merged_limpio)\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/","title":"6.1-main_encuestasV1","text":""},{"location":"seccion/6.1-main_encuestasV1/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport os\nimport datetime\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport logging\nfrom datetime import date\n\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import testfunciones, setup_logger\nfrom Funciones import Conexion_dwh, Conexion_neith\n\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 12-10-2024 11:33\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Encuestas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Encuestas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-12 11:33:22,727 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#funcion-para-limpiar-html","title":"Funci\u00f3n para limpiar HTML","text":"<p>La funci\u00f3n limpiar_html() toma un texto en formato HTML y lo limpia utilizando la biblioteca BeautifulSoup, devolviendo solo el texto sin etiquetas HTML.</p>"},{"location":"seccion/6.1-main_encuestasV1/#funcion-para-cargar-consultas-en-paralelo","title":"Funci\u00f3n para cargar consultas en paralelo","text":"<p>Se utiliza ThreadPoolExecutor para paralelizar la ejecuci\u00f3n de consultas SQL mediante la funci\u00f3n load_query(). Esta funci\u00f3n intenta ejecutar la consulta y devolver los resultados como un dataframe, manejando errores para devolver None si la consulta falla.</p> <pre><code>def limpiar_html(texto_html):\n    soup = BeautifulSoup(texto_html, 'html.parser')\n    return soup.get_text()\n###Librerias para paralelizar\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef load_query(query):\n    try: \n        df_query = pd.read_sql_query(query, motor_consulta)\n        return df_query\n    except:\n        return None\n</code></pre> <pre><code>#Medir tiempos\nstart_time = time.time()\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion_pruebas = Conexion_dwh()\nmotor_pruebas = create_engine(cadena_conexion_pruebas)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-12 11:33:22,823 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-del-diccionario-de-tablas-de-la-base-de-encuestas","title":"Consulta del diccionario de tablas de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener las tablas de la base de datos <code>encuestas</code> que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code> y <code>NombreTabla</code> desde la tabla <code>gb_Dim_Bodega_Inventario de Tablas</code> del DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que las tablas est\u00e9n marcadas como incluidas en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_tablas</code>.</p> <pre><code>##1. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla FROM dwh.`gb_Dim_Bodega_Inventario de Tablas`  \n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_tablas = pd.read_sql_query(sa.text(qr_structure), conn)\n#df_tablas\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#extraccion-de-codigos-de-encuestas","title":"Extracci\u00f3n de c\u00f3digos de encuestas","text":"<p>Se crea una nueva columna <code>sid</code> en el dataframe <code>df_tablas</code> que contiene los c\u00f3digos de encuestas. Estos c\u00f3digos se extraen dividiendo el valor de la columna <code>NombreTabla</code> utilizando el car\u00e1cter de subrayado (<code>_</code>) como delimitador, y tomando el tercer elemento resultante de la divisi\u00f3n (\u00edndice 2). Luego, la columna <code>sid</code> se convierte en tipo <code>str</code> para asegurarse de que todos los valores est\u00e9n en formato de cadena.</p> <pre><code>## Lista de codigos de encuestas\n#df_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\")\ndf_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\",expand = True)[2]\ndf_tablas[\"sid\"] = df_tablas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#conexion-a-la-base-de-datos-neith","title":"Conexi\u00f3n a la base de datos Neith","text":"<p>Se establece la conexi\u00f3n con la base de datos Neith utilizando la funci\u00f3n <code>Conexion_neith()</code> para generar la cadena de conexi\u00f3n y <code>create_engine()</code> para crear el motor de conexi\u00f3n. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base Neith\ncadena_conexion_neith = Conexion_neith()\nmotor_neith = create_engine(cadena_conexion_neith)\nlogger.info('CONEXION A BASE Neith')\n</code></pre> <pre><code>2024-10-12 11:33:23,561 - INFO - CONEXION A BASE Neith\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-de-nombres-de-encuestas","title":"Consulta de nombres de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los nombres de las encuestas desde la tabla <code>lime_surveys_languagesettings</code> de la base de datos <code>encuestas</code> en Neith. La consulta selecciona los campos <code>surveyls_survey_id</code> (identificado como <code>sid</code>) y <code>surveyls_title</code> (nombre de la encuesta). El resultado de la consulta se almacena en el dataframe <code>df_nombre_encuestas</code>, y se asegura que la columna <code>sid</code> sea de tipo <code>str</code>.</p> <pre><code>##2. Consultar nombres de las encuestas\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT surveyls_survey_id as sid, surveyls_title as NombreEncuesta \n    FROM encuestas.lime_surveys_languagesettings \"\"\"\n    df_nombre_encuestas = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_nombre_encuestas[\"sid\"] = df_nombre_encuestas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#fusion-de-tablas-codigos-de-encuestas-y-nombres-de-encuestas","title":"Fusi\u00f3n de tablas: c\u00f3digos de encuestas y nombres de encuestas","text":"<p>Se realiza una combinaci\u00f3n (merge) de los dataframes <code>df_tablas</code> y <code>df_nombre_encuestas</code> utilizando la columna <code>sid</code> como clave de uni\u00f3n. Esto permite agregar los nombres de las encuestas a las tablas correspondientes. Posteriormente, se eliminan las filas con valores nulos en <code>df_tablas_nombres</code>, y se reinicia el \u00edndice del dataframe resultante para garantizar un formato limpio.</p> <pre><code>## Nombres de las tablas de encuestas\ndf_tablas_nombres = pd.merge(df_tablas,df_nombre_encuestas, how ='left', on = 'sid')\ndf_tablas_nombres = df_tablas_nombres.dropna().copy().reset_index(drop =True)\n</code></pre> <pre><code>df_tablas_nombres.to_excel('df_tablas.xlsx',index=False)\n</code></pre> <pre><code>df_tablas_codigos = df_tablas_nombres[['NombreBaseDeDatos', 'NombreTabla', 'sid']]\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-del-diccionario-de-campos-de-la-base-de-encuestas","title":"Consulta del diccionario de campos de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los campos de las tablas de la base de datos <code>encuestas</code>, que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, y <code>NombreCampo</code> desde la tabla <code>gb_Dim_Bodega_Diccionario de Campos</code> en el DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que los campos est\u00e9n marcados como incluidos en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_campos</code>.</p> <pre><code>##3. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla, NombreCampo FROM dwh.`gb_Dim_Bodega_Diccionario de Campos`\n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_campos = pd.read_sql_query(sa.text(qr_structure), conn)\n</code></pre> <pre><code>df_campos_codigo = pd.merge(df_campos,df_tablas_codigos, how ='right', on = ['NombreBaseDeDatos', 'NombreTabla'])\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#extraccion-y-transformacion-de-codigos-qid-y-parent_qid","title":"Extracci\u00f3n y transformaci\u00f3n de c\u00f3digos <code>qid</code> y <code>parent_qid</code>","text":"<p>Se crea la columna <code>qid</code> en el dataframe <code>df_campos_codigo</code>, extrayendo parte del nombre de campo dividiendo el texto por \"X\". Luego, se limpian los datos y se convierten los valores a formato <code>str</code>. Adem\u00e1s, se genera la columna <code>parent_qid</code> tomando los primeros d\u00edgitos del c\u00f3digo <code>qid</code>, mientras que <code>qid</code> contiene los \u00faltimos cinco caracteres.</p> <pre><code>df_campos_codigo['qid'] = df_campos_codigo['NombreCampo'].str.split(\"X\",expand = True)[2]\ndf_campos_codigo_qid = df_campos_codigo.dropna().copy().reset_index(drop =True)\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid[\"qid\"].astype(str)\ndf_campos_codigo_qid[\"parent_qid\"] = df_campos_codigo_qid['qid'].str[:-5]\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid['qid'].str[-5:]\n</code></pre> <pre><code>#df_campos_codigo_qid.to_excel('t1.xlsx',index=False)\n</code></pre> <pre><code>#df_campos_codigo_qid['qid'].str[-5:]\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-de-la-tabla-lime_questions","title":"Consulta de la tabla <code>lime_questions</code>","text":"<p>Se ejecuta una consulta SQL para obtener los datos de la tabla <code>lime_questions</code> en la base de datos <code>encuestas</code>. La consulta selecciona las columnas <code>qid</code>, <code>parent_qid</code>, <code>sid</code>, y <code>title</code>. Posteriormente, las columnas <code>qid</code>, <code>parent_qid</code>, y <code>sid</code> se convierten al tipo <code>str</code> para asegurar una correcta manipulaci\u00f3n de datos. El resultado de la consulta se almacena en el dataframe <code>df_lime_questions</code>.</p> <pre><code>##4. Consultar la tabla lime_questions\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT qid, parent_qid, sid,title FROM encuestas.lime_questions \"\"\"\n    df_lime_questions = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions[\"qid\"] = df_lime_questions[\"qid\"].astype(str)\ndf_lime_questions[\"parent_qid\"] = df_lime_questions[\"parent_qid\"].astype(str)\ndf_lime_questions[\"sid\"] = df_lime_questions[\"sid\"].astype(str)\ndf_lime_questions\n</code></pre> qid parent_qid sid title 0 216 186 832591 SQ004 1 217 186 832591 SQ005 2 218 186 832591 SQ006 3 219 186 832591 SQ007 4 220 186 832591 SQ008 ... ... ... ... ... 1475 2690 0 398684 G01Q12 1476 2691 0 123984 Q00 1477 2692 0 123984 Q01 1478 2693 0 123984 Q02 1479 2694 0 123984 Q04 <p>1480 rows \u00d7 4 columns</p>"},{"location":"seccion/6.1-main_encuestasV1/#ajuste-de-qid-para-las-tablas","title":"Ajuste de <code>qid</code> para las tablas","text":"<p>Se realiza un proceso de combinaci\u00f3n entre los dataframes <code>df_campos_codigo_qid</code> y <code>df_lime_questions</code> para arreglar los campos <code>qid</code>. Se ajustan los nombres de columnas y se completan los valores faltantes de <code>qid</code> utilizando los valores de respaldo. El dataframe resultante, <code>df_campos_2</code>, contiene las columnas clave como <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, <code>NombreCampo</code>, <code>sid</code>, y el <code>qid</code> corregido.</p> <pre><code>#Arreglar qid para las tablas\ndf_campos_1 = pd.merge( df_campos_codigo_qid , df_lime_questions , how ='left', on = ['sid', 'qid'])\ndf_campos_1.rename(columns={'title': 'title_1', 'parent_qid_x':'parent_qid'}, inplace=True)\ndf_campos_1['title'] = df_campos_1['qid']\ndf_campos_2 = pd.merge(df_campos_1,df_lime_questions, how ='left', on = ['sid', 'title', 'parent_qid'])\ndf_campos_2['qid_y'].fillna(df_campos_2['qid_x'], inplace=True)\ndf_campos_2 = df_campos_2[['NombreBaseDeDatos', 'NombreTabla', 'NombreCampo', 'sid', 'qid_y']].copy()\ndf_campos_2.rename(columns={'qid_y':'qid'}, inplace=True)\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-y-limpieza-de-preguntas-de-encuestas-lime_questions_l10ns","title":"Consulta y limpieza de preguntas de encuestas (<code>lime_questions_l10ns</code>)","text":"<p>Se consulta la tabla <code>lime_question_l10ns</code> de la base de datos <code>encuestas</code> para obtener informaci\u00f3n detallada sobre las preguntas de encuestas. Se combina este resultado con el dataframe <code>df_campos_2</code>, eliminando columnas irrelevantes como <code>help</code>, <code>script</code>, y <code>language</code>. Luego, se limpian las preguntas usando la funci\u00f3n <code>limpiar_html</code> para eliminar etiquetas HTML, y se eliminan los caracteres especiales, como corchetes. Finalmente, se filtran las filas para excluir registros con <code>qid</code> no v\u00e1lidos como <code>'count'</code> y <code>'other'</code>, y el dataframe se reorganiza con los datos limpios.</p> <pre><code>##4. Consultar la tabla lime_questions q10\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT * FROM encuestas.lime_question_l10ns \"\"\"\n    df_lime_questions_names = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions_names[\"qid\"] = df_lime_questions_names[\"qid\"].astype(str)\ndf_sol = pd.merge(df_campos_2,df_lime_questions_names, how='left', on='qid')\ndf_sol = df_sol.drop(['help', 'script', 'language'], axis=1)\ndf_sol['question'] = df_sol['question'].astype(str)\ndf_sol['question'] = df_sol['question'].str.strip()\ndf_sol['question_clean'] = df_sol['question'].apply(limpiar_html)\ndf_sol['question_clean'] = df_sol['question_clean'].str.replace(r'[\\[\\]]', '', regex=True)\ndf_clean = df_sol[~df_sol['qid'].isin(['count', 'other'])].copy().reset_index(drop =True)\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_29464\\1878959599.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(texto_html, 'html.parser')\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-de-grupos-de-encuestas-lime_groups","title":"Consulta de grupos de encuestas (<code>lime_groups</code>)","text":"<p>Se ejecuta una consulta SQL para obtener los grupos de encuestas desde la tabla <code>lime_groups</code> y su descripci\u00f3n desde la tabla <code>lime_group_l10ns</code>. La consulta selecciona el <code>gid</code>, <code>sid</code>, y el nombre del grupo (<code>group_name</code>) como <code>DetalleTabla</code>. El resultado se almacena en el dataframe <code>df_lime_groups</code>, asegurando que la columna <code>sid</code> est\u00e9 en formato <code>str</code>.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT e1.gid, e1.sid, e2.group_name as DetalleTabla FROM encuestas.lime_groups as e1\n                    LEFT JOIN encuestas.lime_group_l10ns as e2\n                    ON e1.gid= e2.gid\"\"\"\n    df_lime_groups = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_groups['sid'] = df_lime_groups['sid'].astype(str)\n</code></pre> <pre><code>df_clean = pd.merge( df_clean , df_lime_groups , how='left', on='sid')\n</code></pre> <pre><code>df_clean\n</code></pre> NombreBaseDeDatos NombreTabla NombreCampo sid qid id question question_clean gid DetalleTabla 0 encuestas lime_survey_124282 124282X29X488 124282 488 488.0 &lt;p style=\"text-align: center;\"&gt;&lt;span style=\"fo... PLANCHA N\u00b01 \\n\\n\\n\\nPRINCIPALES\\n\\n\\n\\n\\n\\n\\n\\... 29 REPRESENTANTES 1 encuestas lime_survey_161825 161825X51X1103 161825 1103 1103.0 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 2 encuestas lime_survey_161825 161825X51X1104 161825 1104 1104.0 \u00a1SU OPINION NOS INTERESA! Si desea registrar u... \u00a1SU OPINION NOS INTERESA! Si desea registrar u... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 3 encuestas lime_survey_161825 161825X51X1105SQ001 161825 1113 1113.0 LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 4 encuestas lime_survey_161825 161825X51X1105SQ002 161825 1114 1114.0 LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO ... ... ... ... ... ... ... ... ... ... ... 915 encuestas lime_survey_995137 995137X42X861SQ007 995137 877 877.0 E-mail: E-mail: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 916 encuestas lime_survey_995137 995137X42X862 995137 862 862.0 ESTADO LLAMADA: ESTADO LLAMADA: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 917 encuestas lime_survey_995137 995137X42X863 995137 863 863.0 A\u00d1O: A\u00d1O: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 918 encuestas lime_survey_995137 995137X42X864 995137 864 864.0 SEMESTRE: SEMESTRE: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 919 encuestas lime_survey_995137 995137X42X865 995137 865 865.0 MES: MES: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE <p>920 rows \u00d7 10 columns</p> <pre><code>#df_sol.to_excel('df_sol.xlsx',index=False)\n#df_clean.to_excel('df_clean.xlsx',index=False)\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-de-respuestas-de-encuestas-lime_answers","title":"Consulta de respuestas de encuestas (<code>lime_answers</code>)","text":"<p>Se ejecuta una consulta SQL para obtener las respuestas de encuestas desde la tabla <code>lime_answers</code> y sus descripciones desde la tabla <code>lime_answer_l10ns</code>. La consulta selecciona el <code>aid</code>, <code>qid</code>, <code>code</code>, y la respuesta (<code>answer</code>). El resultado de la consulta se almacena en el dataframe <code>df_lime_answers</code>, asegurando que las columnas <code>qid</code> y <code>aid</code> est\u00e9n en formato <code>str</code> para facilitar su manipulaci\u00f3n.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT a1.aid, a1.qid, a1.code, a2.answer \n                    FROM encuestas.lime_answers as a1\n                    LEFT JOIN encuestas.lime_answer_l10ns as a2\n                    ON a1.aid = a2.aid\"\"\" \n\n    df_lime_answers = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_answers['qid'] = df_lime_answers['qid'].astype(str)\ndf_lime_answers['aid'] = df_lime_answers['aid'].astype(str)\n</code></pre> <pre><code>tables_survey_names = df_clean['NombreTabla'].unique().tolist()\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#procesamiento-y-transformacion-de-encuestas","title":"Procesamiento y transformaci\u00f3n de encuestas","text":"<p>Se ejecuta un ciclo que procesa las tablas de encuestas almacenadas en <code>tables_survey_names</code>. Para cada tabla, se cargan los datos y se identifican las columnas relacionadas con preguntas mediante una combinaci\u00f3n con <code>df_clean</code>. Luego, se ajustan las respuestas utilizando la tabla <code>df_lime_answers</code> para reemplazar c\u00f3digos por respuestas reales y se renombra cada columna con su respectiva pregunta limpia.</p> <p>Dependiendo de la estructura de la tabla, se reestructura utilizando la funci\u00f3n <code>melt</code> para tener una representaci\u00f3n de preguntas y respuestas en formato largo. Se verifican las columnas relevantes, como las relacionadas con identificaci\u00f3n, para mantener la coherencia de los datos y las tablas resultantes se almacenan en archivos CSV si no cumplen con ciertos criterios. Las tablas procesadas se agregan a la lista <code>tablesToConcat</code> para su posterior concatenaci\u00f3n.</p> <p>El proceso tambi\u00e9n incluye la limpieza de columnas como <code>documento</code> y <code>tipo_documento</code>, que se renombran para unificar la estructura entre todas las tablas de encuestas.</p> <pre><code>df_survey = dict()\ncolumnsStatic = ['C\u00e9dula:','NIT:','Tipo Documento de Identidad','N\u00famero Documento de Identidad','Tipo Documento Identidad', 'Numero de identificaci\u00f3n']\ncolumnsId = ['C\u00e9dula:', 'N\u00famero Documento de Identidad', 'Numero de identificaci\u00f3n' ]\ncolumnsTipoId = ['Tipo Documento de Identidad', 'Tipo Documento Identidad' ]\ntablesToConcat = []\n\nfor table_name in tables_survey_names:\n    print(table_name)\n    with motor_neith.begin() as conn:\n        qr_structure = \"\"\"SELECT * FROM encuestas. \"\"\" + table_name\n        df_survey[table_name] = pd.read_sql_query(sa.text(qr_structure), conn)\n\n    dfColumns = pd.DataFrame({'NombreCampo':df_survey[table_name].columns.tolist()})\n    dfColumns_p2 = pd.merge( dfColumns , df_clean , how = 'left' , on = ['NombreCampo'] )\n    dfColumns_p2 = dfColumns_p2[ ~dfColumns_p2['NombreTabla'].isna() ]\n    dfColumns_p2 = dfColumns_p2[['NombreCampo','NombreTabla','sid','qid','question_clean']]\n    dfToValidateAnswer = pd.merge( dfColumns_p2 , df_lime_answers , how = 'left' , on = ['qid'] )\n    dfToValidateAnswer = dfToValidateAnswer[~dfToValidateAnswer['aid'].isna()]\n    columnsTVA = dfToValidateAnswer['NombreCampo'].unique().tolist()\n    df_survey[table_name] = df_survey[table_name][['submitdate'] + dfColumns_p2['NombreCampo'].unique().tolist()] \n    for col in df_survey[table_name].columns.tolist():\n        if col in columnsTVA:\n            df_survey[table_name][col + '_answer'] = pd.merge( df_survey[table_name][[col]] , dfToValidateAnswer[ dfToValidateAnswer['NombreCampo'] == col ] , how = 'left' , left_on = col, right_on = 'code' )['answer']\n            df_survey[table_name] = df_survey[table_name].drop([col], axis=1)\n            df_survey[table_name] = df_survey[table_name].rename({ col + '_answer': col }, axis=1)\n        newName = dfColumns_p2[ dfColumns_p2['NombreCampo'] == col ]['question_clean'].tolist()\n\n        if len(newName) != 0:\n            df_survey[table_name] = df_survey[table_name].rename({ col: newName[0]  }, axis=1)\n\n    columnsToMantain = [x for x in df_survey[table_name].columns.tolist() if x in columnsStatic]\n    df_survey[table_name].fillna(value=np.nan, inplace=True)\n    try:\n        if len(columnsToMantain) == 0:\n            print('No aplica', os.getcwd() +'\\\\' +table_name +'.csv')\n            df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n        elif 'FECHA:' in df_survey[table_name].columns.tolist():\n            print('tiene fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\",'FECHA:'] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        else:\n            print('sin fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\"] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        for colClean in columnsToMantain:\n            df_survey[table_name][colClean] = df_survey[table_name][colClean].astype(str).apply(lambda x: x.replace(\"\\t\" , \"\" ))\n\n    except:\n        print('posible vacio')\n        df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n\n\n    df_survey[table_name] = df_survey[table_name].replace('nan', np.nan)    \n    df_survey[table_name].dropna( subset = columnsToMantain , inplace=True)\n    df_survey[table_name]['DetalleEncuesta'] = df_clean[df_clean['NombreTabla'] == table_name]['DetalleTabla'].tolist()[0] \n\n    for col in columnsId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'documento' }, axis=1)\n    for col in columnsTipoId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'tipo_documento' }, axis=1)\n</code></pre> <pre><code>lime_survey_124282\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_124282.csv\nlime_survey_161825\ntiene fecha\nlime_survey_189858\ntiene fecha\nlime_survey_267495\ntiene fecha\nlime_survey_385698\nsin fecha\nlime_survey_413658\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_413658.csv\nlime_survey_452416\ntiene fecha\nlime_survey_478847\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_478847.csv\nlime_survey_548818\ntiene fecha\nlime_survey_567215\nsin fecha\nlime_survey_576377\ntiene fecha\nlime_survey_586872\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_586872.csv\nlime_survey_599513\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_599513.csv\nlime_survey_699399\nsin fecha\nposible vacio\nlime_survey_757259\nsin fecha\nlime_survey_767954\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_767954.csv\nlime_survey_773674\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_773674.csv\nlime_survey_818378\nsin fecha\nlime_survey_832591\nsin fecha\nlime_survey_934423\ntiene fecha\nlime_survey_959511\ntiene fecha\nlime_survey_995137\ntiene fecha\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#concatenacion-final-de-encuestas","title":"Concatenaci\u00f3n final de encuestas","text":"<p>Se filtran las tablas procesadas en <code>df_survey</code> que est\u00e1n presentes en la lista <code>tablesToConcat</code> y se almacenan en el diccionario <code>df_survey_2</code>. Luego, se concatenan todas las tablas seleccionadas en un \u00fanico dataframe <code>df_survey_final</code>, combinando los resultados y asegurando un \u00edndice continuo.</p> <pre><code>df_survey_2 = dict( filter(lambda item: item[0] in tablesToConcat, df_survey.items()) )\ndf_survey_final =  pd.concat( list(df_survey_2.values()) , ignore_index = True )\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#consulta-y-combinacion-de-datos-de-afiliados","title":"Consulta y combinaci\u00f3n de datos de afiliados","text":"<p>Se realiza una consulta a la tabla <code>BD_Dim_Datos_Fijos</code> en la base DWH para obtener los campos <code>DOCUMENTO</code> y <code>CODDOC</code>. Luego, se hace un merge entre los datos de encuestas (<code>df_survey_final</code>) y la informaci\u00f3n obtenida de la tabla, para complementar los documentos y tipos de documento. Posteriormente, se eliminan las columnas innecesarias y se genera una nueva columna <code>ID_AFILIADO</code> concatenando <code>tipo_documento</code> y <code>documento</code>. Finalmente, se reordenan las columnas colocando <code>ID_AFILIADO</code> al inicio.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion3 = Conexion_dwh()\nmotor3 = create_engine(cadena_conexion3)\nlogger.info('CONEXION A BASE DWH')\n\n\n# Bloque 1: Consulta y carga de datos\nlogger.info(\"Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\")\nquery = \"SELECT DOCUMENTO, CODDOC FROM BD_Dim_Datos_Fijos\"\ndf_datos_fijos = pd.read_sql(query, con=motor3)\nlogger.info(f\"Consulta a BD_Dim_Datos_Fijos completada: {df_datos_fijos.shape[0]} filas cargadas.\")\n\n# Bloque 2: Merge y limpieza de columnas\nlogger.info(\"Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\")\ndf_survey_final = pd.merge(df_survey_final, df_datos_fijos, how='left', left_on='documento', right_on='DOCUMENTO')\ndf_survey_final = df_survey_final.drop(columns=['DOCUMENTO'])\ndf_survey_final['tipo_documento'] = df_survey_final['CODDOC'].fillna('CC')\ndf_survey_final = df_survey_final.drop(columns=['CODDOC'])\n\n# Bloque 3: Generaci\u00f3n de ID y reorganizaci\u00f3n de columnas\nlogger.info(\"Generando 'ID_AFILIADO' y reorganizando columnas.\")\ndf_survey_final['ID_AFILIADO'] = df_survey_final['tipo_documento'].astype(str) + df_survey_final['documento'].astype(str)\n\n# Renombrar columnas\ndf_survey_final = df_survey_final.rename(columns={\n    'submitdate': 'FECHA_ENCUESTA',\n    'documento': 'ID_DOCUMENTO',\n    'Pregunta': 'PREGUNTA',\n    'Respuesta': 'RESPUESTA',\n    'DetalleEncuesta': 'DETALLE_ENCUESTA',\n    'NIT:': 'NIT',\n    'tipo_documento': 'TIPO_DOCUMENTO',\n})\n\n# Reordenar las columnas colocando 'ID_AFILIADO' en la primera posici\u00f3n\ncolumnas = ['ID_AFILIADO'] + [col for col in df_survey_final.columns if col != 'ID']\ndf_survey_final = df_survey_final.reindex(columns=columnas)\n\n# Mostrar las columnas finales\nlogger.info(f\"Proceso completado. Columnas finales: {df_survey_final.columns.tolist()}\")\n</code></pre> <pre><code>2024-10-12 11:42:57,543 - INFO - CONEXION A BASE DWH\n2024-10-12 11:42:57,548 - INFO - Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\n2024-10-12 11:43:06,213 - INFO - Consulta a BD_Dim_Datos_Fijos completada: 975320 filas cargadas.\n2024-10-12 11:43:06,215 - INFO - Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\n2024-10-12 11:43:06,763 - INFO - Generando 'ID_AFILIADO' y reorganizando columnas.\n2024-10-12 11:43:06,916 - INFO - Proceso completado. Columnas finales: ['ID_AFILIADO', 'FECHA_ENCUESTA', 'FECHA:', 'ID_DOCUMENTO', 'PREGUNTA', 'RESPUESTA', 'DETALLE_ENCUESTA', 'NIT', 'TIPO_DOCUMENTO', 'ID_AFILIADO']\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#conexion-a-la-base-de-datos-dwh_1","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion2 = Conexion_dwh()\nmotor2 = create_engine(cadena_conexion2)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-12 11:33:41,951 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/6.1-main_encuestasV1/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>df_survey_final</code> en la tabla <code>BD_Fact_Encuestas</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>#Guardar en base dwh\nalmacenamiento_time = time.time()\ndf_survey_final.to_sql(name='BD_Fact_Encuestas', con=motor2, if_exists = 'replace', index=False)\nlogging.info(f'ALMACENAMIENTO --- {time.time() - almacenamiento_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 11:34:07,244 - INFO - ALMACENAMIENTO --- 25.28 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/","title":"1.2-EstandarAfiliados2.0","text":""},{"location":"seccion/Bloque1/#12-estandarafiliados20_1","title":"1.2-EstandarAfiliados2.0","text":""},{"location":"seccion/Bloque1/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, entre otras) y un conjunto de funciones personalizadas desde el archivo `Funciones.e.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/Bloque1/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Estandar_Afiliados.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Estandar_Afiliados.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 14:52:48,193 - INFO - Importacion de funciones correcta, 17-10-2024 14:52\n2024-10-17 14:52:48,195 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>En esta secci\u00f3n se define un diccionario que contiene las consultas SQL que se van a ejecutar para extraer datos desde distintas tablas en la base de datos principal <code>xml4</code>. Las consultas est\u00e1n organizadas por categor\u00edas como \"Ben\" (beneficiarios) y \"Con\" (c\u00f3nyuges).</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"Tra\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.cedtra) as ID,\n            CONCAT(b.prinom, ' ', b.segnom, ' ', b.priape, ' ', b.segape) AS NOMBRE, \n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO,\n            c3.fecest  AS FECHA_ESTADO,\n            1 AS xml4c086, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR) AS CODDOC, \n            b.cedtra AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c086 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT CONCAT(\n                    CAST(\n                    CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CD' THEN 8\n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'PE' THEN 9            \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'PT' THEN 15 \n                    WHEN coddoc = 'TI' THEN 2\n                    ELSE coddoc \n                    END AS CHAR\n                    ),\n                    cedtra\n                    ) AS cod, fecest\n            FROM subsidio.subsi15 GROUP BY cod) AS c3\n            ON CONCAT( b.coddoc , b.cedtra ) = c3.cod     \n    ''',\n    \"Emp\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ),\n            b.nit\n            ) AS ID, \n            b.razsoc AS NOMBRE, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c085, \n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ) AS CODDOC, \n            b.nit AS DOCUMENTO, \n            c2.prinom AS PRIMER_NOMBRE, \n            c2.segnom AS SEGUNDO_NOMBRE, \n            c2.priape AS PRIMER_APELLIDO, \n            c2.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c085 AS b\n        LEFT JOIN \n            (SELECT CONCAT(\n                CAST(\n                CASE \n                WHEN coddoc = 'CC' THEN 1 \n                WHEN coddoc = 'CE' THEN 4 \n                WHEN coddoc = 'NI' THEN 7 \n                WHEN coddoc = 'PA' THEN 6 \n                WHEN coddoc = 'RC' THEN 3 \n                ELSE coddoc \n                END AS CHAR\n                ),\n                nit\n                ) AS cod, fecest, priape, segape, prinom, segnom\n        FROM subsidio.subsi02 GROUP BY cod) AS c2\n        ON CONCAT(b.tipide, b.nit) = c2.cod\n    ''',\n    \"Con\": '''\n        SELECT \n            CONCAT(coddoc, cedcon) AS ID, \n            CONCAT(prinom, ' ', segnom, ' ', priape, ' ', segape) AS NOMBRE, \n            fecnac AS FECHA_NACIMIENTO, \n            sexo AS SEXO, \n            fecest  AS FECHA_ESTADO, \n            1 AS SUBSI20, \n            coddoc AS CODDOC, \n            cedcon AS DOCUMENTO, \n            prinom AS PRIMER_NOMBRE, \n            segnom AS SEGUNDO_NOMBRE, \n            priape AS PRIMER_APELLIDO, \n            segape AS SEGUNDO_APELLIDO \n        FROM \n            subsidio.subsi20\n    ''', \n    \"Ben\": '''\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.documento) as ID,\n            CONCAT(b.prinom, ' ', b.segnom, ' ', b.priape, ' ', b.segape) AS NOMBRE, \n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c087, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END AS CHAR) AS CODDOC, \n            b.documento AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO \n        FROM \n            xml4.xml4c087 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT documento, fecest\n        FROM \n            subsidio.subsi22 GROUP BY documento) AS c2\n            ON b.documento=c2.documento\n    '''\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-17 14:52:49,981 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque1/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-17 14:52:56,629 - INFO - CONEXION A BASE MINERVA\n2024-10-17 14:52:56,869 - INFO - Cargando Tra \n2024-10-17 14:54:45,788 - INFO - Cargada Tra --- 108.92 seconds ---\n2024-10-17 14:54:45,788 - INFO - Cargando Emp \n2024-10-17 14:54:52,399 - INFO - Cargada Emp --- 6.61 seconds ---\n2024-10-17 14:54:52,400 - INFO - Cargando Con \n2024-10-17 14:54:55,082 - INFO - Cargada Con --- 2.68 seconds ---\n2024-10-17 14:54:55,083 - INFO - Cargando Ben \n2024-10-17 14:57:44,645 - INFO - Cargada Ben --- 169.56 seconds ---\n2024-10-17 14:57:45,632 - INFO - CARGUE TABLAS DESDE MYSQL --- 289.00 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#concatenacion-de-tablas-y-correccion-de-formatos-de-fecha","title":"Concatenaci\u00f3n de tablas y correcci\u00f3n de formatos de fecha","text":"<p>En esta celda se concatenan todas las tablas extra\u00eddas desde MySQL en un solo dataframe (<code>dfTotal</code>). Posteriormente, se procesan las columnas de fechas para convertirlas al formato correcto y reemplazar los errores en las fechas con valores nulos (<code>NaT</code>).</p> <pre><code>#Concatenaci\u00f3n\ntratamiento1_time = time.time()\ndfTotal = pd.concat( list(df_structure.values()) , ignore_index = True )\n\n#Informacion De Origenes\ndfOrigen = dfTotal[['ID','xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087']].groupby('ID')[ ['xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087'] ].max().reset_index()\n\n#Cambiar errores en columnas fechas a NaT y convertirlas a formato datetime\ndfTotal['FECHA_ESTADO'] = pd.to_datetime(dfTotal['FECHA_ESTADO'],errors='coerce')\ndfTotal['FECHA_NACIMIENTO'] = pd.to_datetime(dfTotal['FECHA_NACIMIENTO'],errors='coerce')\nlogger.info(f'TRANSFORMACION PARTE 1 --- {time.time() - tratamiento1_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:58:25,227 - INFO - TRANSFORMACION PARTE 1 --- 17.64 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-tipos-de-columnas","title":"Definici\u00f3n de tipos de columnas","text":"<p>Aqu\u00ed se identifican las columnas del dataframe <code>dfTotal</code> seg\u00fan su tipo de dato: num\u00e9ricas, de texto y de fechas. Se crean listas con los nombres de las columnas correspondientes a cada tipo de dato, lo cual ser\u00e1 \u00fatil para las operaciones posteriores.</p> <pre><code>#Definir columnas con sus tipos\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndates = ['datetime64[ns]']\nnumericColumns = dfTotal.select_dtypes(include=numerics).columns.tolist()\ndatesColumns = dfTotal.select_dtypes(include=dates).columns.tolist()\ntextColumns = [x for x in dfTotal.columns.tolist() if x not in (numericColumns + datesColumns ) ]\nColumnsToCompare = [x for x in dfTotal.columns.tolist() if x not in [ 'ID' , 'FECHA_ESTADO' ,'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\nfinalColumns = [x for x in dfTotal.columns.tolist() if x not in [ 'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\n</code></pre>"},{"location":"seccion/Bloque1/#segunda-fase-de-transformacion-agregacion-por-id","title":"Segunda fase de transformaci\u00f3n: agregaci\u00f3n por ID","text":"<p>Esta celda agrupa los datos por la columna <code>ID</code>, que representa la clave \u00fanica de los registros, y realiza una agregaci\u00f3n de los valores usando la funci\u00f3n <code>unique</code>. Esta agregaci\u00f3n ayuda a identificar los valores \u00fanicos en cada columna por cada ID, y se registra el tiempo tomado para esta transformaci\u00f3n.</p> <pre><code>tratamiento2_time = time.time()\n#Cambian None por NaN\ndfTotal[textColumns] = dfTotal[textColumns].where(pd.notna(dfTotal), np.nan)\n\n#Pasar las columnas tipo texto a UPPER\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.upper())\n\n#En las columnas tipo texto, eliminar espacios al comienzo y al final\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.strip())\n\n#Reemplazar los \"NAN\" por NaN\ndfTotal[textColumns] = dfTotal[textColumns].replace('NAN', np.nan)\n\n#ValIDador de campos repetIDos\nValIDador = dfTotal.groupby('ID')[ ColumnsToCompare ].nunique(dropna=False).reset_index()\nValIDador['suma']  = ValIDador.sum(axis=1, numeric_only=True)\n\n#Tablas de informaci\u00f3n para guia transformaci\u00f3n\nIDProblemas = ValIDador[ValIDador['suma'] &gt; len(ColumnsToCompare) ]['ID'].tolist()\ndfRepetIDos = dfTotal[ dfTotal['ID'].isin(IDProblemas)  ].groupby('ID')[ ColumnsToCompare ].agg(['unique']).reset_index()\ndfRepetIDos.columns = dfRepetIDos.columns.droplevel(1)\ntrazaDf = pd.merge(dfRepetIDos,dfOrigen,on = 'ID' )\ntrazaDf.to_csv(os.getcwd() + r'traza_Estandar_Afiliados.csv')\nlogger.info(f'TRANSFORMACION PARTE 2 --- {time.time() - tratamiento2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 15:05:45,313 - INFO - TRANSFORMACION PARTE 2 --- 410.90 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#limpieza-de-datos-reemplazo-de-valores-nulos","title":"Limpieza de datos: reemplazo de valores nulos","text":"<p>Se realiza una copia del dataframe <code>dfTotal</code> y se reemplazan los valores faltantes (nulos) por <code>NaN</code>, un valor que puede manejarse en pandas para realizar an\u00e1lisis y transformaciones posteriores.</p> <pre><code>#Eliminar duplicados dejando el de la fecha m\u00e1s reciente\nlimpieza_time = time.time()\ndfTotal = dfTotal[finalColumns]\ndfTotal = dfTotal.sort_values('FECHA_ESTADO').drop_duplicates('ID',keep='last')\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 15:06:14,496 - INFO - LIMPIEZA --- 19.69 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#clasificacion-de-columnas-por-tipo","title":"Clasificaci\u00f3n de columnas por tipo","text":"<p>En esta celda se agrupan las columnas del dataframe por su tipo de dato (por ejemplo, columnas de tipo <code>datetime64[ns]</code> y <code>object</code>). Este proceso ayuda a tener una vista general de la estructura de los datos antes de aplicar transformaciones adicionales o realizar an\u00e1lisis.</p> <pre><code>dfTotal = dfTotal.drop( [ 'NOMBRE', 'FECHA_ESTADO'] , axis = 1 )\n# dfTotal.rename(columns={'prinom':'periodo orbital',\n#                         'segnom':'m\u00e9todo descubrimiento',\n#                        'priape':'periodo orbital',\n#                         'segape':'m\u00e9todo descubrimiento'},\n#                inplace=True)\n</code></pre> <pre><code>logger.info(f'Informacion final {dfTotal[\"ID\"].count()} registros')\n</code></pre> <pre><code>2024-10-13 17:53:36,054 - INFO - Informacion final 1113524 registros\n</code></pre>"},{"location":"seccion/Bloque1/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Dim_Datos_Fijos</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfTotal, 'BD_Dim_Datos_Fijos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 15:06:20,532 - INFO - CONEXION A BASE DWH\n2024-10-17 15:06:20,805 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-17 15:07:19,865 - INFO - Tabla almacenada correctamente.\n2024-10-17 15:07:20,453 - INFO - ALMACENAMIENTO --- 59.92 seconds ---\n</code></pre> <pre><code>dfTotal.columns.tolist()\n</code></pre> <pre><code>['ID',\n 'FECHA_NACIMIENTO',\n 'SEXO',\n 'CODDOC',\n 'DOCUMENTO',\n 'PRIMER_NOMBRE',\n 'SEGUNDO_NOMBRE',\n 'PRIMER_APELLIDO',\n 'SEGUNDO_APELLIDO']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 17:54:50,782 - INFO - FINAL ETL --- 4624.90 seconds ---\n</code></pre> <pre><code>dfTotal[dfTotal['ID'] == 'CC858464327']\n</code></pre> <pre><code>dfTotal[dfTotal['ID'] == 'CC858464327']\n</code></pre> <pre><code># Definir ColumnsNoID excluyendo 'ID'\nColumnsNoID = [col for col in dfTotal.columns if col != 'ID']\n\n# Agrupar por 'ID' y aplicar la agregaci\u00f3n 'unique'\ndfTotal[dfTotal['ID'].isin(IDProblemas)].groupby('ID')[ColumnsNoID].agg(['unique'])\n</code></pre> <pre><code>dt = dfTotal.copy().where(pd.notna(dfTotal), np.nan)\n</code></pre> <pre><code>dt.columns.to_series().groupby(dt.dtypes).groups\n</code></pre>"},{"location":"seccion/Bloque1/#14-dimensiones","title":"1.4-Dimensiones","text":""},{"location":"seccion/Bloque1/#14-dimensiones_1","title":"1.4-Dimensiones","text":""},{"location":"seccion/Bloque1/#importacion-de-librerias-y-funciones_1","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code> y otras. Adem\u00e1s, se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>, ajustando el <code>sys.path</code> para incluir el directorio correspondiente.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\n#from datetime import date\nstart_time = time.time()\n#from dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import log_tiempo, guardar_en_dwh, cargar_tablas, StoreDuplicated, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 17-10-2024 14:13\n</code></pre>"},{"location":"seccion/Bloque1/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Dimensiones.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Dimensiones.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 14:13:54,493 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-consultas-sql_1","title":"Definici\u00f3n de consultas SQL","text":"<p>Se crea un diccionario <code>qr_structure</code> que contiene varias consultas SQL para extraer datos de diferentes tablas en las bases de datos <code>subsidio</code>, <code>schoolkits</code> y <code>xml4</code>. Adem\u00e1s, se define el diccionario <code>dim_names</code>, que mapea los nombres de las consultas a sus respectivas tablas de destino en la base de datos DWH. Se registra el inicio del proceso de lectura de las consultas en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"sat25\":'''select tipper as TIPPER, detalle as TIPO_PERSONA from subsidio.sat25''',\n    \"subsi04\":'''select codact as COD_ACTIVIDAD, detalle as ACTIVIDAD_ECONOMICA from subsidio.subsi04''',\n    \"subsi36\":'''select codest as COD_EST_INAC, detalle as ESTADO_INACTIVACION, tipo as ID_TIPO_AFILIADO, codsat as COD_SAT from subsidio.subsi36''',\n    \"SK13\":'''select CODIGOZONA as COD_ZONA, NOMBREZONA as ZONA from schoolkits.SK13''',\n    \"gener18\": '''select coddoc as CODDOC, detdoc as TIPO_DOCUMENTO, codssf as COD_SUPERSUBSIDIO,codgia as COD_GIASS from empresa.gener18''',\n    \"subsi02\":'''select \n        'nit' as 'NIT',\n        'digver' as 'DIGITODEVERIFICACIONDENITDELAEMPRESA',\n        'tipper' as 'INDICAELTIPODEPERSONA.N=NATURAL,J=JUR\u00cdDICA',\n        'coddoc' as 'TIPODEDOCUMENTO',\n        'razsoc' as 'RAZ\u00d3NSOCIALDELAEMPRESA',\n        'priape' as 'PRIMERAPELLIDO.PARAPERSONASNATURALES.',\n        'segape' as 'SEGUNDOAPELLIDO.PARAPERSONASNATURALES.',\n        'prinom' as 'PRIMERNOMBRE.PARAPERSONASNATURALES.',\n        'segnom' as 'SEGUNDONOMBRE.PARAPERSONASNATURALES.',\n        'sigla' as 'SIGLAEMPRESA',\n        'nomcom' as 'NOMBRECOMERCIAL',\n        'coddocreppri' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALPRINCIPAL',\n        'cedrep' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALPRINCIPAL',\n        'repleg' as 'NOMBREDELREPRESENTANTELEGALPRINCIPAL',\n        'coddocrepsup' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALSUPLENTE',\n        'cedrepsup' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALSUPLENTE',\n        'replegsup' as 'NOMBREDELREPRESENTANTELEGALSUPLENTE',\n        'jefper' as 'NOMBREDELJEFEDEPERSONAL',\n        'direccion' as 'DIRECCIONDEDELAEMPRESAENELFORMULARIODEREGISTRO',\n        'codciu' as 'C\u00d3DIGOCIIUDANE',\n        'codbar' as 'C\u00d3DIGODELBARRIO',\n        'celular' as 'N\u00daMERODECELULARDECONTACTO',\n        'telefono' as 'N\u00daMERODETEL\u00c9FONODECONTACTO',\n        'fax' as 'N\u00daMERODEFAXDECONTACTO',\n        'email' as 'CORREOELECTR\u00d3NICODECONTACTO',\n        'codzon' as 'C\u00d3DIGODELAZONADELAEMPRESA'        \n        from subsidio.subsi02''',\n    \"subsi15\":'''select * from subsidio.subsi15''',    \n    \"xml4c085\":'''select \n                    CONCAT(\n                    CAST(\n                    CASE \n                    WHEN b.tipide = 1 THEN 'CC'\n                    WHEN b.tipide = 3 THEN 'RC'\n                    WHEN b.tipide = 4 THEN 'CE'\n                    WHEN b.tipide = 6 THEN 'PA'\n                    WHEN b.tipide = 7 THEN 'NI'            \n                    ELSE b.tipide\n                    END AS CHAR\n                    ),\n                    b.nit\n                    ) AS ID_SUCURSAL,\n                    b.nit as NIT_SUCURSAL,\n                    c4.codsuc as COD_SUCURSAL,\n                    c4.detalle as NOMBRE_SUCURSAL,\n                    c4.direccion as DIRECCION_SUCURSAL,\n                    c1.codciu as COD_CIU,\n                    c4.telefono as TELEFONO_SUCURSAL,\n                    c4.fax as FAX_SUCURSAL,\n                    b.divpol as CODZON_SUCURSAL,\n                    c4.ofiafi  as COD_OFICINA,\n                    c4.nomcon as NOMBRE_CONTACTO,\n                    c4.email as EMAIL_SUCURSAL,\n                    c2.nombre as COD_CALIDAD_SUCURSAL,\n                    b.codact as COD_ACTIVIDAD,\n                    c4.codind as COD_INDEPENDIENTES,\n                    c4.traapo as TRABAJADORES_APORTANTES,\n                    c4.valapo as VALOR_APORTES,\n                    c4.actapr as ACTA_APROBACION,\n                    c3.perafi as PERIODO_AFILIACION,\n                    c4.fecmod as ULT_FECHA_AFILIACION,\n                    IF(b.estado = 1, \"A\", \"I\") AS ESTADO_SUCURSAL,\n                    c4.resest as FECHA_RESP_ESTADO,\n                    c1.codest COD_ESTADO,\n                    c1.fecest as FECHA_ESTADO,\n                    c4.totapo as TOTAL_APORTES,\n                    c4.observacion as OBSERVACIONES\n                from xml4.xml4c085 as b\n                left join \n                    (select CONCAT(\n                            CAST(\n                            CASE \n                            WHEN coddoc = 'CC' THEN 1 \n                            WHEN coddoc = 'CE' THEN 4 \n                            WHEN coddoc = 'NI' THEN 7 \n                            WHEN coddoc = 'PA' THEN 6 \n                            WHEN coddoc = 'RC' THEN 3 \n                            ELSE coddoc \n                            END AS CHAR\n                            ),\n                            nit\n                            ) AS cod, \n                            codciu, \n                            codest, \n                            fecest\n                    from subsidio.subsi02 group by cod) as c1\n                on CONCAT(b.tipide, b.nit) = c1.cod \n                left join xml4.xml4b072 as c2\n                on b.tipapo = c2.tipapo\n                left join \n                    (select CONCAT( tipide , nit ) as cod, \n                    MIN(periodo) as perafi\n                    from xml4.xml4c085 \n                    where estado=1 \n                    group by CONCAT( tipide , nit )) as c3\n                on CONCAT(b.tipide, b.nit) = c3.cod\n                left join subsidio.subsi48 as c4\n                on b.nit = c4.nit\n                group by CONCAT(b.tipide, b.nit)''',\n    \"gener08\":'''select \n        codciu as CODIGO_CIIU_DANE,\n        detciu as NOMBRE_CIUDAD,\n        clarur as TIPO_ZONA\n        FROM empresa.gener08'''\n            }\ndim_names = {\n    \"sat25\":'BD_Dim_Tipo_Persona',\n    \"subsi04\":'BD_Dim_Actividad',\n    \"subsi36\":'BD_Dim_Estados_Inactivacion',\n    \"SK13\":'BD_Dim_Zona',\n    \"gener18\":'BD_Dim_Codigo_Documento',\n    \"xml4c085\":'BD_Dim_Empresas_Sucursales',\n    \"BD_Dim_Tipo_Afiliado\":'BD_Dim_Tipo_Afiliado',\n    \"gener08\":'BD_Dim_Ciudades'\n            }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-17 14:13:54,502 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque1/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-17 14:13:54,563 - INFO - CONEXION A BASE MINERVA\n2024-10-17 14:13:54,850 - INFO - Cargando sat25 \n2024-10-17 14:13:54,877 - INFO - Cargada sat25 --- 0.03 seconds ---\n2024-10-17 14:13:54,878 - INFO - Cargando subsi04 \n2024-10-17 14:13:54,927 - INFO - Cargada subsi04 --- 0.05 seconds ---\n2024-10-17 14:13:54,928 - INFO - Cargando subsi36 \n2024-10-17 14:13:54,954 - INFO - Cargada subsi36 --- 0.03 seconds ---\n2024-10-17 14:13:54,955 - INFO - Cargando SK13 \n2024-10-17 14:13:54,978 - INFO - Cargada SK13 --- 0.02 seconds ---\n2024-10-17 14:13:54,979 - INFO - Cargando gener18 \n2024-10-17 14:13:55,003 - INFO - Cargada gener18 --- 0.02 seconds ---\n2024-10-17 14:13:55,004 - INFO - Cargando subsi02 \n2024-10-17 14:13:56,069 - INFO - Cargada subsi02 --- 1.06 seconds ---\n2024-10-17 14:13:56,070 - INFO - Cargando subsi15 \n2024-10-17 14:14:33,645 - INFO - Cargada subsi15 --- 37.58 seconds ---\n2024-10-17 14:14:33,646 - INFO - Cargando xml4c085 \n2024-10-17 14:14:54,840 - INFO - Cargada xml4c085 --- 21.19 seconds ---\n2024-10-17 14:14:54,840 - INFO - Cargando gener08 \n2024-10-17 14:14:54,873 - INFO - Cargada gener08 --- 0.03 seconds ---\n2024-10-17 14:14:54,917 - INFO - CARGUE TABLAS DESDE MYSQL --- gener08 --- 1.01 minutes ---\n</code></pre>"},{"location":"seccion/Bloque1/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se valida la existencia de registros duplicados en cada una de las tablas cargadas en <code>df_structure</code>. Para ello, se excluye la columna <code>id</code> y se comparan las dem\u00e1s columnas. Los registros duplicados se almacenan usando la funci\u00f3n <code>StoreDuplicated</code>, y posteriormente se eliminan los duplicados del dataframe. El proceso se registra en el log para cada tabla, junto con el tiempo total de ejecuci\u00f3n del validador.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlog_tiempo(logger, f'VALIDADOR DUPLICADOS ', time.time() - validador_time)\n</code></pre> <pre><code>2024-10-17 14:14:54,936 - INFO - VALIDADOR TABLA: sat25\n2024-10-17 14:14:54,941 - INFO - VALIDADOR TABLA: subsi04\n2024-10-17 14:14:54,949 - INFO - VALIDADOR TABLA: subsi36\n2024-10-17 14:14:54,955 - INFO - VALIDADOR TABLA: SK13\n2024-10-17 14:14:54,960 - INFO - VALIDADOR TABLA: gener18\n2024-10-17 14:14:55,325 - INFO - VALIDADOR TABLA: subsi02\n2024-10-17 14:15:02,868 - INFO - VALIDADOR TABLA: subsi15\n2024-10-17 14:15:03,012 - INFO - VALIDADOR TABLA: xml4c085\n2024-10-17 14:15:03,017 - INFO - VALIDADOR TABLA: gener08\n2024-10-17 14:15:03,018 - INFO - VALIDADOR DUPLICADOS  --- 8.10 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#transformacion-de-datos","title":"Transformaci\u00f3n de datos","text":"<p>En esta fase, se realiza una limpieza de los datos en cada tabla de <code>df_structure</code>. Se transforman todas las columnas de tipo texto a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final de los valores, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:15:32,784 - INFO - LIMPIEZA --- 29.76 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#creacion-de-una-nueva-dimension-a-partir-de-valores-unicos","title":"Creaci\u00f3n de una nueva dimensi\u00f3n a partir de valores \u00fanicos","text":"<p>Este bloque de c\u00f3digo extrae los valores \u00fanicos de la columna <code>TIPO</code> de la tabla <code>subsi36</code> dentro de <code>df_structure</code>. A partir de esos valores \u00fanicos, se crea un nuevo DataFrame <code>df_valores_unicos</code>, renombrando la columna a <code>TIPO_AFILIADO</code> y agregando manualmente una columna numerada llamada <code>ID_AFILIADO</code>. Finalmente, el nuevo DataFrame se a\u00f1ade a <code>df_structure</code> con la clave <code>DIM_TIPO_AFILIADO</code> para facilitar su posterior utilizaci\u00f3n en el flujo de trabajo.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['subsi36']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ID_TIPO_AFILIADO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ID_TIPO_AFILIADO'])\n\n# Agregar manualmente una columna numerada como 'index'\ndf_valores_unicos['ID_REGISTRO'] = range(len(df_valores_unicos))\n\n# Agregar manualmente el tipo de afiliado\ntipo_afiliado = { 'T':'Trabajadores', 'B': 'Beneficiarios', 'E':'Empresas'}\ndf_valores_unicos['TIPO_AFILIADO'] = df_valores_unicos['ID_TIPO_AFILIADO'].map(tipo_afiliado)\n\n# Ordenar columnas\ncolumnas_orden = ['ID_REGISTRO','ID_TIPO_AFILIADO', 'TIPO_AFILIADO']\ndf_valores_unicos = df_valores_unicos.reindex(columns=columnas_orden)\n\n# Agregar el DataFrame 'df_valores_unicos' a 'df_structure' con una nueva clave\ndf_structure['BD_Dim_Tipo_Afiliado'] = df_valores_unicos\ndf_structure['BD_Dim_Tipo_Afiliado']\n</code></pre> ID_REGISTRO ID_TIPO_AFILIADO TIPO_AFILIADO 0 0 T Trabajadores 1 1 B Beneficiarios 2 2 E Empresas"},{"location":"seccion/Bloque1/#creacion-de-dim_estados_afiliacion-y-dim_calidad_aportante","title":"Creaci\u00f3n de DIM_ESTADOS_AFILIACION  y DIM_CALIDAD_APORTANTE","text":"<p>Este bloque de c\u00f3digo define los estados de afiliaci\u00f3n de los usuarios y la calidad de los aportantes de la organizaci\u00f3n a partir de las definiciones implementadas para la fase 1 del proyecto</p> <pre><code>#Lista de querys DWH\nqr_structure_dwh = {\n    \"DimAuxEstadoAfiliacion\" : '''select \n        estado_old as COD_EST_AFIL, \n        estado_new as ESTADO_AFILIACION\n        from dwh.gb_DimAuxEstadoAfiliacion''',\n    \"DimTipoVinculacion\" : '''select \n        Tipo as COD_CALIDAD_SUCURSAL,\n        Agrupador1 as CALIDAD_APORTANTE_1,\n        Agrupador2 as CALIDAD_APORTANTE_2,\n        AgrupadorSubsi15 as CALIDAD_APORTANTE_SUBSI15\n        from dwh.gb_DimTipoVinculacion'''\n}\n\ndim_names_dwh = {\n    \"DimAuxEstadoAfiliacion\":'BD_Dim_Estados_Afiliacion',\n    \"DimTipoVinculacion\":'BD_Dim_Calidad_Aportante'\n}\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS DWH')\n\n#Conexion a base DWH\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-17 14:15:32,811 - INFO - LECTURA DE QUERYS DWH\n2024-10-17 14:15:32,812 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:33,085 - INFO - Cargando DimAuxEstadoAfiliacion \n2024-10-17 14:15:33,110 - INFO - Cargada DimAuxEstadoAfiliacion --- 0.03 seconds ---\n2024-10-17 14:15:33,111 - INFO - Cargando DimTipoVinculacion \n2024-10-17 14:15:33,140 - INFO - Cargada DimTipoVinculacion --- 0.03 seconds ---\n2024-10-17 14:15:33,186 - INFO - CARGUE TABLAS DESDE MYSQL --- DimTipoVinculacion --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#guardar-en-base-de-datos-dwh_1","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 14:15:33,195 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:33,468 - INFO - Almacenando tabla sat25 en DWH como BD_Dim_Tipo_Persona\n2024-10-17 14:15:33,974 - INFO - Tabla sat25 almacenada correctamente como BD_Dim_Tipo_Persona.\n2024-10-17 14:15:33,975 - INFO - Almacenando tabla subsi04 en DWH como BD_Dim_Actividad\n2024-10-17 14:15:34,673 - INFO - Tabla subsi04 almacenada correctamente como BD_Dim_Actividad.\n2024-10-17 14:15:34,674 - INFO - Almacenando tabla subsi36 en DWH como BD_Dim_Estados_Inactivacion\n2024-10-17 14:15:35,174 - INFO - Tabla subsi36 almacenada correctamente como BD_Dim_Estados_Inactivacion.\n2024-10-17 14:15:35,175 - INFO - Almacenando tabla SK13 en DWH como BD_Dim_Zona\n2024-10-17 14:15:35,652 - INFO - Tabla SK13 almacenada correctamente como BD_Dim_Zona.\n2024-10-17 14:15:35,653 - INFO - Almacenando tabla gener18 en DWH como BD_Dim_Codigo_Documento\n2024-10-17 14:15:36,128 - INFO - Tabla gener18 almacenada correctamente como BD_Dim_Codigo_Documento.\n2024-10-17 14:15:36,129 - WARNING - La clave subsi02 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-17 14:15:36,129 - WARNING - La clave subsi15 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-17 14:15:36,130 - INFO - Almacenando tabla xml4c085 en DWH como BD_Dim_Empresas_Sucursales\n2024-10-17 14:15:40,326 - INFO - Tabla xml4c085 almacenada correctamente como BD_Dim_Empresas_Sucursales.\n2024-10-17 14:15:40,327 - INFO - Almacenando tabla gener08 en DWH como BD_Dim_Ciudades\n2024-10-17 14:15:41,012 - INFO - Tabla gener08 almacenada correctamente como BD_Dim_Ciudades.\n2024-10-17 14:15:41,014 - INFO - Almacenando tabla BD_Dim_Tipo_Afiliado en DWH como BD_Dim_Tipo_Afiliado\n2024-10-17 14:15:41,343 - INFO - Tabla BD_Dim_Tipo_Afiliado almacenada correctamente como BD_Dim_Tipo_Afiliado.\n2024-10-17 14:15:41,403 - INFO - ALMACENAMIENTO ---  --- 8.21 seconds ---\n</code></pre> <pre><code>guardar_en_dwh(df_structure_dwh, dim_names_dwh, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 14:15:41,418 - INFO - CONEXION A BASE DWH\n2024-10-17 14:15:41,689 - INFO - Almacenando tabla DimAuxEstadoAfiliacion en DWH como BD_Dim_Estados_Afiliacion\n2024-10-17 14:15:42,184 - INFO - Tabla DimAuxEstadoAfiliacion almacenada correctamente como BD_Dim_Estados_Afiliacion.\n2024-10-17 14:15:42,185 - INFO - Almacenando tabla DimTipoVinculacion en DWH como BD_Dim_Calidad_Aportante\n2024-10-17 14:15:42,693 - INFO - Tabla DimTipoVinculacion almacenada correctamente como BD_Dim_Calidad_Aportante.\n2024-10-17 14:15:42,751 - INFO - ALMACENAMIENTO ---  --- 1.33 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 14:15:42,762 - INFO - FINAL ETL --- 108.27 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#13-calendariomensual","title":"1.3-CalendarioMensual","text":""},{"location":"seccion/Bloque1/#13-calendariomensual_1","title":"1.3-CalendarioMensual","text":""},{"location":"seccion/Bloque1/#importacion-de-librerias-y-funciones_2","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) junto con funciones personalizadas desde el archivo <code>Funciones.py</code>, asegurando la inclusi\u00f3n del directorio correcto en <code>sys.path</code>.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport time\nimport os\nfrom datetime import date\nimport logging\nfrom dateutil.relativedelta import relativedelta\nstart_time = time.time()\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 17-10-2024 11:49\n</code></pre>"},{"location":"seccion/Bloque1/#configuracion-del-logger_1","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Calendario_Mensual.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Calendario_Mensual.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-17 11:49:12,170 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque1/#creacion-de-la-tabla-calendario","title":"Creaci\u00f3n de la tabla calendario","text":"<p>Se genera un dataframe <code>dfCalendar</code> con un rango de fechas mensuales desde hace 18 meses hasta el presente. Luego se extraen los campos de a\u00f1o y mes, y se crea un campo <code>Periodo</code> en formato <code>A\u00f1oMes</code>. El proceso se registra en el log.</p> <pre><code>#Creaci\u00f3n tabla calendario\ndfCalendar = pd.DataFrame( pd.date_range(date.today() - relativedelta(months = 18),date.today(), \n              freq='MS').tolist() , columns = ['PRIMER_DIA'] )\ndfCalendar['ANIO'] = dfCalendar['PRIMER_DIA'].dt.year\ndfCalendar['MES'] = dfCalendar['PRIMER_DIA'].dt.month\ndfCalendar['PERIODO'] = dfCalendar['ANIO']*100 + dfCalendar['MES']\ndfCalendar['PERIODO'] = dfCalendar['PERIODO'].astype(str)\nlogger.info('CREACION TABLA CALENDARIO')\n# Nuevo orden de columnas\nnuevo_orden = ['PERIODO', 'ANIO', 'MES','PRIMER_DIA']\n\n# Reordenar usando reindex\ndfCalendar = dfCalendar.reindex(columns=nuevo_orden).copy()\n</code></pre> <pre><code>2024-10-17 11:49:12,186 - INFO - CREACION TABLA CALENDARIO\n</code></pre>"},{"location":"seccion/Bloque1/#guardar-en-base-de-datos-dwh_2","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfCalendar</code> en la tabla <code>DimCalendario</code> de la base DWH, reemplazando su contenido si la tabla ya existe. Se registra el tiempo que toma esta operaci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfCalendar, 'BD_Dim_Calendario', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-17 11:49:12,192 - INFO - CONEXION A BASE DWH\n2024-10-17 11:49:12,632 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Calendario\n2024-10-17 11:49:13,212 - INFO - Tabla almacenada correctamente.\n2024-10-17 11:49:13,299 - INFO - ALMACENAMIENTO ---  --- 1.11 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 11:49:13,311 - INFO - FINAL ETL --- 1.15 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#16-factdatoscontacto","title":"1.6-FactDatosContacto","text":""},{"location":"seccion/Bloque1/#16-factdatoscontacto_1","title":"1.6-FactDatosContacto","text":""},{"location":"seccion/Bloque1/#importacion-de-librerias-y-funciones_3","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, y <code>dateutil</code>, junto con funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, lo que asegura la correcta importaci\u00f3n de las funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, RemoveDuplicated, obtener_conexion, cargar_tablas, guardar_en_dwh, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 14-10-2024 09:07\n</code></pre>"},{"location":"seccion/Bloque1/#configuracion-del-logger_2","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>FactDatosContacto.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactDatosContacto.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-14 09:07:13,360 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-consultas-sql_2","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un diccionario <code>qr_structure</code> con consultas SQL para extraer datos de las tablas <code>subsi15</code>, <code>subsi02</code>, y <code>subsi22</code> de la base de datos <code>subsidio</code>. Estas consultas generan un identificador \u00fanico <code>id</code> y extraen diversas columnas clave para los trabajadores, empresas y beneficiarios. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi15_dc\":'''select \nCONCAT( coddoc, cedtra) as id,codsuc,codlis,cedtra,coddoc,\ndireccion,codciu,codbar,telefono,email,codzon,rural,agro,captra,\ntipdis,horas,salario,tipsal,fecsal,sexo,estciv,tipcon,feccon,trasin,\nvivienda,cabhog,nivedu,tipcot,vendedor,empleador,codcue,ofides,codban,\nnumcue,tipcue,fecemi,feccar,codcat,fecnac,ciunac,fecing,fecpre,fecafi,\nfecsis,fecmod,usumod,usuario,giro,codgir,estado,codest,fecest,carnet,benef,\nruaf,ruasub,fecrua,nota,habdat,notcor,fecexp,codfonpen,numafipen,orisex,\ncodocu,facvul,codetn,codpai,celular1,celular2,tiptar,cedant,pueblo,resguardo, 1 as Trabajador\nfrom subsidio.subsi15''',\n\n##############################SUBSI02###############################################\n    \"subsi02_dc\":'''select \nCONCAT( coddoc , nit ) as id,nit,digver,tipper,coddoc,razsoc,sigla,nomcom,\ncoddocreppri,cedrep,repleg,coddocrepsup,cedrepsup,replegsup,jefper,direccion,\ncodciu,codbar,celular,telefono,fax,email,codzon,dirpri,ciupri,celpri,telpri,\nfaxpri,emailpri,nomcon,ofiafi,codase,calemp,tipemp,tipsoc,tipapo,forpre,pymes,\ncontratista,colegio,habdat,notcor,matmer,codact,codind,feccer,actapr,fecapr,\nfecafi,fecsis,fecmod,estado,resest,codest,fecest,totapo,nomini,coddocrepleg,\npriaperepleg,segaperepleg,prinomrepleg,segnomrepleg,codcaj,pagweb,dirnot,emailnot, 1 as Empresa\nfrom subsidio.subsi02''',\n##############################SUBSI22_23###############################################\n    \"subsi22_dc\":'''select \nCONCAT( coddoc,documento) as id, codben,documento,fecexp,coddoc,priape,segape,prinom,segnom,parent,huerfano,tiphij,\ncaptra,tipdis,nivedu,sexo,fecnac,ciunac,calendario,giro,codgir,estado,codest,fecest,\nregimen,tipocotizante,created_at,updated_at, 1 as Beneficiario\nfrom subsidio.subsi22'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-14 09:07:15,635 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque1/#conexion-y-carga-de-tablas-desde-sql_1","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n# Ejemplo de c\u00f3mo llamarla\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-14 09:07:15,656 - INFO - CONEXION A BASE MINERVA\n2024-10-14 09:07:16,177 - INFO - Cargando subsi15_dc \n2024-10-14 09:08:10,668 - INFO - Cargada subsi15_dc --- 54.49 seconds ---\n2024-10-14 09:08:10,669 - INFO - Cargando subsi02_dc \n2024-10-14 09:08:20,958 - INFO - Cargada subsi02_dc --- 10.29 seconds ---\n2024-10-14 09:08:20,960 - INFO - Cargando subsi22_dc \n2024-10-14 09:08:51,585 - INFO - Cargada subsi22_dc --- 30.63 seconds ---\n2024-10-14 09:08:51,768 - INFO - CARGUE TABLAS DESDE MYSQL --- 96.11 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#validador-de-campos-repetidos_1","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas del diccionario <code>df_structure</code>. Si no existe la columna <code>id</code>, se crea concatenando las columnas <code>numdoc</code> y <code>coddoc</code>. Luego, se compara el resto de las columnas para detectar duplicados. Los registros duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y se eliminan los duplicados con <code>RemoveDuplicated</code>. Se registran en el log los resultados de la validaci\u00f3n, y se manejan adecuadamente los posibles errores.</p> <pre><code># Validador de campos repetidos\nvalidador_time = time.time()\n\n# Iterar sobre todas las tablas excepto 'subsi16'\nfor ky in [x for x in df_structure.keys()]:\n    df = df_structure[ky]\n\n    # Validar si la columna 'id' existe; si no, crearla\n    if 'id' not in df.columns:\n        if 'numdoc' in df.columns and 'coddoc' in df.columns:\n            # Crear la columna 'id' concatenando 'numdoc' y 'coddoc'\n            df = df.assign(id=df['numdoc'].astype(str) + df['coddoc'].astype(str))\n            logger.info(f\"Columna 'id' creada en el DataFrame: {ky}\")\n        else:\n            # Si no existen las columnas necesarias para crear 'id', saltar la tabla\n            logger.error(f\"No se encontraron las columnas 'numdoc' y 'coddoc' necesarias para crear 'id' en el DataFrame: {ky}\")\n            continue  # Saltar a la siguiente tabla si no se pueden crear los valores de 'id'\n\n    # Asegurarse de que la columna 'id' existe antes de proceder\n    if 'id' in df.columns:\n        # Definir las columnas para comparar, excluyendo 'id'\n        ColumnsToCompare = [x for x in df.columns if x != 'id']\n\n        # Imprimir el nombre de la tabla en proceso\n        print(f\"Procesando tabla: {ky}\")\n\n        # Almacenar duplicados\n        try:\n            StoreDuplicated('id', ColumnsToCompare, df, f'trazaDuplicados_{ky}')\n        except KeyError as e:\n            logger.error(f\"Error en StoreDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Limpieza inicial para quitar duplicados\n        try:\n            df_structure[ky] = RemoveDuplicated('id', 'fecest', df)\n        except KeyError as e:\n            logger.error(f\"Error en RemoveDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Registrar la validaci\u00f3n en el log\n        logger.info(f'VALIDADOR TABLA: {ky}')\n    else:\n        logger.error(f\"La columna 'id' no fue creada correctamente en el DataFrame: {ky}\")\n\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>Procesando tabla: subsi15_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi15_dc.csv\n\n\n2024-10-14 09:09:16,105 - INFO - VALIDADOR TABLA: subsi15_dc\n\n\nProcesando tabla: subsi02_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi02_dc.csv\n\n\n2024-10-14 09:09:18,542 - INFO - VALIDADOR TABLA: subsi02_dc\n\n\nProcesando tabla: subsi22_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_subsi22_dc.csv\n\n\n2024-10-14 09:09:25,954 - INFO - VALIDADOR TABLA: subsi22_dc\n2024-10-14 09:09:25,956 - INFO - VALIDADOR DUPLICADOS --- 34.17 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-columnas-con-sus-tipos","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas de cada tabla en el diccionario <code>df_structure</code> en tres tipos: num\u00e9ricas, de fechas y de texto. Las columnas de fechas, identificadas por el prefijo \"fec\", se convierten al formato de fecha utilizando <code>pd.to_datetime()</code>. Se registran los tipos de columnas en tres diccionarios (<code>numericColumns</code>, <code>datesColumns</code>, <code>textColumns</code>) y se asegura que las transformaciones se apliquen correctamente. El tiempo total de este proceso se registra en el log.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 09:09:29,742 - INFO - TRANSFORMACION 2 --- 3.75 seconds ---\n</code></pre> <pre><code>df_structure['subsi15_dc'].columns.to_series().groupby(df_structure['subsi15_dc'].dtypes).groups\n</code></pre> <pre><code>{int64: ['codbar', 'horas', 'salario', 'usuario', 'orisex', 'facvul', 'codetn', 'Trabajador'], datetime64[ns]: ['fecsal', 'feccon', 'fecemi', 'feccar', 'fecnac', 'fecing', 'fecpre', 'fecafi', 'fecsis', 'fecmod', 'fecest', 'fecrua', 'fecexp'], float64: ['usumod', 'ruasub'], object: ['id', 'codsuc', 'codlis', 'cedtra', 'coddoc', 'direccion', 'codciu', 'telefono', 'email', 'codzon', 'rural', 'agro', 'captra', 'tipdis', 'tipsal', 'sexo', 'estciv', 'tipcon', 'trasin', 'vivienda', 'cabhog', 'nivedu', 'tipcot', 'vendedor', 'empleador', 'codcue', 'ofides', 'codban', 'numcue', 'tipcue', 'codcat', 'ciunac', 'giro', 'codgir', 'estado', 'codest', 'carnet', 'benef', 'ruaf', 'nota', 'habdat', 'notcor', 'codfonpen', 'numafipen', 'codocu', 'codpai', 'celular1', 'celular2', 'tiptar', 'cedant', 'pueblo', 'resguardo']}\n</code></pre>"},{"location":"seccion/Bloque1/#transformacion-y-limpieza-de-datos","title":"Transformaci\u00f3n y limpieza de datos","text":"<p>Se realiza una limpieza de las columnas de tipo texto en todas las tablas del diccionario <code>df_structure</code>, excepto en <code>subsi15</code> y <code>subsi16</code>. Las columnas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list( [x for x in df_structure.keys() if  x not in [ 'subsi15' , 'subsi16' ] ]   ):\n    print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NAN', np.nan)\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>subsi15_dc\n\n\nC:\\Users\\flavi\\AppData\\Local\\Temp\\ipykernel_9096\\136011473.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\n\n\nsubsi02_dc\n\n\nC:\\Users\\flavi\\AppData\\Local\\Temp\\ipykernel_9096\\136011473.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\n\n\nsubsi22_dc\n\n\n2024-10-14 09:10:20,666 - INFO - LIMPIEZA --- 50.87 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#concatenacion-de-tablas","title":"Concatenaci\u00f3n de tablas","text":"<p>Se concatenan todas las tablas presentes en el diccionario <code>df_structure</code> en un \u00fanico dataframe llamado <code>df_struc_Total</code>. La concatenaci\u00f3n se realiza ignorando los \u00edndices previos, lo que crea un dataframe consolidado con todos los registros.</p> <pre><code>#Concatenacion Tablas\ndf_struc_Total = pd.concat( list(df_structure.values()) , ignore_index = True )\n</code></pre> <pre><code>#df_struc_Total = df_struc_Total.drop_duplicates(subset='id', keep='last')\n#df_struc_Total.columns.tolist()\n</code></pre>"},{"location":"seccion/Bloque1/#agregacion-de-datos-por-periodo-trimestral","title":"Agregaci\u00f3n de datos por periodo trimestral","text":"<p>Se realiza una agregaci\u00f3n del dataframe <code>df_struc_Total</code> por <code>id</code> y por trimestre, utilizando la columna <code>fecafi</code> para generar los periodos trimestrales. Se aplican diferentes funciones de agregaci\u00f3n como <code>last</code>, <code>first</code>, <code>sum</code>, y <code>nunique</code> seg\u00fan corresponda a cada columna. Despu\u00e9s de la agregaci\u00f3n, se elimina la columna <code>Trimestre</code> si no es necesaria en el dataframe final. El proceso se registra en el log, indicando tanto el inicio como la finalizaci\u00f3n de la agregaci\u00f3n.</p> <pre><code># Iniciar la agregaci\u00f3n en el log\nlogger.info('INICIO DE AGREGACI\u00d3N POR PERIODO')\n\n# Convertir 'fecafi' a un formato trimestral para la agrupaci\u00f3n y evitar fragmentaci\u00f3n\ndf_struc_Total = pd.concat([df_struc_Total, pd.to_datetime(df_struc_Total['fecafi'], errors='coerce').dt.to_period('Q').rename('Trimestre')], axis=1)\n\n# Agrupar por 'id' y 'Trimestre'\ndf_agg = df_struc_Total.groupby(['id', 'Trimestre'], as_index=False).agg({\n    'codsuc': 'last', 'codlis': 'last',  'cedtra': 'last',  'coddoc': 'last',  'direccion': 'last',  'codciu': 'last',  'codbar': 'last',  'telefono': 'last',\n    'email': 'last',  'codzon': 'last',  'rural': 'last',  'agro': 'last',  'captra': 'last',  'tipdis': 'last',  'horas': 'sum',              # Sumar las horas \n    'salario': 'last',           # Mantener el \u00faltimo salario registrado\n    'tipsal': 'last',  'fecsal': 'last',  'sexo': 'last',  'estciv': 'last',  'tipcon': 'last',  'feccon': 'last',  'trasin': 'last',  'vivienda': 'last',\n    'cabhog': 'last',  'nivedu': 'last',  'tipcot': 'last',  'vendedor': 'last',  'empleador': 'last',  'codcue': 'last',  'ofides': 'last',  'codban': 'last',\n    'numcue': 'last',  'tipcue': 'last',  'fecemi': 'last',  'feccar': 'last',  'codcat': 'last',  'fecnac': 'last',  'ciunac': 'last',  'fecing': 'last',\n    'fecpre': 'last',  'fecafi': 'first',           # Mantener la primera fecha \n    'fecsis': 'last',  'fecmod': 'last',  'usumod': 'last',  'usuario': 'last',  'giro': 'last',  'codgir': 'last',  'estado': 'last',  'codest': 'last',\n    'fecest': 'last',  'carnet': 'last',  'benef': 'last',  'ruaf': 'last',  'ruasub': 'last',  'fecrua': 'last',  'nota': 'last',  'habdat': 'last',\n    'notcor': 'last',  'fecexp': 'last',  'codfonpen': 'last',  'numafipen': 'last',  'orisex': 'last',  'codocu': 'last',  'facvul': 'last',  'codetn': 'last',\n    'codpai': 'last',  'celular1': 'last',  'celular2': 'last',  'tiptar': 'last',  'cedant': 'last',  'pueblo': 'last',  'resguardo': 'last',  'Trabajador': 'last',\n    'nit': 'last',   'digver': 'last',  'tipper': 'last',  'razsoc': 'last',  'sigla': 'last',  'nomcom': 'last',  'coddocreppri': 'last',  'cedrep': 'last',\n    'repleg': 'last',  'coddocrepsup': 'last',  'cedrepsup': 'last',  'replegsup': 'last',  'jefper': 'last',  'celular': 'last',  'fax': 'last',  'dirpri': 'last',\n    'ciupri': 'last',  'celpri': 'last',  'telpri': 'last',  'faxpri': 'last',  'emailpri': 'last',  'nomcon': 'last',  'ofiafi': 'last',  'codase': 'last',\n    'calemp': 'last',  'tipemp': 'last',  'tipsoc': 'last',  'tipapo': 'last',  'forpre': 'last',  'pymes': 'last',  'contratista': 'last',  'colegio': 'last',\n    'matmer': 'last',  'codact': 'last',  'codind': 'last',  'feccer': 'last',  'actapr': 'last',  'fecapr': 'last',  'resest': 'last',  'totapo': 'last',\n    'nomini': 'last',  'coddocrepleg': 'last',  'priaperepleg': 'last',  'segaperepleg': 'last',  'prinomrepleg': 'last',  'segnomrepleg': 'last',  'codcaj': 'last',  'pagweb': 'last',\n    'dirnot': 'last',  'emailnot': 'last',  'Empresa': 'last',  'codben': 'nunique',          # N\u00famero de beneficiarios \u00fanicos\n    'documento': 'last',  'priape': 'last',  'segape': 'last',  'prinom': 'last',   'segnom': 'last',  'parent': 'last',  'huerfano': 'last',  'tiphij': 'last',\n    'calendario': 'last',  'regimen': 'last',  'tipocotizante': 'last',  'created_at': 'last',  'updated_at': 'last',  'Beneficiario': 'last'\n})\n\n# Eliminar columna 'Trimestre' si no se necesita en el DataFrame final\ndf_agg.drop(columns=['Trimestre'], inplace=True)\n\n# Finalizar la agregaci\u00f3n en el log\nlogger.info('AGREGACI\u00d3N POR PERIODO FINALIZADA')\n\n# Asignar el DataFrame agrupado a df_struc_Total para la siguiente etapa\ndf_struc_Total = df_agg\n</code></pre> <pre><code>2024-10-14 09:10:22,167 - INFO - INICIO DE AGREGACI\u00d3N POR PERIODO\n2024-10-14 09:10:46,655 - INFO - AGREGACI\u00d3N POR PERIODO FINALIZADA\n</code></pre> <pre><code>df_struc_Total = df_agg\n# Convertir todos los nombres de las columnas de df_struc_Total a may\u00fasculas\ndf_struc_Total.columns = df_struc_Total.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf_struc_Total = df_struc_Total.rename(columns={\n    'ID': 'ID_AFILIADO', \n    'CODCIU': 'COD_CIU',\n    'CODSUC':'COD_SUCURSAL',\n    'CODZON':'COD_ZON',\n    'ESTCIV':'ESTADO_CIVIL',\n    'NUMCUE':'NUMERO_CUENTA',\n    'TIPCUE':'TIPO_CUENTA',\n    'CODBAN':'COD_BANCO'\n    })\n\n# Seleccionar solo las columnas que deseas conservar\ndf_struc_Total = df_struc_Total[[\n    'ID_AFILIADO', 'COD_CIU', 'COD_SUCURSAL', 'COD_ZON', 'ESTADO_CIVIL', 'NUMERO_CUENTA', 'TIPO_CUENTA', 'COD_BANCO',\n    'TELEFONO','EMAIL','DIRECCION'\n    ]]\n</code></pre> <pre><code>df_struc_Total\n</code></pre> ID_AFILIADO COD_CIU COD_SUCURSAL COD_ZON ESTADO_CIVIL NUMERO_CUENTA TIPO_CUENTA COD_BANCO TELEFONO EMAIL DIRECCION 0 CC04619 47001 001 47001 1 None E NaN None None CRA 36 31-60 1 CC05715 47001 001 47001 1 None E NaN 4311683 None CALLE 13 2 27 2 CC09039 47001 001 47001 1 None E NaN None None None 3 CC090913 47001 001 47001 1 None E NaN 4502857 None MZ 33 CASA 20 PARQUE 4 CC091013 47001 001 47001 1 None E NaN None None MZ A CASA 15 ... ... ... ... ... ... ... ... ... ... ... ... 463467 TI98120770829 47001 001 47001 1 None E NaN None None MZ 125 CS2 CIUDADELA 463468 TI99012615902 47001 001 47001 1 None E NaN 4201354 TIENDASAMARIASAS@GAMAIL.COM CL 12 N 17-21 463469 TI99022410527 47001 001 47001 1 None E NaN None None CRA 50 N.24A-150 463470 TI99032003377 47001 001 47001 1 None E NaN None None MZ 14 CS 6 URB EL CISNE 463471 TI99040317160 47001 001 47001 1 None E NaN None CAMILO036@GMAIL.COM CL 32 N31-28 COREA <p>463472 rows \u00d7 11 columns</p>"},{"location":"seccion/Bloque1/#guardar-en-la-base-de-datos-dwh-con-manejo-de-errores","title":"Guardar en la base de datos DWH con manejo de errores","text":"<p>Se intenta guardar los primeros 1000 registros del dataframe <code>df_struc_Total</code> en la tabla <code>BD_FactDatosContactos</code> en la base de datos DWH. Si el proceso es exitoso, se registra el tiempo de almacenamiento en el log. En caso de error, se realiza un rollback para evitar transacciones pendientes y se registra el error en el log.</p> <pre><code># Llamada a la funci\u00f3n para un \u00fanico DataFrame\nguardar_en_dwh(df_struc_Total, 'BD_FactDatosContactos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-14 09:10:50,097 - INFO - CONEXION A BASE DWH\n2024-10-14 09:10:50,662 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactDatosContactos\n2024-10-14 09:11:44,758 - INFO - Tabla almacenada correctamente.\n2024-10-14 09:11:45,308 - INFO - ALMACENAMIENTO --- 55.21 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 09:11:45,346 - INFO - FINAL ETL --- 272.06 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#15-factafiliacion","title":"1.5-FactAfiliacion","text":""},{"location":"seccion/Bloque1/#15-factafiliacion_1","title":"1.5-FactAfiliacion","text":""},{"location":"seccion/Bloque1/#importacion-de-librerias-y-funciones_4","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, cargar_tablas, StoreDuplicated, RemoveDuplicated, RemoveErrors, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 14-10-2024 15:53\n</code></pre>"},{"location":"seccion/Bloque1/#configuracion-del-logger_3","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Afiliacion_Historica.log</code>, utilizando el nivel de detalle <code>INFO</code>. El proceso se inicia con un registro de inicio en el log.</p> <pre><code>logger = setup_logger(log_filename='Afiliacion_Historica.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-14 15:53:26,401 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-consultas-sql_3","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un conjunto de consultas SQL en <code>qr_structure</code> para extraer datos de diversas tablas de la base de datos <code>subsidio</code>. Estas consultas filtran los registros basados en fechas relevantes para los \u00faltimos 2 a\u00f1os. \u00a1Todo listo para leer los datos y continuar el proceso! \ud83d\udcca</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi15\":'''select * from (\nselect nit, fecafi , fecsis, fecest,horas, salario, codest, estado,coddoc, cedtra as numdocumento,\nCONCAT( coddoc, cedtra) as id,\nIF( estado &lt;&gt; 'A' , fecest , NULL ) as fecret, \nCONCAT(coddoc, cedtra,nit, fecafi) as keyTrab,\ncodsuc ,\ncodlis ,\ncodcat \nFROM subsidio.subsi15\n) as ll\nwhere\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)''',\n##############################SUBSI16###############################################\n    \"subsi16\":''' SELECT t1.id, t1.nit,  t1.fecafi  , t1.fecret, t1.fecsis, codest , keyTrab,coddoc, cedtra as numdocumento, estado from (\n select CONCAT( coddoc, cedtra) as id, nit, fecafi  , fecret, fecsis, codest, CONCAT(coddoc, cedtra,nit, fecafi) as keyTrab, coddoc, cedtra, NULL as estado \nfrom subsidio.subsi16 ) as t1\nwhere\n(t1.fecsis &lt;= CURRENT_DATE()\nAND t1.fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (t1.fecsis &lt;= CURRENT_DATE()\nAND t1.fecret IS NULL)\nor\n(t1.fecafi &lt;= CURRENT_DATE()\nAND t1.fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (t1.fecafi &lt;= CURRENT_DATE()\nAND t1.fecret IS NULL)''',\n##############################SUBSI02###############################################\n    \"subsi02\":'''select * from (\nselect CONCAT( coddoc , nit ) as id, fecafi , fecsis, fecest, calemp, tipper,codact ,codest , estado, coddoc, nit as numdocumento,\nIF( estado &lt;&gt; 'A' , fecest , NULL ) as fecret \nfrom subsidio.subsi02\n) as emp\nwhere\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)''',\n##############################SUBSI22_23###############################################\n    \"subsi22_23\":'''select * from (\nselect id, p1.codben  , p2.codcat , p2.codlis , p2.codsuc , p1.fecafi, p1.fecret,\np1.fecest,p1.codest ,p1.estado, p1.fecsis,  p1.captra, p1.nivedu, p1.coddoc,p1.documento as numdocumento, p1.coddocTrab, p1.cedtra\nFROM (SELECT CONCAT( d1.coddoc,d1.documento) as id , d1.fecest,\n            IF( d1.estado = 'I' , d1.fecest , NULL ) as fecret,\n            d1.codben, d2.fecafi, d1.estado, d2.coddoc as coddocTrab, d2.cedtra, d1.codest, d2.fecsis, d1.captra, d1.nivedu, d1.coddoc,d1.documento\n            FROM\n            subsidio.subsi22 as d1\n            RIGHT jOIN\n            subsidio.subsi23 as d2\n            ON d1.codben=d2.codben) as p1\n    LEFT JOIN  (select * from subsidio.subsi15) as p2  #tabla trabajadores\n    ON p1.coddoc = p2.coddoc\n    AND p1.cedtra = p2.cedtra\n    ) as ben\n    where\n(fecsis &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecsis &lt;= CURRENT_DATE()\nAND fecret IS NULL)\nor\n(fecafi &lt;= CURRENT_DATE()\nAND fecret &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) as CHAR)     )\nOR (fecafi &lt;= CURRENT_DATE()\nAND fecret IS NULL)'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n</code></pre> <pre><code>2024-10-14 15:54:00,525 - INFO - CONEXION A BASE MINERVA\n</code></pre> <pre><code>#Lista de querys\nqr_structure = {\n##############################xml4c086###############################################\n    \"xml4c086\":'''\n        SELECT \n            * \n        FROM\n            (SELECT \n                p1.nit, \n                p1.salario, \n                p1.estado, \n                p1.tipide, \n                p1.id, \n                p1.cod,  \n                p1.periodo, \n                p2.perret, \n                p3.perafi,\n                p4.horas, \n                p4.fecsis, \n                p4.fecest, \n                p4.codest, \n                p4.codsuc, \n                p4.codlis,\n                p5.nombre AS codcat,\n                CONCAT(p1.cod, p1.nit, p3.perafi) AS keyTrab\n        FROM\n            (SELECT \n                nit, \n                salario, \n                'A' as estado, \n                coddoc as tipide, \n                cedtra as id, \n                CONCAT( coddoc , cedtra ) AS cod,  \n                periodo,  \n                codcat  \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra ), periodo) AS p1\n        LEFT JOIN \n            (SELECT \n                CONCAT( coddoc , cedtra ) AS cod,\n                IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c086), MAX(periodo), NULL) AS perret \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) AS p2\n            ON p1.cod = p2.cod \n        LEFT JOIN \n            (SELECT \n                CONCAT( coddoc , cedtra ) AS cod,\n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) AS p3\n            ON p1.cod = p3.cod\n        LEFT JOIN \n            (SELECT \n                CONCAT(\n                CAST(\n                CASE \n                WHEN coddoc = 'CC' THEN 1 \n                WHEN coddoc = 'CD' THEN 8\n                WHEN coddoc = 'CE' THEN 4 \n                WHEN coddoc = 'PE' THEN 9            \n                WHEN coddoc = 'PA' THEN 6 \n                WHEN coddoc = 'PT' THEN 15 \n                WHEN coddoc = 'TI' THEN 2\n                ELSE coddoc \n                END AS CHAR),\n                cedtra\n                ) AS cod, \n                horas, \n                fecsis, \n                fecest, \n                codest, \n                codsuc, \n                codlis \n            FROM \n                subsidio.subsi15 GROUP BY cod) AS p4\n            ON p1.cod = p4.cod\n        LEFT JOIN xml4.xml4b008 AS p5\n            ON p1.codcat = p5.codcat\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= @fechaInferior)\n            OR \n            (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND perret IS NULL)) AS m\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior''',\n\n##############################xml4c085###############################################\n\n    \"xml4c085\":'''\n        SELECT \n            periodo, \n            nit as id, \n            tipide, \n            tipper,\n            MIN(perafi) AS perafi,\n            MIN(perret) AS perret,\n            MAX(codest) AS codest,\n            cod, \n            estado,\n            MAX(nombre) AS calemp,\n            MAX(codact) AS codact, \n            MIN(fecsis) AS fecsis,\n            MIN(fecest) AS fecest\n        FROM\n            (SELECT \n            b.nit, \n            b.periodo, \n            b.tipide, \n            b.cod, \n            b.tipper, \n            b.estado,\n            c.perafi, \n            a.nombre,\n            b.fecret AS perret, \n            b.codact, \n            s.fecest,\n            s.fecsis,\n            s.codest\n        FROM\n        (SELECT \n            CONCAT( tipide , nit ) AS cod,\n            nit, \n            tipide,\n            periodo, \n            estado, \n            IF(tipide=7, 'Jur\u00eddica', 'Natural') AS tipper,\n            codact,\n            tipapo,\n            IF( estado = 2 OR estado = 4, periodo , NULL ) AS fecret\n        FROM xml4.xml4c085 GROUP BY cod, periodo) AS b\n        LEFT JOIN \n            (SELECT \n                CONCAT( tipide , nit ) AS cod, \n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c085 \n            WHERE \n                estado=1 GROUP BY CONCAT( tipide , nit )) AS c\n            ON b.cod = c.cod\n        LEFT JOIN \n            (SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN coddoc = 'CC' THEN 1 \n            WHEN coddoc = 'CE' THEN 4 \n            WHEN coddoc = 'NI' THEN 7 \n            WHEN coddoc = 'PA' THEN 6 \n            WHEN coddoc = 'RC' THEN 3 \n            ELSE coddoc \n            END AS CHAR\n            ),\n            nit\n            ) AS cod, \n            fecsis, \n            fecest, \n            codest\n        FROM \n            subsidio.subsi02 GROUP BY cod) AS s\n        ON b.cod = s.cod\n        LEFT JOIN \n            xml4.xml4b072 AS a\n        ON b.tipapo = a.tipapo\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND CONCAT(SUBSTRING(fecret, 1, 4), '-', SUBSTRING(fecret, 5, 2), '-01') &gt;= @fechaInferior)\n            OR \n            (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n            AND fecret IS NULL)) AS p1\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior\n\n        GROUP BY periodo, cod, estado''',\n\n##############################xml4c087###############################################\n\n    \"xml4c087\":'''\n        SELECT \n            *\n         FROM  \n         (SELECT \n             d1.periodo,  \n             CONCAT(d1.coddoc,d1.documento) AS cod,\n             d1.cedtra,\n             d1.codtra AS coddoc,\n             d1.coddoc AS coddocben,\n             d1.documento,\n             d6.nombre AS codcat,\n             d4.perafi,\n             d2.codest,\n             d2.fecest,\n             d3.perret,\n             d2.codben, \n             'A' AS estado,\n             d5.fecsis,\n             d1.codigo_dicap AS captra,\n             d2.nivedu,\n             d7.codlis,\n             d7.codsuc\n        FROM\n            xml4.xml4c087 AS d1 \n        LEFT JOIN \n          (SELECT \n              documento, \n              codben,\n              fecest,\n              codest, \n              nivedu\n            FROM \n                subsidio.subsi22 GROUP BY documento) AS d2\n        ON d1.documento=d2.documento\n        LEFT JOIN \n        (SELECT \n            coddoc, \n            documento,\n            IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c087), MAX(periodo), NULL) AS perret \n        FROM \n            xml4.xml4c087 GROUP BY coddoc, documento) AS d3\n        ON d1.coddoc = d3.coddoc\n        AND d1.documento = d3.documento\n        LEFT JOIN \n            (SELECT \n                coddoc, \n                documento,\n                MIN(periodo) AS perafi \n            FROM \n                xml4.xml4c087 GROUP BY coddoc, documento) AS d4\n        ON d1.coddoc = d4.coddoc\n        AND d1.documento = d4.documento\n        LEFT JOIN\n            subsidio.subsi23 AS d5\n        ON d1.codben=d5.codben\n         LEFT JOIN xml4.xml4b008 AS d6\n         ON d1.codcat = d6.codcat\n        LEFT JOIN subsidio.subsi15 AS d7\n        ON d1.cedtra = d7.cedtra\n        AND d1.coddoc = d7.coddoc\n\n        WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= @fechaInferior)\n        OR (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND perret IS NULL)\n        GROUP BY \n            d1.documento, d1.coddoc, periodo) as p1\n       WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= @fechaSuperior\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= @fechaInferior\n\n'''\n}\n\n# Conectar a la base de datos y ejecutar el query\nwith motor.begin() as conn:\n    # Ejecutar las sentencias SET\n    conn.execute(sa.text(\"SET @fechaInferior := DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH);\"))\n    conn.execute(sa.text(\"SET @fechaSuperior := CURRENT_DATE();\"))\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-14 16:24:08,003 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque1/#conexion-y-llamada-a-funcion-de-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y llamada a funci\u00f3n de carga de tablas desde SQL","text":"<p>En este bloque de c\u00f3digo, se realiza la conexi\u00f3n a la base de datos Minerva y se utiliza la funci\u00f3n <code>cargar_tablas</code> para cargar m\u00faltiples tablas desde SQL. La funci\u00f3n toma como par\u00e1metros el motor de conexi\u00f3n, las consultas SQL, la estructura de los DataFrames y el <code>logger</code> para registrar el proceso. Esto optimiza la reutilizaci\u00f3n del c\u00f3digo y mantiene un flujo centralizado para el cargue de datos.</p> <pre><code>#Conexion a base Minerva\n#motor = create_engine(obtener_conexion('minerva'))\n#logger.info('CONEXION A BASE MINERVA')\n# Ejemplo de c\u00f3mo llamarla\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-14 16:24:11,900 - INFO - Cargando xml4c086 \n2024-10-14 16:25:25,950 - INFO - Cargada xml4c086 --- 74.05 seconds ---\n2024-10-14 16:25:25,951 - INFO - Cargando xml4c085 \n2024-10-14 16:25:31,199 - INFO - Cargada xml4c085 --- 5.25 seconds ---\n2024-10-14 16:25:31,200 - INFO - Cargando xml4c087 \n2024-10-14 16:31:36,731 - INFO - Cargada xml4c087 --- 365.53 seconds ---\n2024-10-14 16:31:37,038 - INFO - CARGUE TABLAS DESDE MYSQL --- 445.14 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#validador-de-campos-repetidos_2","title":"Validador de campos repetidos","text":"<p>Se valida la presencia de registros duplicados en todas las tablas. Si no existe la columna <code>cod</code>, se crea concatenando las columnas <code>numdoc</code> y <code>coddoc</code>. A continuaci\u00f3n, se comparan las columnas relevantes para detectar duplicados y se almacenan los resultados usando <code>StoreDuplicated</code>. Luego, se eliminan los duplicados usando <code>RemoveDuplicated</code>, y se registra el proceso en el log. Cualquier error se maneja de manera adecuada, registrando tambi\u00e9n los casos en que no se puede crear la columna <code>cod</code>.</p> <pre><code># Validador de campos repetidos\nvalidador_time = time.time()\n\n# Iterar sobre todas las tablas\nfor ky in [x for x in df_structure.keys()]:\n    df = df_structure[ky]\n\n    # Validar si la columna 'cod' existe; si no, crearla\n    if 'cod' not in df.columns:\n        if 'numdoc' in df.columns and 'coddoc' in df.columns:\n            # Crear la columna 'cod' concatenando 'numdoc' y 'coddoc'\n            df = df.assign(cod=df['numdoc'].astype(str) + df['coddoc'].astype(str))\n            logger.info(f\"Columna 'id' creada en el DataFrame: {ky}\")\n        else:\n            # Si no existen las columnas necesarias para crear 'cod', saltar la tabla\n            logger.error(f\"No se encontraron las columnas 'numdoc' y 'coddoc' necesarias para crear 'id' en el DataFrame: {ky}\")\n            continue  # Saltar a la siguiente tabla si no se pueden crear los valores de 'cod'\n\n    # Asegurarse de que la columna 'cod' existe antes de proceder\n    if 'cod' in df.columns:\n        # Definir las columnas para comparar, excluyendo 'id'\n        ColumnsToCompare = [x for x in df.columns if x != 'cod']\n\n        # Imprimir el nombre de la tabla en proceso\n        print(f\"Procesando tabla: {ky}\")\n\n        # Almacenar duplicados\n        try:\n            StoreDuplicated('cod', ColumnsToCompare, df, f'trazaDuplicados_{ky}')\n        except KeyError as e:\n            logger.error(f\"Error en StoreDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Limpieza inicial para quitar duplicados\n        try:\n            df_structure[ky] = RemoveDuplicated('cod', 'fecest', df)\n        except KeyError as e:\n            logger.error(f\"Error en RemoveDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Registrar la validaci\u00f3n en el log\n        logger.info(f'VALIDADOR TABLA: {ky}')\n    else:\n        logger.error(f\"La columna 'cod' no fue creada correctamente en el DataFrame: {ky}\")\n\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>Procesando tabla: xml4c086\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c086.csv\n\n\n2024-10-14 16:34:50,428 - INFO - VALIDADOR TABLA: xml4c086\n\n\nProcesando tabla: xml4c085\nGuardando duplicados\n\n\n2024-10-14 16:35:00,031 - INFO - VALIDADOR TABLA: xml4c085\n\n\nDuplicados guardados en: trazaDuplicados_xml4c085.csv\nProcesando tabla: xml4c087\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c087.csv\n\n\n2024-10-14 16:40:35,024 - INFO - VALIDADOR TABLA: xml4c087\n2024-10-14 16:40:35,025 - INFO - VALIDADOR DUPLICADOS --- 510.98 seconds ---\n</code></pre> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list( [x for x in df_structure.keys()]):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'cod' ] ]\n    print([ky])\n    StoreDuplicated('cod' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = RemoveDuplicated('cod', 'fecest', df_structure[ky])\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>['xml4c086']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c086.csv\n\n\n2024-10-14 16:43:13,922 - INFO - VALIDADOR TABLA: xml4c086\n2024-10-14 16:43:14,009 - INFO - VALIDADOR TABLA: xml4c085\n\n\n['xml4c085']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c085.csv\n['xml4c087']\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c087.csv\n\n\n2024-10-14 16:43:16,813 - INFO - VALIDADOR TABLA: xml4c087\n2024-10-14 16:43:16,815 - INFO - VALIDADOR DUPLICADOS --- 4.17 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#definicion-de-columnas-con-sus-tipos_1","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas de cada tabla en num\u00e9ricas, fechas y texto. Las columnas que contienen \"fec\" en su nombre se convierten al tipo fecha, y se crean los periodos de afiliaci\u00f3n y retiro en formato <code>A\u00f1oMes</code>. El proceso se registra en el log, incluyendo el tiempo total de ejecuci\u00f3n.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list( [x for x in df_structure.keys() ] ):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\n    #Se crean Los periodo para afiliaci\u00f3n y retiro (correcci\u00f3n cuando hay NAN en afi o en ret)\n    df_structure[ky]['perafi'] = (df_structure[ky]['perafi']).fillna(188001).astype(int).astype(str)\n    df_structure[ky]['perret'] = (df_structure[ky]['perret']).fillna(300001).astype(int).astype(str)\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:23:42,901 - INFO - TRANSFORMACION 2 --- 0.44 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#transformacion-de-texto-y-limpieza","title":"Transformaci\u00f3n de texto y limpieza","text":"<p>Se realiza una limpieza de las columnas de tipo texto en todas las tablas. Las columnas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El proceso utiliza <code>.loc</code> para evitar advertencias de asignaci\u00f3n, y el tiempo total de la limpieza se registra en el log.</p> <pre><code># Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in [x for x in df_structure.keys()]:\n    print(ky)\n    # Usar .loc para modificar las columnas seleccionadas y evitar SettingWithCopyWarning\n    # Pasar las columnas tipo texto a UPPER\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.upper())\n\n    # En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.strip())\n\n    # Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>xml4c086\nxml4c085\nxml4c087\n\n\nC:\\Users\\Equipo\\AppData\\Local\\Temp\\ipykernel_780\\2321073015.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky].loc[:, textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n2024-10-14 17:24:06,921 - INFO - LIMPIEZA --- 6.07 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#concatenacion-de-tablas_1","title":"Concatenaci\u00f3n de tablas","text":"<p>Se concatenan las tablas en un \u00fanico dataframe <code>df_struc_Total</code>, ignorando los \u00edndices previos.</p> <pre><code>#Concatenacion Tablas\ndf_struc_Total = pd.concat( list(df_structure.values()) , ignore_index = True )\n</code></pre> <pre><code>BaseXmlTotal = df_struc_Total\n</code></pre> <pre><code>BaseXmlTotal\n</code></pre> nit salario estado tipide id cod periodo perret perafi horas ... tipper calemp codact cedtra coddoc coddocben documento codben captra nivedu 0 900280342 1300000.0 A 1.0 7142937 17142937 202402 202402 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 819005979 535600.0 A 1.0 70091318 170091318 202401 202401 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 71376503 1300000.0 A 15.0 7425498 157425498 202406 NaN 202309 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 900668068 1000000.0 A 1.0 1082968618 11082968618 202306 202307 202001 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 900117086 1950000.0 A 1.0 85455480 185455480 202401 202402 202401 240.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 585110 NaN NaN A NaN NaN 149597057 202409 NaN 202408 NaN ... NaN NaN NaN 9910609 1 1 49597057 NaN 2.0 NaN 585111 NaN NaN A NaN NaN 21082877639 202409 NaN 202303 NaN ... NaN NaN NaN 9910731 1 2 1082877639 314997.0 2.0 01 585112 NaN NaN A NaN NaN 11082939020 202409 NaN 201912 NaN ... NaN NaN NaN 9910731 1 1 1082939020 NaN 2.0 NaN 585113 NaN NaN A NaN NaN 21083018512 202409 NaN 202308 NaN ... NaN NaN NaN 9910731 1 2 1083018512 310091.0 2.0 01 585114 NaN NaN A NaN NaN 11221984582 202409 NaN 202112 NaN ... NaN NaN NaN 997043231071997 9 1 1221984582 NaN 2.0 NaN <p>585115 rows \u00d7 29 columns</p>"},{"location":"seccion/Bloque1/#registro-de-afiliacion","title":"Registro de afiliaci\u00f3n","text":"<p>Se a\u00f1ade una nueva columna <code>Afiliado</code> en el dataframe <code>BaseXmlTotal</code>. Esta columna indica si un registro estaba afiliado en un periodo espec\u00edfico, basado en las fechas de afiliaci\u00f3n (<code>periodoAfi</code>) y retiro (<code>periodoRet</code>). Si el periodo actual (<code>Periodo</code>) se encuentra entre las fechas de afiliaci\u00f3n y retiro, el valor es <code>'si'</code>, de lo contrario, es <code>'no'</code>.</p> <pre><code>#registro de afiliaci\u00f3n\nBaseXmlTotal['Afiliado'] = np.where( \n   ( ( BaseXmlTotal['perafi'] &lt;= BaseXmlTotal['periodo'] ) &amp; ( BaseXmlTotal['periodo'] &lt;= BaseXmlTotal['perret'] ) )    \n    , 'si', 'no' )\n</code></pre>"},{"location":"seccion/Bloque1/#organizacion-de-columnas","title":"Organizaci\u00f3n de columnas","text":"<p>Se reorganizan las columnas del dataframe <code>BaseXmlTotal</code>. La columna <code>periodo</code> se mueve a la primera posici\u00f3n y la columna <code>Afiliado</code> se coloca en la tercera posici\u00f3n del dataframe.</p> <pre><code>#Organizar Columnas\nf_column = BaseXmlTotal.pop('periodo')\nBaseXmlTotal.insert(0,'periodo', f_column )\nf_column = BaseXmlTotal.pop('Afiliado')\nBaseXmlTotal.insert(2,'Afiliado', f_column )\n</code></pre>"},{"location":"seccion/Bloque1/#agregacion-de-datos","title":"Agregaci\u00f3n de datos","text":"<p>Se agrupan los datos de <code>BaseXmlTotal</code> por las columnas <code>cod</code> y <code>periodo</code>, donde el periodo se agrupa en bloques trimestrales. Las columnas clave se agregan utilizando funciones como <code>last</code>, <code>first</code>, <code>sum</code> y <code>count</code>, dependiendo de la naturaleza de los datos. Despu\u00e9s de la agregaci\u00f3n, el dataframe resultante se asigna nuevamente a <code>BaseXmlTotal</code> y se registra el tiempo total de la operaci\u00f3n en el log.</p> <pre><code># Iniciar la agregaci\u00f3n\n# tagreg_start_time = time.time()\n# logger.info('INICIO DE AGREGACI\u00d3N DE DATOS')\n\n# # Agrupar BaseXmlTotal\n# # Se agrupa por 'periodo' y 'cod' para reducir la cantidad de registros finales\n# # Se agregan columnas clave seg\u00fan las instrucciones proporcionadas\n# BaseXmlTotal['periodo'] = BaseXmlTotal['periodo'].astype(str).str[:6]  # Extraer el formato yyyymm\n# #Comentar la siguiente linea en caso de necesitar continuar con la agregacion original y solo estructurar las columnas\n# BaseXmlTotal['periodo'] = (BaseXmlTotal['periodo'].astype(int) // 3 * 3).astype(str)  # Agrupar cada 3 meses\n# BaseXmlTotal_agg = BaseXmlTotal.groupby(['cod','periodo'], as_index=False).agg({\n#     'periodo':'last',\n#     'Afiliado': 'last',\n#     'perafi': 'first',\n#     'fecsis': 'last',\n#     'fecest': 'last',\n#     'calemp': 'last',\n#     'tipper': 'last',\n#     'codact': 'last',\n#     'codest': 'last',\n#     'estado': 'last',\n#     'tipide': 'last',\n#     'id': 'last',\n#     'documento': 'last',\n#     'perret': 'last',\n#     'codben': 'count',\n#     'codcat': 'last',\n#     'codlis': 'last',\n#     'codsuc': 'last',\n#     'captra': 'last',\n#     'nivedu': 'last',\n#     'coddoc': 'last',\n#     'cedtra': 'last',\n#     'nit': 'last',\n#     'horas': 'sum',\n#     'salario': 'last'\n# })\n\n# # Opcional: Mostrar las primeras filas para verificar\n# # print(BaseXmlTotal_agg.head())\n\n\n# # Asignar el DataFrame agrupado a BaseXmlTotal para la siguiente etapa\n# BaseXmlTotal = BaseXmlTotal_agg\n\n\n# # Finalizar la agregaci\u00f3n\n# logger.info(f'AGREGACI\u00d3N FINALIZADA --- {time.time() - tagreg_start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:25:28,867 - INFO - INICIO DE AGREGACI\u00d3N DE DATOS\n2024-10-14 17:25:31,591 - INFO - AGREGACI\u00d3N FINALIZADA --- 2.72 seconds ---\n</code></pre> <pre><code># Renombrar las columnas\nBaseXmlTotal = BaseXmlTotal.rename(columns={\n     'cod':'ID_REGISTRO',\n     'periodo': 'PERIODO',\n     'Afiliado':'ID_AFILIADO',\n     'calemp':'COD_CALIDAD_SUCURSAL',\n     'tipper':'TIPPER',\n     'codact':'COD_ACT',\n     'codest':'COD_EST_INAC',\n     'codcat':'CATEGORIA',\n     'estado':'COD_EST_AFIL',\n     'codsuc':'ID_SUCURSAL', #Ajustar \n     'perafi':'PERIODO_AFILIACION',\n     'perret':'PERIODO_RETIRO',\n     'fecsis':'FECHA_SISTEMA',\n     'fecest':'FECHA_ESTADO',\n     'codben':'COD_BENEFICIARIO',\n     'tipide':'TIPO_DOCUMENTO',\n     'id':'',\n     'coddocben':'',\n     'documento':'',\n\n\n\n#     'codlis':'',\n#     'captra':'',\n#     'nivedu':'',\n#     'coddoc':'',\n#     'cedtra':'',\n#     'nit':'',\n#     'horas':'',\n#     'salario':'' \n# })\n# BaseXmlTotal.columns.tolist()\n</code></pre>"},{"location":"seccion/Bloque1/#guardar-en-base-de-datos-dwh_3","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>BaseXmlTotal</code> en la tabla <code>BD_FactAfiliacion</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(BaseXmlTotal, 'BD_Fact_Afiliacion', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-14 17:34:15,493 - INFO - CONEXION A BASE DWH\n2024-10-14 17:34:15,768 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactAfiliacion\n2024-10-14 17:35:17,055 - INFO - Tabla almacenada correctamente.\n2024-10-14 17:35:17,723 - INFO - ALMACENAMIENTO --- 62.23 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/#registro-de-finalizacion-del-etl","title":"Registro de finalizaci\u00f3n del ETL","text":"<p>Se registra el tiempo total de ejecuci\u00f3n del proceso ETL desde su inicio hasta su finalizaci\u00f3n en el log, indicando que el proceso ha concluido con \u00e9xito.</p> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-14 17:36:48,552 - INFO - FINAL ETL --- 6204.20 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/","title":"2.1-FactAportes","text":""},{"location":"seccion/Bloque2/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, y <code>dateutil</code>, junto con funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, lo que asegura la correcta importaci\u00f3n de las funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport time\nimport os\nimport logging\nstart_time = time.time()\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 12-10-2024 23:19\n</code></pre>"},{"location":"seccion/Bloque2/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>FactAportes.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactAportes.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-12 23:19:33,037 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando <code>create_engine</code> y la cadena de conexi\u00f3n generada por <code>Conexion_dwh()</code>. Se registra en el log la confirmaci\u00f3n de la conexi\u00f3n.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n</code></pre> <pre><code>2024-10-12 23:19:33,547 - INFO - CONEXION A BASE MINERVA\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL para la tabla <code>subsi11</code> en el diccionario <code>qr_structure</code>, que extrae todas las columnas de la tabla y genera una nueva columna <code>id_no_codoc</code> basada en el campo <code>nit</code>. El proceso de lectura de las consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################SUBSI15###############################################\n    \"subsi11\":'''select *, nit as id_no_codoc from subsidio.subsi11'''    \n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-12 23:19:33,555 - INFO - LECTURA DE QUERYS\n</code></pre> <pre><code># Cargue de tablas desde sql\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-12 23:20:49,036 - INFO - Carga subsi11 --- 74.97 seconds ---\n2024-10-12 23:20:49,427 - INFO - CARGUE TABLAS DESDE MYSQL --- 75.86 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-columnas-con-sus-tipos","title":"Definici\u00f3n de columnas con sus tipos","text":"<p>Se clasifican las columnas del diccionario <code>df_structure</code> en tres categor\u00edas: columnas num\u00e9ricas, columnas de fechas (aquellas que comienzan con \"fec\") y columnas de texto. Las columnas de fechas se convierten al formato de fecha usando <code>pd.to_datetime()</code> con manejo de errores. Las listas de columnas se almacenan en tres diccionarios: <code>numericColumns</code>, <code>datesColumns</code> y <code>textColumns</code>. El tiempo total del proceso se registra en el log.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 23:20:50,697 - INFO - TRANSFORMACION 2 --- 1.26 seconds ---\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/Bloque2/#22-factentrega","title":"2.2-FactEntrega","text":""},{"location":"seccion/Bloque2/#importacion-de-librerias-y-funciones_1","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport os\nimport logging\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 12:10\n</code></pre>"},{"location":"seccion/Bloque2/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Fact_entrega.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code># Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='Fact_entrega.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\nstart_time = time.time()\n</code></pre> <pre><code>2024-10-13 12:10:32,172 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-consultas-sql_1","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL en el diccionario <code>qr_structure</code> que extrae datos de las tablas <code>subsi09</code> y <code>subsi146</code> a trav\u00e9s de un <code>INNER JOIN</code>. La consulta selecciona varias columnas clave relacionadas con fechas, valores y documentos. El proceso de lectura de las consultas se registra en el log.</p> <pre><code>qr_structure = {\n    \"fact_entrega\": f\"SELECT s09.id AS id, s09.fecanu AS f_anu, s09.fecasi AS f_asig_cuota, s09.fecent AS f_disp_benef, s09.feccon AS f_consign, s09.fecche AS f_cheque, s09.fecfos AS f_fosfec, s09.valcre AS val_credito, s09.valaju AS val_ajuste, s09.documento AS doc_contable, s146.fecenv AS f_envio, s146.feccon AS f_conciliac, s146.fecrec AS f_rechazo, s146.fecpre AS f_prescrip, s146.fecifz AS f_interfaz, s146.fecpto AS f_reemplazo, s146.valor AS val_cuota, s146.mempag AS num_memorando, s146.doccon AS doc_conciliac, s146.docpre AS doc_prescrip FROM subsidio.subsi09 s09 INNER JOIN subsidio.subsi146 s146 ON s09.id = s146.id09\"\n}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:10:32,186 - INFO - LECTURA DE QUERYS\n</code></pre> <pre><code>qr_structure\n</code></pre> <pre><code>{'fact_entrega': 'SELECT s09.id AS id, s09.fecanu AS f_anu, s09.fecasi AS f_asig_cuota, s09.fecent AS f_disp_benef, s09.feccon AS f_consign, s09.fecche AS f_cheque, s09.fecfos AS f_fosfec, s09.valcre AS val_credito, s09.valaju AS val_ajuste, s09.documento AS doc_contable, s146.fecenv AS f_envio, s146.feccon AS f_conciliac, s146.fecrec AS f_rechazo, s146.fecpre AS f_prescrip, s146.fecifz AS f_interfaz, s146.fecpto AS f_reemplazo, s146.valor AS val_cuota, s146.mempag AS num_memorando, s146.doccon AS doc_conciliac, s146.docpre AS doc_prescrip FROM subsidio.subsi09 s09 INNER JOIN subsidio.subsi146 s146 ON s09.id = s146.id09'}\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:10:32,843 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:10:33,357 - INFO - Cargando fact_entrega \n2024-10-13 12:10:34,590 - INFO - Cargada fact_entrega --- 1.23 seconds ---\n2024-10-13 12:10:34,679 - INFO - CARGUE TABLAS DESDE MYSQL --- 1.83 seconds ---\n</code></pre> <pre><code>df_structure['fact_entrega']\n</code></pre> id f_anu f_asig_cuota f_disp_benef f_consign f_cheque f_fosfec val_credito val_ajuste doc_contable f_envio f_conciliac f_rechazo f_prescrip f_interfaz f_reemplazo val_cuota num_memorando doc_conciliac doc_prescrip 0 20985911 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 1 20985912 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 2 20985913 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 40243 1.0 None None 3 20929805 None 2022-02-21 2022-02-23 None None None 0 0 None 2022-04-08 2022-04-12 2022-02-23 None None None 34994 1.0 None None 4 20979008 None 2022-02-20 2022-02-23 None None None 0 0 None 2022-05-26 2022-06-01 2022-02-23 None None None 34994 2.0 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 39438 25809207 None 2024-09-25 2024-09-27 None None None 0 0 None None None 2024-09-27 None None None 44491 NaN None None 39439 25778144 None 2024-09-18 2024-09-24 None None None 0 0 None None None 2024-09-24 None None None 44491 NaN None None 39440 25691627 None 2024-09-19 2024-09-24 None None None 0 0 None 2024-10-11 None 2024-09-24 None None None 44491 102.0 None None 39441 25691628 None 2024-09-19 2024-09-24 None None None 0 0 None 2024-10-11 None 2024-09-24 None None None 44491 102.0 None None 39442 25802299 None 2024-09-25 2024-09-27 None None None 0 0 None None None 2024-09-27 None None None 51165 NaN None None <p>39443 rows \u00d7 20 columns</p>"},{"location":"seccion/Bloque2/#proceso-de-limpieza-y-validacion-de-duplicados","title":"Proceso de limpieza y validaci\u00f3n de duplicados","text":"<p>Se define un proceso para comparar y limpiar los datos de cada tabla en <code>df_structure</code>. Se comparan las columnas listadas en <code>ColumnsToCompare</code> para identificar duplicados y eliminarlos. Luego, se aplica una limpieza adicional para detectar y corregir errores en los datos utilizando las funciones <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, y <code>RemoveErrors</code>. Los duplicados y errores se registran en archivos espec\u00edficos para cada tabla y se registra el proceso en el log.</p> <pre><code># Columnas que deseas comparar y limpiar\ncolumId = 'id'  # Ajusta seg\u00fan tu identificador \u00fanico\nColumnsToCompare = ['f_anu', 'f_asig_cuota', 'f_disp_benef', 'f_consign', 'f_cheque', 'f_fosfec', \n                    'val_credito', 'val_ajuste', 'doc_contable', 'f_envio', 'f_conciliac', \n                    'f_rechazo', 'f_prescrip', 'f_interfaz', 'f_reemplazo', 'val_cuota', \n                    'num_memorando', 'doc_conciliac', 'doc_prescrip']\n\n# Iterar sobre los DataFrames en df_structure\nfor key, df in df_structure.items():\n    print(f\"Procesando tabla: {key}\")\n\n    # Guardar duplicados\n    StoreDuplicated(columId, ColumnsToCompare,df, f'duplicados_fact_entrega_{key}')\n    logger.info(f'Duplicados guardados: {key}')\n\n    # Eliminar duplicados por fecha\n    df_cleaned = RemoveDuplicated(columId, 'f_disp_benef', df)\n    logger.info(f'Duplicados por fecha eliminados: {key}')\n\n    # Limpiar errores\n    df_final = RemoveErrors(df_cleaned, f'errores_fact_entrega_{key}')\n\n    # Registrar informaci\u00f3n de la limpieza\n    logger.info(f'Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: {key}')\n</code></pre> <pre><code>Procesando tabla: fact_entrega\nGuardando duplicados\n\n\n2024-10-13 12:10:34,996 - INFO - Duplicados guardados: fact_entrega\n2024-10-13 12:10:35,044 - INFO - Duplicados por fecha eliminados: fact_entrega\n2024-10-13 12:10:35,046 - INFO - Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: fact_entrega\n\n\nDuplicados guardados en: duplicados_fact_entrega_fact_entrega.csv\n</code></pre> <pre><code>df_final\n</code></pre> id f_anu f_asig_cuota f_disp_benef f_consign f_cheque f_fosfec val_credito val_ajuste doc_contable f_envio f_conciliac f_rechazo f_prescrip f_interfaz f_reemplazo val_cuota num_memorando doc_conciliac doc_prescrip 33616 7011580 None 2011-04-19 2011-04-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-04-25 None None None 20346 78.0 None None 33615 7005252 None 2011-04-19 2011-04-25 None None None 0 0 None None None 2011-04-25 None 2024-02-29 2024-02-29 20346 NaN None 0011070 33587 6974521 None 2011-04-19 2011-04-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-04-25 None None None 23397 78.0 None None 33586 7236763 None 2011-07-18 2011-07-25 None None None 0 0 None 2024-02-29 2024-02-29 2011-07-25 None None None 23397 78.0 None None 33585 7203942 None 2011-07-18 2011-07-25 None None None 0 0 None 2024-02-14 2024-02-15 2011-07-25 None None None 23397 77.0 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 28003 17431466 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28004 17431470 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28005 17431471 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28006 17431490 None 0000-00-00 NaT None None None 0 0 None None None 2023-07-29 None 2023-08-31 2023-08-31 31450 NaN None 0010481 28007 17431507 None 0000-00-00 NaT None None None 0 0 None 2023-08-25 2023-08-25 2023-07-29 None None None 31450 54.0 None None <p>39443 rows \u00d7 20 columns</p>"},{"location":"seccion/Bloque2/#definicion-y-transformacion-de-columnas","title":"Definici\u00f3n y transformaci\u00f3n de columnas","text":"<p>Se identifican las columnas num\u00e9ricas, de texto y de fecha en cada tabla dentro de <code>df_structure</code>. Las columnas de fecha, cuyo nombre comienza con \"fec\", se convierten a formato <code>datetime</code>. Adem\u00e1s, se crean las columnas <code>periodoAfi</code> y <code>periodoRet</code> basadas en las columnas de fechas de afiliaci\u00f3n (<code>fecafi</code>) y retiro (<code>fecret</code>), formateando los periodos como <code>A\u00f1oMes</code>. Si alguna de estas columnas no se encuentra, se registra una advertencia en el log. El tiempo total del proceso de transformaci\u00f3n se registra en el log.</p> <pre><code># Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    # Obtener columnas num\u00e9ricas\n    numericColumns[ky] = df_structure[ky].select_dtypes(include=numerics).columns.tolist()\n\n    # Obtener columnas de fecha\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec')]\n\n    # Obtener columnas de texto\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in (numericColumns[ky] + datesColumns[ky])] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('cod')] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('ced')] + \\\n                       [x for x in df_structure[ky].columns.tolist() if x.startswith('num')]\n\n    textColumns[ky] = list(set(textColumns[ky]))\n\n    # Convertir columnas de fecha a datetime\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n\n    # Crear nuevas columnas 'periodoAfi' y 'periodoRet'\n    if 'fecafi' in df_structure[ky].columns and 'fecret' in df_structure[ky].columns:\n        df_structure[ky]['periodoAfi'] = (df_structure[ky]['fecafi'].dt.year * 100 + df_structure[ky]['fecafi'].dt.month).fillna(188001).astype(int).astype(str)\n        df_structure[ky]['periodoRet'] = (df_structure[ky]['fecret'].dt.year * 100 + df_structure[ky]['fecret'].dt.month).fillna(300001).astype(int).astype(str)\n    else:\n        logger.warning(f'Columnas \"fecafi\" y/o \"fecret\" no encontradas en la tabla para {ky}.')\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:35,141 - WARNING - Columnas \"fecafi\" y/o \"fecret\" no encontradas en la tabla para fact_entrega.\n2024-10-13 12:10:35,143 - INFO - TRANSFORMACION 2 --- 0.02 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#tratamiento-de-errores-afiliacion-mayor-que-retiro","title":"Tratamiento de errores: afiliaci\u00f3n mayor que retiro","text":"<p>Se filtran los registros en cada tabla de <code>df_structure</code> donde el periodo de afiliaci\u00f3n (<code>periodoAfi</code>) es menor o igual al periodo de retiro (<code>periodoRet</code>). Si las columnas <code>periodoAfi</code> o <code>periodoRet</code> no est\u00e1n presentes en alguna tabla, se genera una advertencia en el log. El tiempo total de este proceso de correcci\u00f3n de errores se registra en el log.</p> <pre><code># Tratamiento de errores afi &gt; ret\ntrataerro_time = time.time()\nfor ky in list(df_structure.keys()):\n    # Filtrar filas donde periodoAfi es menor o igual a periodoRet\n    if 'periodoAfi' in df_structure[ky].columns and 'periodoRet' in df_structure[ky].columns:\n        df_structure[ky] = df_structure[ky][df_structure[ky]['periodoAfi'] &lt;= df_structure[ky]['periodoRet']]\n    else:\n        logger.warning(f'Columnas \"periodoAfi\" y/o \"periodoRet\" no encontradas en la tabla para {ky}.')\n\nlogger.info(f'TRATAMIENTO ERRORES --- {time.time() - trataerro_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:35,156 - WARNING - Columnas \"periodoAfi\" y/o \"periodoRet\" no encontradas en la tabla para fact_entrega.\n2024-10-13 12:10:35,161 - INFO - TRATAMIENTO ERRORES --- 0.00 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#limpieza-de-texto-y-eliminacion-de-nan","title":"Limpieza de texto y eliminaci\u00f3n de NaN","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las cadenas de texto se convierten a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code>. El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code># Limpieza de texto y eliminaci\u00f3n de NaN\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.upper())\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply(lambda x: x.astype(str).str.strip())\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_6048\\3248070240.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace(['NAN', 'NONE'], np.nan)\n2024-10-13 12:10:35,812 - INFO - LIMPIEZA --- 0.63 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#creacion-de-tabla-de-calendario-y-cross-join","title":"Creaci\u00f3n de tabla de calendario y <code>Cross Join</code>","text":"<p>Se genera un dataframe de calendario que contiene periodos mensuales de los \u00faltimos 18 meses. Luego, se realiza un <code>cross join</code> entre el dataframe <code>df_struc_Total</code> (que contiene los datos de la consulta <code>fact_entrega</code>) y el calendario utilizando una clave auxiliar <code>kp</code>. Este proceso permite combinar todos los periodos con los registros existentes. El tiempo total de la operaci\u00f3n se registra en el log.</p> <pre><code># Concatenaci\u00f3n de tablas (si hubiese m\u00e1s de una tabla, en este caso se omitir\u00e1 ya que solo tenemos subsi11)\n# df_struc_Total = pd.concat(list(df_structure.values()), ignore_index=True)\n\n# Creaci\u00f3n tabla calendario y cross join\ncross_time = time.time()\ndfCalendar = pd.DataFrame(pd.date_range(date.today() - relativedelta(months=18), date.today(), freq='MS').tolist(), columns=['Fechas'])\ndfCalendar['Periodo'] = (dfCalendar['Fechas'].dt.year * 100 + dfCalendar['Fechas'].dt.month).astype(str)\ndfCalendar = dfCalendar[['Periodo']]\n\n# Cross join\ndf_struc_Total = df_structure['fact_entrega']\ndf_struc_Total['kp'] = 0\ndfCalendar['kp'] = 0\nFactEntregaTotal = df_struc_Total.merge(dfCalendar, on='kp', how='outer')\n\nlogger.info(f'TRANSFORMACION 3 CROSS JOIN --- {time.time() - cross_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:10:36,184 - INFO - TRANSFORMACION 3 CROSS JOIN --- 0.36 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#creacion-de-la-columna-entrega-y-organizacion-de-columnas","title":"Creaci\u00f3n de la columna <code>Entrega</code> y organizaci\u00f3n de columnas","text":"<p>Se verifica si la columna <code>periodoRet</code> est\u00e1 presente en el dataframe <code>FactEntregaTotal</code>. Si existe, se crea la columna <code>Entrega</code> indicando <code>'si'</code> si el periodo actual est\u00e1 dentro del periodo de retiro, y <code>'no'</code> en caso contrario. Si no est\u00e1 presente la columna <code>periodoRet</code>, se asigna <code>'no'</code> por defecto. Luego, se eliminan las columnas innecesarias (como <code>kp</code>) y se reorganizan las columnas colocando <code>Periodo</code> en la primera posici\u00f3n y <code>Entrega</code> en la tercera.</p> <pre><code># Verificar que 'periodoRet' existe en el DataFrame\nif 'periodoRet' in FactEntregaTotal.columns:\n    # Creaci\u00f3n de la columna 'Entrega'\n    FactEntregaTotal['Entrega'] = np.where(\n        (FactEntregaTotal['Periodo'] &lt;= FactEntregaTotal['periodoRet']),\n        'si', 'no'\n    )\nelse:\n    logger.warning(\"'periodoRet' no encontrado en FactEntregaTotal.\")\n    # Si 'periodoRet' no est\u00e1 disponible, podr\u00edas definir una l\u00f3gica alternativa o asignar 'no' por defecto\n    FactEntregaTotal['Entrega'] = 'no'\n\n# Eliminar columnas innecesarias\nFactEntregaTotal = FactEntregaTotal.drop(['kp'], axis=1)\n\n# Organizar Columnas\nf_column = FactEntregaTotal.pop('Periodo')\nFactEntregaTotal.insert(0, 'Periodo', f_column)\nf_column = FactEntregaTotal.pop('Entrega')\nFactEntregaTotal.insert(2, 'Entrega', f_column)\n</code></pre> <pre><code>2024-10-13 12:10:36,196 - WARNING - 'periodoRet' no encontrado en FactEntregaTotal.\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-a-la-base-de-datos-dwh_1","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 12:10:36,343 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/Bloque2/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>FactEntregaTotal</code> en la tabla <code>BD_FactEntrega</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>guardar_en_dwh(FactEntregaTotal, 'BD_FactEntrega', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:10:36,354 - INFO - CONEXION A BASE DWH\n2024-10-13 12:10:36,852 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactEntrega\n2024-10-13 12:11:51,844 - INFO - Tabla almacenada correctamente.\n2024-10-13 12:11:52,636 - INFO - ALMACENAMIENTO --- 76.28 seconds ---\n</code></pre> <pre><code>FactEntregaTotal.columns.tolist()\n</code></pre> <pre><code>['Periodo',\n 'id',\n 'Entrega',\n 'f_anu',\n 'f_asig_cuota',\n 'f_disp_benef',\n 'f_consign',\n 'f_cheque',\n 'f_fosfec',\n 'val_credito',\n 'val_ajuste',\n 'doc_contable',\n 'f_envio',\n 'f_conciliac',\n 'f_rechazo',\n 'f_prescrip',\n 'f_interfaz',\n 'f_reemplazo',\n 'val_cuota',\n 'num_memorando',\n 'doc_conciliac',\n 'doc_prescrip']\n</code></pre> <pre><code># Iniciar la agregaci\u00f3n\ntagreg_start_time = time.time()\nlogger.info('INICIO DE AGREGACI\u00d3N DE DATOS')\n\n# Agrupar FactEntregaTotal\n# Se agrupa por 'Periodo' y 'id' para reducir la cantidad de registros finales\n# Se agregan columnas clave seg\u00fan las instrucciones proporcionadas\nFactEntregaTotal['Periodo'] = FactEntregaTotal['Periodo'].astype(str).str[:6]  # Extraer el formato yyyymm\n#Comentar la siguiente linea en caso de necesitar continuar con la agregacion original y solo estructurar las columnas\n#FactEntregaTotal['Periodo'] = (FactEntregaTotal['Periodo'].astype(int) // 3 * 3).astype(str)  # Agrupar cada 3 meses\nFactEntregaAgg = FactEntregaTotal.groupby(['id','Periodo'], as_index=False).agg({\n    'Periodo':'last',\n    'id':'last',\n    'Entrega':'count',\n    'f_anu':'last',\n    'f_asig_cuota':'last',\n    'f_disp_benef':'last',\n    'f_consign':'count',\n    'f_cheque':'count',\n    'f_fosfec':'last',\n    'val_credito':'sum',\n    'val_ajuste':'sum',\n    'doc_contable':'last',\n    'f_envio':'last',\n    'f_conciliac':'last',\n    'f_rechazo':'last',\n    'f_prescrip':'last',\n    'f_interfaz':'last',\n    'f_reemplazo':'last',\n    'val_cuota':'last',\n    'num_memorando':'count',\n    'doc_conciliac':'last',\n    'doc_prescrip':'last'\n})\n\n# Asignar el DataFrame agrupado a FactEntregaTotal para la siguiente etapa\n#FactEntregaTotal = FactEntregaAgg\n\n\n# Finalizar la agregaci\u00f3n\nlogger.info(f'AGREGACI\u00d3N FINALIZADA --- {time.time() - tagreg_start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:11:52,655 - INFO - INICIO DE AGREGACI\u00d3N DE DATOS\n2024-10-13 12:11:53,719 - INFO - AGREGACI\u00d3N FINALIZADA --- 1.06 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#23-factsubsidiovivienda","title":"2.3-FactSubsidioVivienda","text":""},{"location":"seccion/Bloque2/#importacion-de-librerias-y-funciones_2","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 12:13\n</code></pre>"},{"location":"seccion/Bloque2/#configuracion-inicial-del-logger_1","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>SubsidioVivienda.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code># Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='SubsidioVivienda.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 12:13:18,582 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-consultas-sql_2","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define un conjunto de consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de diferentes tablas de la base de datos <code>vivienda</code>. Cada consulta selecciona columnas clave relacionadas con postulaciones, beneficiarios, informaci\u00f3n financiera, y resultados de solicitud de subsidio. El diccionario <code>dim_names</code> se utiliza para mapear el nombre de cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"subvi02\":'''SELECT \n                CONCAT(coddoc, cedpos) as id,\n                documento,numrad,cedpos,coddoc,direccion,\n                codciu,telefono,celular,codzon,email,fecnac,sexo,estciv,nit,codsuc,telemp,\n                email_empre,codact,salario,porapo,tippob,tippos,discap,cabhog,numasi,tipmad,\n                prioridad,estado,fecest,fecdig,usuario,periodo,fecmod,usumod,modsol,fecact,nota\n                FROM vivienda.subvi02''',\n    \"subvi03\":'''SELECT \n                documento,modsol,tippro,coddep,codciu,tipviv,valviv,\n                valaho,valces,fecter,totapo,fecini,totsub,totfin\n                FROM vivienda.subvi03''',\n    \"subvi04\":'''SELECT\n                numpos,documento,priape,segape,prinom,segnom,fecnac,numdoc,\n                coddoc,sexo,parent,reemplazo,estciv,discap,codocu,ingres\n                FROM vivienda.subvi04''',\n    \"subvi05\":'''select \n                documento,credito,cueaho,codban,ciuban,\n                fecaho,fonces,fecces,fecini,ciufon\n                FROM vivienda.subvi05''',\n    \"subvi06\":'''select \n                documento,periodo,numasi,fecasi,valaju,valsub,nota,\n                estado,fecven,fecpag,fecleg,ruaf,ruasub,feccom\n                FROM vivienda.subvi06'''\n               }\ndim_names = {\n    \"subvi02\":'BD_Datos_Postulacion',\n    \"subvi03\":'BD_Detalle_Postulacion',\n    \"subvi04\":'BD_Beneficiario_Postulacion',\n    \"subvi05\":'BD_Financiero_Postulacion',\n    \"subvi06\":'BD_Resultado_Solicitud_Subsidio'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:13:18,605 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-y-carga-de-tablas-desde-sql_1","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:13:18,649 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:13:21,312 - INFO - Cargando subvi02 \n2024-10-13 12:13:33,944 - INFO - Cargada subvi02 --- 12.63 seconds ---\n2024-10-13 12:13:33,945 - INFO - Cargando subvi03 \n2024-10-13 12:13:35,751 - INFO - Cargada subvi03 --- 1.81 seconds ---\n2024-10-13 12:13:35,753 - INFO - Cargando subvi04 \n2024-10-13 12:13:36,504 - INFO - Cargada subvi04 --- 0.75 seconds ---\n2024-10-13 12:13:36,507 - INFO - Cargando subvi05 \n2024-10-13 12:13:36,747 - INFO - Cargada subvi05 --- 0.24 seconds ---\n2024-10-13 12:13:36,749 - INFO - Cargando subvi06 \n2024-10-13 12:13:37,016 - INFO - Cargada subvi06 --- 0.27 seconds ---\n2024-10-13 12:13:37,100 - INFO - CARGUE TABLAS DESDE MYSQL --- 18.45 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la presencia de registros duplicados en todas las tablas del diccionario <code>df_structure</code>. Las columnas utilizadas para comparar duplicados excluyen la columna <code>id</code>. Los duplicados detectados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:37,261 - INFO - VALIDADOR TABLA: subvi02\n2024-10-13 12:13:37,307 - INFO - VALIDADOR TABLA: subvi03\n2024-10-13 12:13:37,403 - INFO - VALIDADOR TABLA: subvi04\n2024-10-13 12:13:37,443 - INFO - VALIDADOR TABLA: subvi05\n2024-10-13 12:13:37,486 - INFO - VALIDADOR TABLA: subvi06\n2024-10-13 12:13:37,487 - INFO - VALIDADOR DUPLICADOS --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla dentro de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total de la operaci\u00f3n se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:38,477 - INFO - LIMPIEZA --- 0.98 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-a-la-base-de-datos-dwh_2","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 12:13:38,489 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/Bloque2/#guardar-en-base-de-datos-dwh_1","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. Si las tablas ya existen en la base de datos, se reemplazan con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:13:38,503 - INFO - CONEXION A BASE DWH\n2024-10-13 12:13:39,027 - INFO - Almacenando tabla subvi02 en DWH como BD_Datos_Postulacion\n2024-10-13 12:13:43,943 - INFO - Tabla subvi02 almacenada correctamente.\n2024-10-13 12:13:43,944 - INFO - Almacenando tabla subvi03 en DWH como BD_Detalle_Postulacion\n2024-10-13 12:13:46,613 - INFO - Tabla subvi03 almacenada correctamente.\n2024-10-13 12:13:46,615 - INFO - Almacenando tabla subvi04 en DWH como BD_Beneficiario_Postulacion\n2024-10-13 12:13:50,163 - INFO - Tabla subvi04 almacenada correctamente.\n2024-10-13 12:13:50,165 - INFO - Almacenando tabla subvi05 en DWH como BD_Financiero_Postulacion\n2024-10-13 12:13:52,874 - INFO - Tabla subvi05 almacenada correctamente.\n2024-10-13 12:13:52,876 - INFO - Almacenando tabla subvi06 en DWH como BD_Resultado_Solicitud_Subsidio\n2024-10-13 12:13:54,956 - INFO - Tabla subvi06 almacenada correctamente.\n2024-10-13 12:13:55,062 - INFO - ALMACENAMIENTO --- 16.56 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 12:13:55,073 - INFO - FINAL ETL --- 36.52 seconds ---\n</code></pre> <pre><code># \ntabla_consulta = 'subvi06'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['documento', 'periodo']).agg({\n    'numasi': 'first',  # Mantener el primer valor de asignaci\u00f3n\n    'fecasi': 'first',  # Fecha de asignaci\u00f3n m\u00e1s temprana\n    'valaju': 'sum',  # Sumar valores ajustados si es que cambian en el mismo periodo\n    'valsub': 'sum',  # Sumar valores del subsidio\n    'nota': 'last',  # Mantener la \u00faltima nota registrada\n    'estado': 'last',  # \u00daltimo estado registrado\n    'fecven': 'last',  # \u00daltima fecha de vencimiento registrada\n    'fecpag': 'last',  # \u00daltima fecha de pago registrada\n    'fecleg': 'last',  # \u00daltima fecha de legalizaci\u00f3n\n    'ruaf': 'last',  # \u00daltimo valor de ruaf\n    'ruasub': 'count',  \n    'feccom': 'last'  # \u00daltima fecha de comentario registrada\n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>BD_Resultado_Solicitud_Subsidio\nNumero de registros 4174\n\n\n\n\n\n(4174, 4174)\n</code></pre> <pre><code># \ntabla_consulta = 'subvi05'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Financiero_Postulacion\nNumero de registros 10046\n\n\n\n\n\n['documento',\n 'credito',\n 'cueaho',\n 'codban',\n 'ciuban',\n 'fecaho',\n 'fonces',\n 'fecces',\n 'fecini',\n 'ciufon']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi04'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Beneficiario_Postulacion\nNumero de registros 22172\n\n\n\n\n\n['numpos',\n 'documento',\n 'priape',\n 'segape',\n 'prinom',\n 'segnom',\n 'fecnac',\n 'numdoc',\n 'coddoc',\n 'sexo',\n 'parent',\n 'reemplazo',\n 'estciv',\n 'discap',\n 'codocu',\n 'ingres']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi03'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\nprint(f'No existen campos para agrupar, numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n</code></pre> <pre><code>BD_Detalle_Postulacion\nNo existen campos para agrupar, numero de registros 12793\n\n\n\n\n\n['documento',\n 'modsol',\n 'tippro',\n 'coddep',\n 'codciu',\n 'tipviv',\n 'valviv',\n 'valaho',\n 'valces',\n 'fecter',\n 'totapo',\n 'fecini',\n 'totsub',\n 'totfin']\n</code></pre> <pre><code># \ntabla_consulta = 'subvi02'\nprint(dim_names[tabla_consulta])\ntabla = df_structure[tabla_consulta]\ntabla.columns.tolist()\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['documento', 'periodo']).agg({\n    'salario': 'last',  # Promedio de salario si es que cambia\n    'prioridad': 'count',  # Promedio de prioridad si tiene m\u00faltiples registros\n    'estado': 'last',  # Conservamos el \u00faltimo estado registrado\n    'fecmod': 'last',  # \u00daltima fecha de modificaci\u00f3n\n    'codciu': 'last',  # Primera ciudad asociada\n    'telefono': 'last',  # El primer n\u00famero de tel\u00e9fono registrado\n    'email': 'last',  # El primer email registrado\n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>BD_Datos_Postulacion\n\n\n\n\n\n(13131, 13131)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/Bloque2/#24-crearfactfosfec","title":"2.4-CrearFactFosfec","text":""},{"location":"seccion/Bloque2/#importacion-de-bibliotecas-y-configuracion-inicial","title":"Importaci\u00f3n de bibliotecas y configuraci\u00f3n inicial","text":"<p>Se importan las bibliotecas necesarias, como <code>sqlalchemy</code>, <code>pandas</code>, <code>pymysql</code>, y <code>dateutil</code>. Adem\u00e1s, se importa un m\u00f3dulo de funciones personalizadas desde un archivo externo <code>Funciones.py</code>, incluyendo funciones como <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, <code>RemoveErrors</code>, <code>Conexion_Minerva</code>, y <code>Conexion_dwh</code>. Tambi\u00e9n se configura el logger para registrar eventos en un archivo <code>FactFosFec.log</code>, y se inicia el proceso ETL, registrando el evento de inicio en el log.</p> <pre><code># Importar bibliotecas necesarias\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport logging\n\n\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, cargar_tablas, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n\n# Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='FactFosFec.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 12:15:50,105 - INFO - COMIENZO ETL\n\n\nImportacion de funciones correcta, 13-10-2024 12:15\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-consultas-sql_3","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL en el diccionario <code>qr_structure</code> que extrae datos de las tablas <code>fosfec160</code>, <code>fosfec07</code>, <code>fosfec09</code>, y <code>fosfec21</code> de la base de datos <code>fosfec</code>, utilizando varias combinaciones de <code>INNER JOIN</code> y <code>LEFT JOIN</code>. La consulta selecciona columnas clave relacionadas con fechas, documentos y salarios. El proceso de lectura de consultas se registra en el log.</p> <pre><code># Consultas SQL\n\nqr_structure = {\n    \"fact_foscec\": '''\n    SELECT \n        fosfec160.id,\n        fosfec160.cedtra,\n        fosfec160.tipdoc,\n        fosfec160.fecexp,\n        fosfec160.fecsis,\n        fosfec07.fecasi,\n        fosfec07.fecest,\n        fosfec09.fecnac,\n        fosfec160.fecfin,\n        fosfec09.fecdig,\n        fosfec09.fecsal,\n        fosfec160.fecenv,\n        fosfec160.fecterm,\n        fosfec160.salario,\n        fosfec09.ultsal,\n        fosfec160.bonopen,\n        fosfec07.recsub,\n        fosfec07.cuosub,\n        fosfec160.numdaviplata,\n        fosfec07.documento,\n        fosfec160.numres,\n        fosfec160.codcat,\n        fosfec160.codpen,\n        fosfec07.codces,\n        fosfec160.codcaj\n    FROM fosfec.fosfec160 \n    INNER JOIN fosfec.fosfec07 \n        ON CONVERT(fosfec.fosfec160.cedtra USING utf8) COLLATE utf8_unicode_ci = CONVERT(fosfec.fosfec07.cedtra USING utf8) COLLATE utf8_unicode_ci \n    INNER JOIN fosfec.fosfec09 \n        ON fosfec.fosfec07.cedtra = fosfec.fosfec09.cedtra \n    LEFT JOIN fosfec.fosfec21 \n        ON CONVERT(fosfec.fosfec21.cedcon USING utf8) COLLATE utf8_unicode_ci = fosfec.fosfec160.cedtra\n    '''\n}\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 12:15:50,127 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque2/#conexion-y-carga-de-tablas-desde-sql_2","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 12:15:50,282 - INFO - CONEXION A BASE MINERVA\n2024-10-13 12:15:50,815 - INFO - Cargando fact_foscec \n2024-10-13 12:20:07,867 - INFO - Cargada fact_foscec --- 257.05 seconds ---\n2024-10-13 12:20:07,954 - INFO - CARGUE TABLAS DESDE MYSQL --- 257.67 seconds ---\n</code></pre>"},{"location":"seccion/Bloque2/#definicion-de-funciones-de-transformacion-y-limpieza","title":"Definici\u00f3n de funciones de transformaci\u00f3n y limpieza","text":"<p>Se define el identificador \u00fanico <code>columId</code> como <code>'id'</code>, y se crea una lista llamada <code>ColumnsToCompare</code> que contiene las columnas clave del dataframe que se utilizar\u00e1n en los procesos de transformaci\u00f3n y limpieza. Estas columnas incluyen datos sobre fechas, salarios, documentos, y c\u00f3digos. Esta lista es esencial para las operaciones posteriores, como la identificaci\u00f3n de duplicados y la validaci\u00f3n de datos.</p> <pre><code># Definici\u00f3n de funciones de transformaci\u00f3n y limpieza\ncolumId = 'id'  # Ajusta seg\u00fan tu identificador \u00fanico\n\n# Lista actualizada de columnas que cumplen con las restricciones\nColumnsToCompare = [\n    'id', 'cedtra', 'tipdoc', 'fecexp', 'fecsis', 'fecasi', 'fecest', 'fecnac', \n    'fecfin', 'fecdig', 'fecsal', 'fecenv', 'fecterm', 'salario', 'ultsal', \n    'bonopen', 'recsub', 'cuosub', 'numdaviplata', 'documento', 'numres', \n    'codcat', 'codpen', 'codces', 'codcaj'\n]\n</code></pre>"},{"location":"seccion/Bloque2/#proceso-de-limpieza-y-validacion-de-duplicados_1","title":"Proceso de limpieza y validaci\u00f3n de duplicados","text":"<p>Se itera sobre los dataframes en <code>df_structure</code>, guardando los duplicados mediante la funci\u00f3n <code>StoreDuplicated</code> usando las columnas especificadas en <code>ColumnsToCompare</code>. Si la columna <code>f_disp_benef</code> est\u00e1 presente, se eliminan duplicados en funci\u00f3n de esa fecha; en caso contrario, se omite esa operaci\u00f3n y se registra una advertencia en el log. Posteriormente, se limpia el dataframe de errores utilizando la funci\u00f3n <code>RemoveErrors</code>. Cada tabla procesada se registra en el log al completar la validaci\u00f3n y limpieza de duplicados. </p> <pre><code># Iterar sobre los DataFrames para realizar transformaciones\nfor key, df in df_structure.items():\n    print(f\"Procesando tabla: {key}\")\n\n    # Guardar duplicados\n    StoreDuplicated('N/A', ColumnsToCompare, df, 'duplicados_fact_fosfec_' + key)\n\n    # Eliminar duplicados por fecha (aseg\u00farate de tener una columna de fecha adecuada en tu DataFrame)\n    if 'f_disp_benef' in df.columns:\n        df_cleaned = RemoveDuplicated(columId, 'f_disp_benef', df)\n    else:\n        logger.warning(f'Columna de fecha \"f_disp_benef\" no encontrada en {key}. Se omite la eliminaci\u00f3n de duplicados por fecha.')\n        df_cleaned = df\n\n    # Limpiar errores (si fuera aplicable a tu contexto)\n    df_final = RemoveErrors(df_cleaned, 'errores_fact_foscec_' + key)\n\n    logger.info('Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: ' + key)\n</code></pre> <pre><code>2024-10-13 12:20:08,077 - WARNING - Columna de fecha \"f_disp_benef\" no encontrada en fact_foscec. Se omite la eliminaci\u00f3n de duplicados por fecha.\n2024-10-13 12:20:08,079 - INFO - Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: fact_foscec\n\n\nProcesando tabla: fact_foscec\n</code></pre> <pre><code>df_structure['fact_foscec']\n</code></pre> id cedtra tipdoc fecexp fecsis fecasi fecest fecnac fecfin fecdig ... bonopen recsub cuosub numdaviplata documento numres codcat codpen codces codcaj 0 31914 1004271963 CC 2012-11-21 2024-04-09 2024-05-16 2024-04-10 1994-01-09 None 2024-04-09 ... N S 2 3005770663 0023812 1 B 230201 None None 1 25102 1004344943 CC 2005-10-24 2022-12-03 None 2022-12-16 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017310 1 A 230301 None None 2 25102 1004344943 CC 2005-10-24 2022-12-03 2023-02-15 2023-01-11 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017493 1 A 230301 None None 3 25285 1004344943 CC 2005-10-24 2023-01-05 None 2022-12-16 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017310 1 A 230301 None None 4 25285 1004344943 CC 2005-10-24 2023-01-05 2023-02-15 2023-01-11 1989-06-04 None 2023-01-05 ... N S 1 3014970292 0017493 1 A 230301 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17118 34281 19604571 CC 2004-03-01 2024-10-04 None 2022-10-10 1985-01-25 None 2022-09-10 ... N S 4 3128019398 0016479 1 None 230301 None None 17119 34285 1082968559 CC 2011-06-21 2024-10-08 None 2024-09-17 1993-04-03 None 2024-09-10 ... N S 2 3242483705 0025692 1 None 230301 None None 17120 34287 1081786023 CC 2004-07-09 2024-10-08 2017-04-20 2020-02-06 1986-05-18 None 2018-06-13 ... N S 2 3113203794 0005686 1 None 230301 SINAFP None 17121 34293 1082841439 CC 2004-07-09 2024-10-10 None 2023-10-03 1983-02-21 None 2024-02-01 ... N S 1 3187737349 0020743 1 None 231001 None 07 17122 34293 1082841439 CC 2004-07-09 2024-10-10 None 2024-03-11 1983-02-21 None 2024-02-01 ... N S 1 3187737349 0022497 1 None 231001 None 07 <p>17123 rows \u00d7 25 columns</p>"},{"location":"seccion/Bloque2/#guardar-en-la-base-de-datos-dwh","title":"Guardar en la base de datos DWH","text":"<p>Se guarda el dataframe <code>fact_foscec</code> en la tabla <code>BD_FactFosfec</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log, seguido del registro de finalizaci\u00f3n del proceso ETL, incluyendo el tiempo total desde el inicio del proceso.</p> <pre><code>guardar_en_dwh(df_structure['fact_foscec'], 'BD_FactFosfec', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-13 12:21:05,422 - INFO - CONEXION A BASE DWH\n2024-10-13 12:21:05,934 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactFosfec\n2024-10-13 12:21:11,117 - INFO - Tabla almacenada correctamente.\n2024-10-13 12:21:11,227 - INFO - ALMACENAMIENTO --- 5.80 seconds ---\n</code></pre> <pre><code># Tiempo total de ejecuci\u00f3n\nlogger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 23:45:03,410 - INFO - FINAL ETL --- 243.36 seconds ---\n</code></pre> <pre><code># \ntabla = df_structure['fact_foscec']\nprint(f'Numero de registros {tabla.shape[0]}')\ntabla.columns.tolist()\n\n# Agrupamos por documento y periodo, aplicando funciones de agregaci\u00f3n que tengan sentido\ndf_grouped = tabla.groupby(['fecasi', 'documento']).agg({\n    'cedtra': 'last',  \n    'tipdoc': 'last',  \n    'fecest': 'last',  \n    'salario': 'last', \n    'ultsal': 'last', \n    'bonopen': 'last', \n    'recsub': 'last',  \n    'cuosub': 'sum',   \n    'numdaviplata': 'last',  \n    'codcat': 'last',  \n    'codpen': 'last',  \n}).reset_index()\n\n# Comparar el n\u00famero de registros antes y despu\u00e9s\noriginal_count = tabla.shape[0]\ngrouped_count = df_grouped.shape[0]\n\noriginal_count, grouped_count\n</code></pre> <pre><code>Numero de registros 17123\n\n\n\n\n\n(17123, 9460)\n</code></pre> <pre><code>df_grouped\n</code></pre> fecasi documento cedtra tipdoc fecest salario ultsal bonopen recsub cuosub numdaviplata codcat codpen 0 2014-07-24 0000002 85473356 CC None 1.560.000 737717 None S 2 NO TENGO A 230301 1 2014-07-24 0000003 39032364 CC None 720.000 781242 None N 0 3152117593 A 25-14 2 2014-07-24 0000021 85473678 CC None 877803 1129093 None S 1 3013570721 A 231001 3 2014-07-24 0000024 39046011 CC None 3328000 1284000 None N 0 3175011658 B 230301 4 2014-07-24 0000030 7629151 CC None 1300000 1300000 N N 0 3103527803 A 230201 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9455 2024-09-18 0025288 1004272164 CC 2024-08-26 13000000 1300000 N N 0 3243202882 A 230301 9456 2024-09-18 0025289 57467186 CC 2024-09-25 1950000 3300000 N S 1 3182112369 B 230201 9457 2024-09-18 0025290 1128105347 CC 2024-08-27 1300000 1300000 N S 2 3145862495 A 231001 9458 2024-09-18 0025294 36667152 CC None 1300000 1300000 N N 0 3238985219 A 25-14 9459 2024-09-18 0025295 1082977918 CC None 1442158 1442158 N N 0 3008801927 A 230201 <p>9460 rows \u00d7 13 columns</p>"},{"location":"seccion/Bloque4/","title":"Bloque4","text":"<p>'Bloque4'</p>"},{"location":"seccion/Bloque4/#41-factcolegio","title":"4.1-FactColegio","text":""},{"location":"seccion/Bloque4/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 13-10-2024 11:09\n</code></pre>"},{"location":"seccion/Bloque4/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Colegio.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Colegio.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-13 11:09:15,597 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque4/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se definen tres consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de las tablas <code>colegio12</code>, <code>colegio13</code>, y <code>colegio15</code> de la base de datos <code>colegio</code>. Cada consulta selecciona columnas clave relacionadas con estudiantes, acudientes y matr\u00edculas. El diccionario <code>dim_names</code> se utiliza para mapear cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"colegio12\":'''select \n                ano,numdoc,coddoc,ciuexp,priape,segape,prinom,segnom,rh,eps,codsex,dirnac,ciunac,fecnac,\n                dirres,barrio,estrato,codciu,telres,email,nota,observacion,docpad,docmad,docacu,codben,codcat,tipo\n                from colegio.colegio12''',\n    \"colegio13\":'''select \n                numdoc,coddoc,ciuexp,priape,segape,prinom,segnom,codpar,profesion,empresa,ocupacion,codsex,\n                direccion,codciu,telefono,email\n                from colegio.colegio13''',\n    \"colegio15\":'''select \n                ano,nummat,numdoc,codgra,codgru,fecmat,colant,numlib,numfol,codcon,codest,codcat,\n                estado,fecest,motivo,usuario,estbol\n                from colegio.colegio15'''\n               }\ndim_names = {\n    \"colegio12\":'BD_Datos_Estudiantes',\n    \"colegio13\":'BD_Datos_Acudientes',\n    \"colegio15\":'BD_Datos_Matriculas'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-13 11:09:15,612 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque4/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n a la base de datos de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-13 11:09:15,679 - INFO - CONEXION A BASE MINERVA\n2024-10-13 11:09:16,195 - INFO - Cargando colegio12 \n2024-10-13 11:09:16,800 - INFO - Cargada colegio12 --- 0.60 seconds ---\n2024-10-13 11:09:16,802 - INFO - Cargando colegio13 \n2024-10-13 11:09:16,983 - INFO - Cargada colegio13 --- 0.18 seconds ---\n2024-10-13 11:09:16,985 - INFO - Cargando colegio15 \n2024-10-13 11:09:17,167 - INFO - Cargada colegio15 --- 0.18 seconds ---\n2024-10-13 11:09:17,252 - INFO - CARGUE TABLAS DESDE MYSQL --- 1.57 seconds ---\n</code></pre>"},{"location":"seccion/Bloque4/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la presencia de registros duplicados en todas las tablas de <code>df_structure</code>. Se comparan las columnas especificadas excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla utilizando <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,353 - INFO - VALIDADOR TABLA: colegio12\n2024-10-13 11:09:17,381 - INFO - VALIDADOR TABLA: colegio13\n2024-10-13 11:09:17,409 - INFO - VALIDADOR TABLA: colegio15\n2024-10-13 11:09:17,410 - INFO - VALIDADOR DUPLICADOS --- 0.15 seconds ---\n</code></pre>"},{"location":"seccion/Bloque4/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final de cada valor, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code>. El tiempo total de la limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA  --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,538 - INFO - LIMPIEZA  --- 0.12 seconds ---\n</code></pre>"},{"location":"seccion/Bloque4/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-13 11:09:17,550 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/Bloque4/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code># Conexi\u00f3n a la base DWH utilizando 'with' para una conexi\u00f3n segura\nalmacenamiento_time = time.time()\nlogger.info('CONEXION A BASE DWH')\n\n# Utilizamos 'with' para asegurar que la conexi\u00f3n se cierre autom\u00e1ticamente\nwith create_engine(obtener_conexion('dwh')).begin() as conn:\n    for ky in df_structure.keys():\n        logger.info(f'Almacenando tabla {ky} en DWH como {dim_names[ky]}')\n        # Guardar cada DataFrame en su tabla correspondiente en la base DWH\n        df_structure[ky].to_sql(name=dim_names[ky], con=conn, if_exists='replace', index=False)\n        logger.info(f'Tabla {ky} almacenada correctamente.')\nlogger.info(f'ALMACENAMIENTO --- {time.time() - almacenamiento_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:17,563 - INFO - CONEXION A BASE DWH\n2024-10-13 11:09:18,066 - INFO - Almacenando tabla colegio12 en DWH como BD_Datos_Estudiantes\n2024-10-13 11:09:20,287 - INFO - Tabla colegio12 almacenada correctamente.\n2024-10-13 11:09:20,289 - INFO - Almacenando tabla colegio13 en DWH como BD_Datos_Acudientes\n2024-10-13 11:09:21,393 - INFO - Tabla colegio13 almacenada correctamente.\n2024-10-13 11:09:21,395 - INFO - Almacenando tabla colegio15 en DWH como BD_Datos_Matriculas\n2024-10-13 11:09:22,511 - INFO - Tabla colegio15 almacenada correctamente.\n2024-10-13 11:09:22,616 - INFO - ALMACENAMIENTO --- 5.05 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-13 11:09:22,627 - INFO - FINAL ETL --- 7.06 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/Bloque4/#41-factcolegio_fijos","title":"4.1-FactColegio_fijos","text":""},{"location":"seccion/Bloque4/#importacion-de-librerias-y-funciones_1","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport pymysql\nimport time\nimport logging\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import setup_logger, Conexion_dwh, testfunciones\nprint(testfunciones())\n# Configuraci\u00f3n inicial\ntime_start = time.time()\nlogger = setup_logger(log_filename='Merge_Colegio_DimDatosFijos.log', log_level=logging.INFO)\nlogger.info('INICIO DE PROCESO')\n</code></pre> <pre><code>2024-10-05 10:25:55,575 - INFO - INICIO DE PROCESO\n\n\nImportacion de funciones correcta, 05-10-2024 10:25\n</code></pre>"},{"location":"seccion/Bloque4/#definicion-de-consultas-sql-con-columnas-requeridas","title":"Definici\u00f3n de consultas SQL con columnas requeridas","text":"<p>Se definen consultas SQL en el diccionario <code>qr_structure</code> para extraer los datos de las tablas <code>BD_Datos_Estudiantes</code> y <code>BD_Datos_Acudientes</code>. Se asegura que las columnas necesarias, como <code>id</code>, <code>fecnac</code>, <code>sexo</code>, <code>coddoc</code>, <code>numdocumento</code>, <code>prinom</code>, <code>segnom</code>, <code>priape</code>, y <code>segape</code>, est\u00e9n presentes en los resultados. Se utiliza la funci\u00f3n <code>create_engine()</code> para conectar a la base de datos DWH. La conexi\u00f3n exitosa se registra en el log.</p> <pre><code># Definir las consultas SQL para cargar datos y asegurarse de crear la estructura con los campos requeridos\nrequired_columns = ['id', 'fecnac', 'sexo', 'coddoc', 'numdocumento', 'prinom', 'segnom', 'priape', 'segape']\nqr_structure = {\n    \"colegio12\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS id,\n        fecnac,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS numdocumento,\n        prinom,\n        segnom,\n        priape,\n        segape\n    FROM BD_Datos_Estudiantes''',\n    \"colegio13\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS id,\n        NULL AS fecnac,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS numdocumento,\n        prinom,\n        segnom,\n        priape,\n        segape\n    FROM BD_Datos_Acudientes'''\n}\n\n# Conectar a la base de datos DWH\nmotor = create_engine(Conexion_dwh())\nlogger.info('CONEXI\u00d3N A BASE DE DATOS DWH ESTABLECIDA')\n</code></pre> <pre><code>2024-10-05 10:26:01,809 - INFO - CONEXI\u00d3N A BASE DE DATOS DWH ESTABLECIDA\n</code></pre>"},{"location":"seccion/Bloque4/#cargar-y-estructurar-tablas-con-las-columnas-requeridas","title":"Cargar y estructurar tablas con las columnas requeridas","text":"<p>Se cargan las tablas <code>colegio12</code> y <code>colegio13</code> desde la base de datos, garantizando que todas las columnas requeridas est\u00e9n presentes en los dataframes resultantes. Si alguna columna falta, se crea con el valor <code>\"SinDato\"</code>. Posteriormente, se concatenan ambas tablas en un solo dataframe <code>df_colegio_combined</code>. Los eventos de carga y concatenaci\u00f3n se registran en el log.</p> <pre><code># Cargar las tablas en DataFrames y asegurar la estructura requerida\ndf_structure = {}\nfor key, query in qr_structure.items():\n    try:\n        with motor.begin() as conn:\n            logger.info(f'CARGANDO TABLA: {key}')\n            df = pd.read_sql_query(sa.text(query), conn)\n\n            # Asegurarse de que las columnas requeridas est\u00e9n presentes en la estructura\n            for column in required_columns:\n                if column not in df.columns:\n                    df[column] = \"SinDato\"\n\n            df_structure[key] = df[required_columns]\n            logger.info(f'TABLA {key} CARGADA Y ESTRUCTURADA')\n    except Exception as e:\n        logger.error(f'Error al cargar la tabla {key}: {str(e)}')\n\n# Concatenar las tablas de colegio12 y colegio13\ndf_colegio_combined = pd.concat([df_structure['colegio12'], df_structure['colegio13']], ignore_index=True)\nlogger.info('TABLAS colegio12 Y colegio13 COMBINADAS')\n</code></pre> <pre><code>2024-10-05 11:00:40,892 - INFO - CARGANDO TABLA: colegio12\n2024-10-05 11:00:41,178 - INFO - TABLA colegio12 CARGADA Y ESTRUCTURADA\n2024-10-05 11:00:41,256 - INFO - CARGANDO TABLA: colegio13\n2024-10-05 11:00:41,352 - INFO - TABLA colegio13 CARGADA Y ESTRUCTURADA\n2024-10-05 11:00:41,431 - INFO - TABLAS colegio12 Y colegio13 COMBINADAS\n</code></pre>"},{"location":"seccion/Bloque4/#filtrar-registros-unicos-por-id","title":"Filtrar registros \u00fanicos por <code>id</code>","text":"<p>Se eliminan los registros duplicados en el dataframe <code>df_colegio_combined</code> manteniendo solo los registros con <code>id</code> \u00fanicos. El dataframe resultante se almacena en <code>df_colegio_unique</code>, y el proceso se registra en el log.</p> <pre><code># Obtener solo los registros con 'id' \u00fanicos\ndf_colegio_unique = df_colegio_combined.drop_duplicates(subset='id')\nlogger.info('REGISTROS \u00daNICOS DE ID OBTENIDOS')\n</code></pre> <pre><code>2024-10-05 11:00:45,227 - INFO - REGISTROS \u00daNICOS DE ID OBTENIDOS\n</code></pre> <pre><code>df_colegio_unique\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 TI1001941478 2003-08-18 M TI 1001941478 ISAAC DAVID SEVILLA LOPEZ 2 TI1003266048 2002-12-25 M TI 1003266048 JORGE LUIS GARCIA JIMENEZ 4 TI1004358737 2002-04-24 M TI 1004358737 CARLOS DAVID GUZMAN SARMIENTO 6 TI1004367232 2003-06-10 F TI 1004367232 ARIADNA SOFIA EFFER OTERO 8 TI1004462648 2003-05-10 M TI 1004462648 SEBASTIAN ALEJANDRO JOYA MORALES ... ... ... ... ... ... ... ... ... ... 3719 CC93400713 None M CC 93400713 OSCAR ALEDT GIRALDO CORREA 3720 CC94520471 None M CC 94520471 JEAN PAUL TORRES VELEZ 3721 CC9770518 None M CC 9770518 MAURICIO None CADENA HOYOS 3722 CC98668306 None M CC 98668306 JUAN CARLOS POSADA GALLARDO 3723 CCMAGDALENA None M CC MAGDALENA FREDYS JOSE CAMPO MENDOZA <p>2428 rows \u00d7 9 columns</p>"},{"location":"seccion/Bloque4/#filtrar-registros-unicos-no-presentes-en-dimdatosfijos","title":"Filtrar registros \u00fanicos no presentes en <code>DimDatosFijos</code>","text":"<p>Se carga la tabla <code>BD_DimDatosFijos</code> desde la base de datos DWH y se filtran los registros del dataframe <code>df_colegio_combined</code> que no est\u00e1n presentes en la tabla <code>DimDatosFijos</code> basada en el campo <code>id</code>. Los registros faltantes se almacenan en el dataframe <code>df_missing_in_dwh</code>, y ambos procesos se registran en el log.</p> <pre><code># Obtener la tabla DimDatosFijos del DWH\ndf_dwh = pd.read_sql_query(\"SELECT * FROM dwh.BD_DimDatosFijos\", motor)\nlogger.info('TABLA DimDatosFijos CARGADA')\n\n# Filtrar los registros \u00fanicos que no est\u00e1n presentes en DimDatosFijos\ndf_missing_in_dwh = df_colegio_combined[~df_colegio_combined['id'].isin(df_dwh['id'])]\nlogger.info('FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos')\n</code></pre> <pre><code>2024-10-05 11:01:22,826 - INFO - TABLA DimDatosFijos CARGADA\n2024-10-05 11:01:23,375 - INFO - FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos\n</code></pre> <pre><code>df_missing_in_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 96 RC1048070119 2007-06-07 M RC 1048070119 JOSHUA None PASCAGAZA MAZENETT 136 RC1067624048 2013-12-01 M RC 1067624048 JUAN ESTEBAN RIVAS DAVILA 159 RC1081798749 2007-04-17 F RC 1081798749 JUANA VALENTINA GARCIA SANCHEZ 166 RC1081815011 2011-02-21 F RC 1081815011 KARLA SOFIA GARCIA SANCHEZ 282 RC1082866607 2005-08-30 M RC 1082866607 MAURICIO DAVID MOGOLLON DURAN ... ... ... ... ... ... ... ... ... ... 2352 TI1205965079 2015-01-15 M TI 1205965079 YEIKO JULIAN REY AGUDELO 2371 TI1205966118 2015-10-09 F TI 1205966118 SOFIA VALENTINA CABALLERO PEREZ 2374 TI1205966119 2015-10-12 F TI 1205966119 SALOME None RAMIREZ JIMENEZ 2381 NP1205966853 2016-04-05 M NP 1205966853 SIMON JOSE ARANGO LAFAURIE 2384 TI1205967151 2016-06-13 F TI 1205967151 TANIA ROSA CANTILLO RUEDA <p>169 rows \u00d7 9 columns</p> <pre><code>df_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 RC1025772327 2016-03-13 M RC 1025772327 THIAGO ALEJANDRO VALENCIA SALDARRIAGA 1 RC1083028689 2015-09-20 F RC 1083028689 BETZAIN None BARROS JIMENEZ 2 TI1081809711 2009-10-09 M TI 1081809711 JUAN DAVID BOLA\u00d1OS SANJUAN 3 TI1128200488 2008-08-07 F TI 1128200488 LAURA MARGARITA MESA JIMENEZ 4 TI1082916077 2008-05-27 M TI 1082916077 SEBASTIAN ANDRES VILLALOBO PALENCIA ... ... ... ... ... ... ... ... ... ... 975246 CC85466094 NaT None CC None CAMILO None VARGAS SALCEDO 975247 CC85471573 NaT None CC None JUAN CARLOS SANDOVAL VIVES 975248 CC85475426 NaT None CC None ASDRUBAL ENRIQUE POMARES CORREDOR 975249 CC858464327 NaT None CC None GABRIEL ANGEL BARRETO PION 975250 CCMAGDALENA NaT None CC None FREDYS JOSE CAMPO MENDOZA <p>975251 rows \u00d7 9 columns</p>"},{"location":"seccion/Bloque4/#guardar-registros-faltantes-en-la-tabla-dimdatosfijos","title":"Guardar registros faltantes en la tabla <code>DimDatosFijos</code>","text":"<p>Se conectan los registros faltantes en la tabla <code>BD_DimDatosFijos</code> en el DWH y se almacenan los registros del dataframe <code>df_missing_in_dwh</code> en la base de datos utilizando el modo <code>append</code>. El proceso de almacenamiento se registra en el log, y en caso de error, se captura y se registra el mensaje correspondiente.</p> <pre><code># Conectar a la base de datos DWH\nmotor3 = create_engine(Conexion_dwh())\n# Guardar los registros faltantes en la tabla DimDatosFijos\nalmacenamiento_time = time.time()\ntry:\n    df_missing_in_dwh.to_sql(name='BD_Dim_datosfijos', con=motor3, if_exists='append', index=False)\n    logger.info(f'ALMACENAMIENTO DE REGISTROS FALTANTES EN DimDatosFijos --- {time.time() - almacenamiento_time:.2f} seconds ---')\nexcept Exception as e:\n    logger.error(f'Error al almacenar registros en DimDatosFijos: {str(e)}')\n</code></pre> <pre><code>2024-10-05 11:02:18,222 - INFO - ALMACENAMIENTO DE REGISTROS FALTANTES EN DimDatosFijos --- 0.97 seconds ---\n</code></pre>"},{"location":"seccion/Bloque4/#cargar-la-tabla-dimdatosfijos-desde-el-dwh","title":"Cargar la tabla <code>DimDatosFijos</code> desde el DWH","text":"<p>Se carga la tabla <code>BD_DimDatosFijos</code> desde la base de datos DWH en el dataframe <code>df_dwh</code> utilizando <code>pd.read_sql_query</code>. El evento de carga exitosa se registra en el log.</p> <pre><code># Obtener la tabla DimDatosFijos del DWH\ndf_dwh = pd.read_sql_query(\"SELECT * FROM dwh.BD_DimDatosFijos\", motor)\nlogger.info('TABLA DimDatosFijos CARGADA')\n</code></pre> <pre><code>2024-10-05 11:02:57,473 - INFO - TABLA DimDatosFijos CARGADA\n</code></pre> <pre><code>df_dwh\n</code></pre> id fecnac sexo coddoc numdocumento prinom segnom priape segape 0 RC1025772327 2016-03-13 M RC 1025772327 THIAGO ALEJANDRO VALENCIA SALDARRIAGA 1 RC1083028689 2015-09-20 F RC 1083028689 BETZAIN None BARROS JIMENEZ 2 TI1081809711 2009-10-09 M TI 1081809711 JUAN DAVID BOLA\u00d1OS SANJUAN 3 TI1128200488 2008-08-07 F TI 1128200488 LAURA MARGARITA MESA JIMENEZ 4 TI1082916077 2008-05-27 M TI 1082916077 SEBASTIAN ANDRES VILLALOBO PALENCIA ... ... ... ... ... ... ... ... ... ... 975246 CC85466094 NaT None CC None CAMILO None VARGAS SALCEDO 975247 CC85471573 NaT None CC None JUAN CARLOS SANDOVAL VIVES 975248 CC85475426 NaT None CC None ASDRUBAL ENRIQUE POMARES CORREDOR 975249 CC858464327 NaT None CC None GABRIEL ANGEL BARRETO PION 975250 CCMAGDALENA NaT None CC None FREDYS JOSE CAMPO MENDOZA <p>975251 rows \u00d7 9 columns</p> <pre><code>\n</code></pre>"},{"location":"seccion/Bloque5/","title":"Bloque5","text":""},{"location":"seccion/Bloque5/#51-factserviciosocial-copy","title":"5.1-FactServicioSocial copy","text":""},{"location":"seccion/Bloque5/#51-factserviciosocial-copy_1","title":"5.1-FactServicioSocial copy","text":""},{"location":"seccion/Bloque5/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 16-10-2024 21:35\n</code></pre>"},{"location":"seccion/Bloque5/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>ServicioSocial.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='ServicioSocial.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-16 21:35:42,419 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque5/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se definen dos consultas SQL en el diccionario <code>qr_structure</code> para extraer datos de las tablas <code>estudiantes</code> y <code>parientes</code> de la base de datos <code>nsijec</code>. Cada consulta selecciona columnas clave relacionadas con estudiantes y sus parientes. El diccionario <code>dim_names</code> se utiliza para mapear cada consulta a su respectiva tabla en la base de datos de destino. El proceso de lectura de consultas se registra en el log.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"estudiantes\":'''select \n                actividad_productiva,carga_masiva,convenio,created_at,created_by,direccion,enfermedad_alergia,estado,\n                fecha_inscripcion,grado,id,infraestructura,institucion,instructor,jornada,mod_prestacion_servicio,\n                modalidad,municipio,nivel_escolar,nombre_enfermedad_alergia,nombres_medicamentos,observacion,periodo,persona,pertenencia_etnica,pob_estudiante,poblacion,proyecto,pueblo_indigena,resguardo,telefono,tipo_localidad,toma_medicamentos,trabajador_adolecente,updated_at,updated_by\n                from nsijec.estudiantes''',\n    \"colegio13\":'''select \n                acudiente,created_at,created_by,direccion,email,estrato,estudiante,id,municipio,\n                parentesco,pariente,telefono,updated_at,updated_by\n                from nsijec.parientes'''                \n               }\n\n#Lista de querys Neith\nqr_structureNeith = {\n    \"acudientes\":'''select * from saipi.acudientes''',\n    \"registros\":'''select * from saipi.registros''',\n    \"valoracion\":'''select * from saipi.valoracion'''\n    }\n\ndim_names = {\n    \"estudiantes\":'BD_Estudiantes',\n    \"colegio13\":'BD_Parientes_Estudiantes',\n    \"acudientes\":'BD_Acudientes_Estudiantes',\n    \"registros\":'BD_Registros_Estudiantes',\n    \"valoracion\":'BD_Valoracion_Estudiantes'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-16 21:35:42,439 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/Bloque5/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code># Cargar tablas desde la base 'minerva'\nmotor = create_engine(obtener_conexion('minerva'))\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-16 21:35:43,360 - INFO - Cargando estudiantes \n2024-10-16 21:35:45,336 - INFO - Cargada estudiantes --- 1.98 seconds ---\n2024-10-16 21:35:45,338 - INFO - Cargando colegio13 \n2024-10-16 21:35:45,418 - INFO - Cargada colegio13 --- 0.08 seconds ---\n2024-10-16 21:35:45,496 - INFO - CARGUE TABLAS DESDE MYSQL --- colegio13 --- 2.62 seconds ---\n</code></pre> <pre><code># Cargar tablas desde la base 'neith'\nmotorNeith = create_engine(obtener_conexion('neith'))\ncargar_tablas(motorNeith, qr_structureNeith, df_structure, logger)\n</code></pre> <pre><code>2024-10-16 21:35:46,018 - INFO - Cargando acudientes \n2024-10-16 21:35:46,542 - INFO - Cargada acudientes --- 0.52 seconds ---\n2024-10-16 21:35:46,544 - INFO - Cargando registros \n2024-10-16 21:35:47,163 - INFO - Cargada registros --- 0.62 seconds ---\n2024-10-16 21:35:47,165 - INFO - Cargando valoracion \n2024-10-16 21:35:47,737 - INFO - Cargada valoracion --- 0.57 seconds ---\n2024-10-16 21:35:47,825 - INFO - CARGUE TABLAS DESDE MYSQL --- valoracion --- 2.31 seconds ---\n</code></pre>"},{"location":"seccion/Bloque5/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas de <code>df_structure</code>, excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-16 21:35:48,125 - INFO - VALIDADOR TABLA: estudiantes\n2024-10-16 21:35:48,156 - INFO - VALIDADOR TABLA: colegio13\n2024-10-16 21:35:48,211 - INFO - VALIDADOR TABLA: acudientes\n2024-10-16 21:35:48,294 - INFO - VALIDADOR TABLA: registros\n2024-10-16 21:35:48,357 - INFO - VALIDADOR TABLA: valoracion\n2024-10-16 21:35:48,358 - INFO - VALIDADOR DUPLICADOS --- 0.52 seconds ---\n</code></pre>"},{"location":"seccion/Bloque5/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_28760\\3328129721.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nC:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_28760\\3328129721.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\n2024-10-16 21:35:49,815 - INFO - LIMPIEZA --- 1.44 seconds ---\n</code></pre>"},{"location":"seccion/Bloque5/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH utilizando los nombres definidos en <code>dim_names</code>. Si la clave de la tabla no est\u00e1 presente en <code>dim_names</code>, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra, junto con los nombres de las tablas que se almacenaron correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-16 21:35:49,828 - INFO - CONEXION A BASE DWH\n2024-10-16 21:35:50,307 - INFO - Almacenando tabla estudiantes en DWH como BD_Estudiantes\n2024-10-16 21:35:55,525 - INFO - Tabla estudiantes almacenada correctamente como BD_Estudiantes.\n2024-10-16 21:35:55,527 - INFO - Almacenando tabla colegio13 en DWH como BD_Parientes_Estudiantes\n2024-10-16 21:35:56,276 - INFO - Tabla colegio13 almacenada correctamente como BD_Parientes_Estudiantes.\n2024-10-16 21:35:56,278 - INFO - Almacenando tabla acudientes en DWH como BD_Acudientes_Estudiantes\n2024-10-16 21:35:58,477 - INFO - Tabla acudientes almacenada correctamente como BD_Acudientes_Estudiantes.\n2024-10-16 21:35:58,479 - INFO - Almacenando tabla registros en DWH como BD_Registros_Estudiantes\n2024-10-16 21:36:01,440 - INFO - Tabla registros almacenada correctamente como BD_Registros_Estudiantes.\n2024-10-16 21:36:01,443 - INFO - Almacenando tabla valoracion en DWH como BD_Valoracion_Estudiantes\n2024-10-16 21:36:04,326 - INFO - Tabla valoracion almacenada correctamente como BD_Valoracion_Estudiantes.\n2024-10-16 21:36:04,416 - INFO - ALMACENAMIENTO ---  --- 14.59 seconds ---\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-16 21:36:04,428 - INFO - FINAL ETL --- 22.13 seconds ---\n</code></pre>"},{"location":"seccion/Bloque5/#union-de-dataframes-con-registro-en-dwh","title":"Uni\u00f3n de DataFrames con registro en DWH","text":"<p>Este c\u00f3digo identifica y elimina columnas con m\u00e1s del 95% de datos vac\u00edos en tres DataFrames: <code>valoracion</code>, <code>registros</code> y <code>acudientes</code>. Posteriormente, realiza una union entre estas tablas limpias, comenzando con la fusi\u00f3n de <code>acudientes</code> con <code>registros</code>, seguida por la uni\u00f3n con la tabla <code>valoracion</code>. El DataFrame resultante, <code>final_merged_limpio</code>, se guarda en la tabla <code>BD_FactServicioSocial</code> dentro del DWH utilizando la funci\u00f3n <code>guardar_en_dwh</code>, reemplazando los datos existentes si es necesario.</p> <pre><code># Asumiendo que valoracion_df, registros_df y acudientes_df est\u00e1n en df_structure\nvaloracion_df = df_structure['valoracion']\nregistros_df = df_structure['registros']\nacudientes_df = df_structure['acudientes']\n\n# Funci\u00f3n para calcular las columnas con m\u00e1s del threshold% de datos vac\u00edos\ndef columnas_con_muchos_nulos(df, threshold=0.95):\n    porcentaje_nulos = df.isnull().mean()\n    columnas_excluir = porcentaje_nulos[porcentaje_nulos &gt; threshold].index\n    return columnas_excluir\n\n# Verificar qu\u00e9 columnas tienen m\u00e1s del 95% de datos vac\u00edos y eliminarlas\nthreshold = 0.95\ncolumnas_acudientes_excluir = columnas_con_muchos_nulos(acudientes_df, threshold)\ncolumnas_registros_excluir = columnas_con_muchos_nulos(registros_df, threshold)\ncolumnas_valoracion_excluir = columnas_con_muchos_nulos(valoracion_df, threshold)\n\n# Excluir las columnas identificadas en cada DataFrame\nacudientes_df_limpio = acudientes_df.drop(columns=columnas_acudientes_excluir)\nregistros_df_limpio = registros_df.drop(columns=columnas_registros_excluir)\nvaloracion_df_limpio = valoracion_df.drop(columns=columnas_valoracion_excluir)\n\n# Realizar las uniones con las tablas ya filtradas\n# Paso 1: Unir la tabla de acudientes con registros\nacudientes_registros_merged = pd.merge(acudientes_df_limpio, registros_df_limpio, left_on='registro_id', right_on='id', suffixes=('_acudiente', '_registro'))\n\n# Paso 2: Unir el resultado anterior con la tabla de valoraciones\nfinal_merged_limpio = pd.merge(acudientes_registros_merged, valoracion_df_limpio, left_on='registro_id', right_on='registro_id', suffixes=('', '_valoracion'))\n\nguardar_en_dwh(final_merged_limpio, 'BD_FactServicioSocial', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-16 21:36:04,572 - INFO - CONEXION A BASE DWH\n2024-10-16 21:36:05,085 - INFO - Almacenando tabla \u00fanica en DWH como BD_FactServicioSocial\n2024-10-16 21:36:10,499 - INFO - Tabla almacenada correctamente.\n2024-10-16 21:36:10,609 - INFO - ALMACENAMIENTO ---  --- 6.04 seconds ---\n</code></pre> <pre><code>def df_columnas(df):\n    # Obtener los nombres de las columnas\n    nombres_columnas = df.columns.tolist()\n\n    # Crear un nuevo DataFrame con los nombres de las columnas\n    df_nombres = pd.DataFrame(nombres_columnas, columns=['Nombre_Columnas'])\n\n    return df_nombres\n\ntabla_columnas = df_columnas(final_merged_limpio)\n</code></pre>"},{"location":"seccion/Bloque6/","title":"Bloque6","text":""},{"location":"seccion/Bloque6/#61-main_encuestasv1","title":"6.1-main_encuestasV1","text":""},{"location":"seccion/Bloque6/#61-main_encuestasv1_1","title":"6.1-main_encuestasV1","text":""},{"location":"seccion/Bloque6/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport os\nimport datetime\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport logging\nfrom datetime import date\n\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import testfunciones, setup_logger\nfrom Funciones import Conexion_dwh, Conexion_neith\n\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 12-10-2024 11:33\n</code></pre>"},{"location":"seccion/Bloque6/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Encuestas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Encuestas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-12 11:33:22,727 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/Bloque6/#funcion-para-limpiar-html","title":"Funci\u00f3n para limpiar HTML","text":"<p>La funci\u00f3n limpiar_html() toma un texto en formato HTML y lo limpia utilizando la biblioteca BeautifulSoup, devolviendo solo el texto sin etiquetas HTML.</p>"},{"location":"seccion/Bloque6/#funcion-para-cargar-consultas-en-paralelo","title":"Funci\u00f3n para cargar consultas en paralelo","text":"<p>Se utiliza ThreadPoolExecutor para paralelizar la ejecuci\u00f3n de consultas SQL mediante la funci\u00f3n load_query(). Esta funci\u00f3n intenta ejecutar la consulta y devolver los resultados como un dataframe, manejando errores para devolver None si la consulta falla.</p> <pre><code>def limpiar_html(texto_html):\n    soup = BeautifulSoup(texto_html, 'html.parser')\n    return soup.get_text()\n###Librerias para paralelizar\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef load_query(query):\n    try: \n        df_query = pd.read_sql_query(query, motor_consulta)\n        return df_query\n    except:\n        return None\n</code></pre> <pre><code>#Medir tiempos\nstart_time = time.time()\n</code></pre>"},{"location":"seccion/Bloque6/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion_pruebas = Conexion_dwh()\nmotor_pruebas = create_engine(cadena_conexion_pruebas)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-12 11:33:22,823 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-del-diccionario-de-tablas-de-la-base-de-encuestas","title":"Consulta del diccionario de tablas de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener las tablas de la base de datos <code>encuestas</code> que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code> y <code>NombreTabla</code> desde la tabla <code>gb_Dim_Bodega_Inventario de Tablas</code> del DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que las tablas est\u00e9n marcadas como incluidas en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_tablas</code>.</p> <pre><code>##1. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla FROM dwh.`gb_Dim_Bodega_Inventario de Tablas`  \n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_tablas = pd.read_sql_query(sa.text(qr_structure), conn)\n#df_tablas\n</code></pre>"},{"location":"seccion/Bloque6/#extraccion-de-codigos-de-encuestas","title":"Extracci\u00f3n de c\u00f3digos de encuestas","text":"<p>Se crea una nueva columna <code>sid</code> en el dataframe <code>df_tablas</code> que contiene los c\u00f3digos de encuestas. Estos c\u00f3digos se extraen dividiendo el valor de la columna <code>NombreTabla</code> utilizando el car\u00e1cter de subrayado (<code>_</code>) como delimitador, y tomando el tercer elemento resultante de la divisi\u00f3n (\u00edndice 2). Luego, la columna <code>sid</code> se convierte en tipo <code>str</code> para asegurarse de que todos los valores est\u00e9n en formato de cadena.</p> <pre><code>## Lista de codigos de encuestas\n#df_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\")\ndf_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\",expand = True)[2]\ndf_tablas[\"sid\"] = df_tablas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/Bloque6/#conexion-a-la-base-de-datos-neith","title":"Conexi\u00f3n a la base de datos Neith","text":"<p>Se establece la conexi\u00f3n con la base de datos Neith utilizando la funci\u00f3n <code>Conexion_neith()</code> para generar la cadena de conexi\u00f3n y <code>create_engine()</code> para crear el motor de conexi\u00f3n. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base Neith\ncadena_conexion_neith = Conexion_neith()\nmotor_neith = create_engine(cadena_conexion_neith)\nlogger.info('CONEXION A BASE Neith')\n</code></pre> <pre><code>2024-10-12 11:33:23,561 - INFO - CONEXION A BASE Neith\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-de-nombres-de-encuestas","title":"Consulta de nombres de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los nombres de las encuestas desde la tabla <code>lime_surveys_languagesettings</code> de la base de datos <code>encuestas</code> en Neith. La consulta selecciona los campos <code>surveyls_survey_id</code> (identificado como <code>sid</code>) y <code>surveyls_title</code> (nombre de la encuesta). El resultado de la consulta se almacena en el dataframe <code>df_nombre_encuestas</code>, y se asegura que la columna <code>sid</code> sea de tipo <code>str</code>.</p> <pre><code>##2. Consultar nombres de las encuestas\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT surveyls_survey_id as sid, surveyls_title as NombreEncuesta \n    FROM encuestas.lime_surveys_languagesettings \"\"\"\n    df_nombre_encuestas = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_nombre_encuestas[\"sid\"] = df_nombre_encuestas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/Bloque6/#fusion-de-tablas-codigos-de-encuestas-y-nombres-de-encuestas","title":"Fusi\u00f3n de tablas: c\u00f3digos de encuestas y nombres de encuestas","text":"<p>Se realiza una combinaci\u00f3n (merge) de los dataframes <code>df_tablas</code> y <code>df_nombre_encuestas</code> utilizando la columna <code>sid</code> como clave de uni\u00f3n. Esto permite agregar los nombres de las encuestas a las tablas correspondientes. Posteriormente, se eliminan las filas con valores nulos en <code>df_tablas_nombres</code>, y se reinicia el \u00edndice del dataframe resultante para garantizar un formato limpio.</p> <pre><code>## Nombres de las tablas de encuestas\ndf_tablas_nombres = pd.merge(df_tablas,df_nombre_encuestas, how ='left', on = 'sid')\ndf_tablas_nombres = df_tablas_nombres.dropna().copy().reset_index(drop =True)\n</code></pre> <pre><code>df_tablas_nombres.to_excel('df_tablas.xlsx',index=False)\n</code></pre> <pre><code>df_tablas_codigos = df_tablas_nombres[['NombreBaseDeDatos', 'NombreTabla', 'sid']]\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-del-diccionario-de-campos-de-la-base-de-encuestas","title":"Consulta del diccionario de campos de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los campos de las tablas de la base de datos <code>encuestas</code>, que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, y <code>NombreCampo</code> desde la tabla <code>gb_Dim_Bodega_Diccionario de Campos</code> en el DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que los campos est\u00e9n marcados como incluidos en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_campos</code>.</p> <pre><code>##3. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla, NombreCampo FROM dwh.`gb_Dim_Bodega_Diccionario de Campos`\n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_campos = pd.read_sql_query(sa.text(qr_structure), conn)\n</code></pre> <pre><code>df_campos_codigo = pd.merge(df_campos,df_tablas_codigos, how ='right', on = ['NombreBaseDeDatos', 'NombreTabla'])\n</code></pre>"},{"location":"seccion/Bloque6/#extraccion-y-transformacion-de-codigos-qid-y-parent_qid","title":"Extracci\u00f3n y transformaci\u00f3n de c\u00f3digos <code>qid</code> y <code>parent_qid</code>","text":"<p>Se crea la columna <code>qid</code> en el dataframe <code>df_campos_codigo</code>, extrayendo parte del nombre de campo dividiendo el texto por \"X\". Luego, se limpian los datos y se convierten los valores a formato <code>str</code>. Adem\u00e1s, se genera la columna <code>parent_qid</code> tomando los primeros d\u00edgitos del c\u00f3digo <code>qid</code>, mientras que <code>qid</code> contiene los \u00faltimos cinco caracteres.</p> <pre><code>df_campos_codigo['qid'] = df_campos_codigo['NombreCampo'].str.split(\"X\",expand = True)[2]\ndf_campos_codigo_qid = df_campos_codigo.dropna().copy().reset_index(drop =True)\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid[\"qid\"].astype(str)\ndf_campos_codigo_qid[\"parent_qid\"] = df_campos_codigo_qid['qid'].str[:-5]\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid['qid'].str[-5:]\n</code></pre> <pre><code>#df_campos_codigo_qid.to_excel('t1.xlsx',index=False)\n</code></pre> <pre><code>#df_campos_codigo_qid['qid'].str[-5:]\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-de-la-tabla-lime_questions","title":"Consulta de la tabla <code>lime_questions</code>","text":"<p>Se ejecuta una consulta SQL para obtener los datos de la tabla <code>lime_questions</code> en la base de datos <code>encuestas</code>. La consulta selecciona las columnas <code>qid</code>, <code>parent_qid</code>, <code>sid</code>, y <code>title</code>. Posteriormente, las columnas <code>qid</code>, <code>parent_qid</code>, y <code>sid</code> se convierten al tipo <code>str</code> para asegurar una correcta manipulaci\u00f3n de datos. El resultado de la consulta se almacena en el dataframe <code>df_lime_questions</code>.</p> <pre><code>##4. Consultar la tabla lime_questions\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT qid, parent_qid, sid,title FROM encuestas.lime_questions \"\"\"\n    df_lime_questions = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions[\"qid\"] = df_lime_questions[\"qid\"].astype(str)\ndf_lime_questions[\"parent_qid\"] = df_lime_questions[\"parent_qid\"].astype(str)\ndf_lime_questions[\"sid\"] = df_lime_questions[\"sid\"].astype(str)\ndf_lime_questions\n</code></pre> qid parent_qid sid title 0 216 186 832591 SQ004 1 217 186 832591 SQ005 2 218 186 832591 SQ006 3 219 186 832591 SQ007 4 220 186 832591 SQ008 ... ... ... ... ... 1475 2690 0 398684 G01Q12 1476 2691 0 123984 Q00 1477 2692 0 123984 Q01 1478 2693 0 123984 Q02 1479 2694 0 123984 Q04 <p>1480 rows \u00d7 4 columns</p>"},{"location":"seccion/Bloque6/#ajuste-de-qid-para-las-tablas","title":"Ajuste de <code>qid</code> para las tablas","text":"<p>Se realiza un proceso de combinaci\u00f3n entre los dataframes <code>df_campos_codigo_qid</code> y <code>df_lime_questions</code> para arreglar los campos <code>qid</code>. Se ajustan los nombres de columnas y se completan los valores faltantes de <code>qid</code> utilizando los valores de respaldo. El dataframe resultante, <code>df_campos_2</code>, contiene las columnas clave como <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, <code>NombreCampo</code>, <code>sid</code>, y el <code>qid</code> corregido.</p> <pre><code>#Arreglar qid para las tablas\ndf_campos_1 = pd.merge( df_campos_codigo_qid , df_lime_questions , how ='left', on = ['sid', 'qid'])\ndf_campos_1.rename(columns={'title': 'title_1', 'parent_qid_x':'parent_qid'}, inplace=True)\ndf_campos_1['title'] = df_campos_1['qid']\ndf_campos_2 = pd.merge(df_campos_1,df_lime_questions, how ='left', on = ['sid', 'title', 'parent_qid'])\ndf_campos_2['qid_y'].fillna(df_campos_2['qid_x'], inplace=True)\ndf_campos_2 = df_campos_2[['NombreBaseDeDatos', 'NombreTabla', 'NombreCampo', 'sid', 'qid_y']].copy()\ndf_campos_2.rename(columns={'qid_y':'qid'}, inplace=True)\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-y-limpieza-de-preguntas-de-encuestas-lime_questions_l10ns","title":"Consulta y limpieza de preguntas de encuestas (<code>lime_questions_l10ns</code>)","text":"<p>Se consulta la tabla <code>lime_question_l10ns</code> de la base de datos <code>encuestas</code> para obtener informaci\u00f3n detallada sobre las preguntas de encuestas. Se combina este resultado con el dataframe <code>df_campos_2</code>, eliminando columnas irrelevantes como <code>help</code>, <code>script</code>, y <code>language</code>. Luego, se limpian las preguntas usando la funci\u00f3n <code>limpiar_html</code> para eliminar etiquetas HTML, y se eliminan los caracteres especiales, como corchetes. Finalmente, se filtran las filas para excluir registros con <code>qid</code> no v\u00e1lidos como <code>'count'</code> y <code>'other'</code>, y el dataframe se reorganiza con los datos limpios.</p> <pre><code>##4. Consultar la tabla lime_questions q10\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT * FROM encuestas.lime_question_l10ns \"\"\"\n    df_lime_questions_names = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions_names[\"qid\"] = df_lime_questions_names[\"qid\"].astype(str)\ndf_sol = pd.merge(df_campos_2,df_lime_questions_names, how='left', on='qid')\ndf_sol = df_sol.drop(['help', 'script', 'language'], axis=1)\ndf_sol['question'] = df_sol['question'].astype(str)\ndf_sol['question'] = df_sol['question'].str.strip()\ndf_sol['question_clean'] = df_sol['question'].apply(limpiar_html)\ndf_sol['question_clean'] = df_sol['question_clean'].str.replace(r'[\\[\\]]', '', regex=True)\ndf_clean = df_sol[~df_sol['qid'].isin(['count', 'other'])].copy().reset_index(drop =True)\n</code></pre> <pre><code>C:\\Users\\GESTION GEAM\\AppData\\Local\\Temp\\ipykernel_29464\\1878959599.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(texto_html, 'html.parser')\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-de-grupos-de-encuestas-lime_groups","title":"Consulta de grupos de encuestas (<code>lime_groups</code>)","text":"<p>Se ejecuta una consulta SQL para obtener los grupos de encuestas desde la tabla <code>lime_groups</code> y su descripci\u00f3n desde la tabla <code>lime_group_l10ns</code>. La consulta selecciona el <code>gid</code>, <code>sid</code>, y el nombre del grupo (<code>group_name</code>) como <code>DetalleTabla</code>. El resultado se almacena en el dataframe <code>df_lime_groups</code>, asegurando que la columna <code>sid</code> est\u00e9 en formato <code>str</code>.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT e1.gid, e1.sid, e2.group_name as DetalleTabla FROM encuestas.lime_groups as e1\n                    LEFT JOIN encuestas.lime_group_l10ns as e2\n                    ON e1.gid= e2.gid\"\"\"\n    df_lime_groups = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_groups['sid'] = df_lime_groups['sid'].astype(str)\n</code></pre> <pre><code>df_clean = pd.merge( df_clean , df_lime_groups , how='left', on='sid')\n</code></pre> <pre><code>df_clean\n</code></pre> NombreBaseDeDatos NombreTabla NombreCampo sid qid id question question_clean gid DetalleTabla 0 encuestas lime_survey_124282 124282X29X488 124282 488 488.0 &lt;p style=\"text-align: center;\"&gt;&lt;span style=\"fo... PLANCHA N\u00b01 \\n\\n\\n\\nPRINCIPALES\\n\\n\\n\\n\\n\\n\\n\\... 29 REPRESENTANTES 1 encuestas lime_survey_161825 161825X51X1103 161825 1103 1103.0 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 2 encuestas lime_survey_161825 161825X51X1104 161825 1104 1104.0 \u00a1SU OPINION NOS INTERESA! Si desea registrar u... \u00a1SU OPINION NOS INTERESA! Si desea registrar u... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 3 encuestas lime_survey_161825 161825X51X1105SQ001 161825 1113 1113.0 LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 4 encuestas lime_survey_161825 161825X51X1105SQ002 161825 1114 1114.0 LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO ... ... ... ... ... ... ... ... ... ... ... 915 encuestas lime_survey_995137 995137X42X861SQ007 995137 877 877.0 E-mail: E-mail: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 916 encuestas lime_survey_995137 995137X42X862 995137 862 862.0 ESTADO LLAMADA: ESTADO LLAMADA: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 917 encuestas lime_survey_995137 995137X42X863 995137 863 863.0 A\u00d1O: A\u00d1O: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 918 encuestas lime_survey_995137 995137X42X864 995137 864 864.0 SEMESTRE: SEMESTRE: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 919 encuestas lime_survey_995137 995137X42X865 995137 865 865.0 MES: MES: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE <p>920 rows \u00d7 10 columns</p> <pre><code>#df_sol.to_excel('df_sol.xlsx',index=False)\n#df_clean.to_excel('df_clean.xlsx',index=False)\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-de-respuestas-de-encuestas-lime_answers","title":"Consulta de respuestas de encuestas (<code>lime_answers</code>)","text":"<p>Se ejecuta una consulta SQL para obtener las respuestas de encuestas desde la tabla <code>lime_answers</code> y sus descripciones desde la tabla <code>lime_answer_l10ns</code>. La consulta selecciona el <code>aid</code>, <code>qid</code>, <code>code</code>, y la respuesta (<code>answer</code>). El resultado de la consulta se almacena en el dataframe <code>df_lime_answers</code>, asegurando que las columnas <code>qid</code> y <code>aid</code> est\u00e9n en formato <code>str</code> para facilitar su manipulaci\u00f3n.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT a1.aid, a1.qid, a1.code, a2.answer \n                    FROM encuestas.lime_answers as a1\n                    LEFT JOIN encuestas.lime_answer_l10ns as a2\n                    ON a1.aid = a2.aid\"\"\" \n\n    df_lime_answers = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_answers['qid'] = df_lime_answers['qid'].astype(str)\ndf_lime_answers['aid'] = df_lime_answers['aid'].astype(str)\n</code></pre> <pre><code>tables_survey_names = df_clean['NombreTabla'].unique().tolist()\n</code></pre>"},{"location":"seccion/Bloque6/#procesamiento-y-transformacion-de-encuestas","title":"Procesamiento y transformaci\u00f3n de encuestas","text":"<p>Se ejecuta un ciclo que procesa las tablas de encuestas almacenadas en <code>tables_survey_names</code>. Para cada tabla, se cargan los datos y se identifican las columnas relacionadas con preguntas mediante una combinaci\u00f3n con <code>df_clean</code>. Luego, se ajustan las respuestas utilizando la tabla <code>df_lime_answers</code> para reemplazar c\u00f3digos por respuestas reales y se renombra cada columna con su respectiva pregunta limpia.</p> <p>Dependiendo de la estructura de la tabla, se reestructura utilizando la funci\u00f3n <code>melt</code> para tener una representaci\u00f3n de preguntas y respuestas en formato largo. Se verifican las columnas relevantes, como las relacionadas con identificaci\u00f3n, para mantener la coherencia de los datos y las tablas resultantes se almacenan en archivos CSV si no cumplen con ciertos criterios. Las tablas procesadas se agregan a la lista <code>tablesToConcat</code> para su posterior concatenaci\u00f3n.</p> <p>El proceso tambi\u00e9n incluye la limpieza de columnas como <code>documento</code> y <code>tipo_documento</code>, que se renombran para unificar la estructura entre todas las tablas de encuestas.</p> <pre><code>df_survey = dict()\ncolumnsStatic = ['C\u00e9dula:','NIT:','Tipo Documento de Identidad','N\u00famero Documento de Identidad','Tipo Documento Identidad', 'Numero de identificaci\u00f3n']\ncolumnsId = ['C\u00e9dula:', 'N\u00famero Documento de Identidad', 'Numero de identificaci\u00f3n' ]\ncolumnsTipoId = ['Tipo Documento de Identidad', 'Tipo Documento Identidad' ]\ntablesToConcat = []\n\nfor table_name in tables_survey_names:\n    print(table_name)\n    with motor_neith.begin() as conn:\n        qr_structure = \"\"\"SELECT * FROM encuestas. \"\"\" + table_name\n        df_survey[table_name] = pd.read_sql_query(sa.text(qr_structure), conn)\n\n    dfColumns = pd.DataFrame({'NombreCampo':df_survey[table_name].columns.tolist()})\n    dfColumns_p2 = pd.merge( dfColumns , df_clean , how = 'left' , on = ['NombreCampo'] )\n    dfColumns_p2 = dfColumns_p2[ ~dfColumns_p2['NombreTabla'].isna() ]\n    dfColumns_p2 = dfColumns_p2[['NombreCampo','NombreTabla','sid','qid','question_clean']]\n    dfToValidateAnswer = pd.merge( dfColumns_p2 , df_lime_answers , how = 'left' , on = ['qid'] )\n    dfToValidateAnswer = dfToValidateAnswer[~dfToValidateAnswer['aid'].isna()]\n    columnsTVA = dfToValidateAnswer['NombreCampo'].unique().tolist()\n    df_survey[table_name] = df_survey[table_name][['submitdate'] + dfColumns_p2['NombreCampo'].unique().tolist()] \n    for col in df_survey[table_name].columns.tolist():\n        if col in columnsTVA:\n            df_survey[table_name][col + '_answer'] = pd.merge( df_survey[table_name][[col]] , dfToValidateAnswer[ dfToValidateAnswer['NombreCampo'] == col ] , how = 'left' , left_on = col, right_on = 'code' )['answer']\n            df_survey[table_name] = df_survey[table_name].drop([col], axis=1)\n            df_survey[table_name] = df_survey[table_name].rename({ col + '_answer': col }, axis=1)\n        newName = dfColumns_p2[ dfColumns_p2['NombreCampo'] == col ]['question_clean'].tolist()\n\n        if len(newName) != 0:\n            df_survey[table_name] = df_survey[table_name].rename({ col: newName[0]  }, axis=1)\n\n    columnsToMantain = [x for x in df_survey[table_name].columns.tolist() if x in columnsStatic]\n    df_survey[table_name].fillna(value=np.nan, inplace=True)\n    try:\n        if len(columnsToMantain) == 0:\n            print('No aplica', os.getcwd() +'\\\\' +table_name +'.csv')\n            df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n        elif 'FECHA:' in df_survey[table_name].columns.tolist():\n            print('tiene fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\",'FECHA:'] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        else:\n            print('sin fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\"] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        for colClean in columnsToMantain:\n            df_survey[table_name][colClean] = df_survey[table_name][colClean].astype(str).apply(lambda x: x.replace(\"\\t\" , \"\" ))\n\n    except:\n        print('posible vacio')\n        df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n\n\n    df_survey[table_name] = df_survey[table_name].replace('nan', np.nan)    \n    df_survey[table_name].dropna( subset = columnsToMantain , inplace=True)\n    df_survey[table_name]['DetalleEncuesta'] = df_clean[df_clean['NombreTabla'] == table_name]['DetalleTabla'].tolist()[0] \n\n    for col in columnsId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'documento' }, axis=1)\n    for col in columnsTipoId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'tipo_documento' }, axis=1)\n</code></pre> <pre><code>lime_survey_124282\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_124282.csv\nlime_survey_161825\ntiene fecha\nlime_survey_189858\ntiene fecha\nlime_survey_267495\ntiene fecha\nlime_survey_385698\nsin fecha\nlime_survey_413658\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_413658.csv\nlime_survey_452416\ntiene fecha\nlime_survey_478847\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_478847.csv\nlime_survey_548818\ntiene fecha\nlime_survey_567215\nsin fecha\nlime_survey_576377\ntiene fecha\nlime_survey_586872\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_586872.csv\nlime_survey_599513\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_599513.csv\nlime_survey_699399\nsin fecha\nposible vacio\nlime_survey_757259\nsin fecha\nlime_survey_767954\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_767954.csv\nlime_survey_773674\nNo aplica h:\\.shortcut-targets-by-id\\1mkTxMHmUmOqVynakPkgZMLxqRsPiIptC\\Bloques Bodega\\Bloque6\\lime_survey_773674.csv\nlime_survey_818378\nsin fecha\nlime_survey_832591\nsin fecha\nlime_survey_934423\ntiene fecha\nlime_survey_959511\ntiene fecha\nlime_survey_995137\ntiene fecha\n</code></pre>"},{"location":"seccion/Bloque6/#concatenacion-final-de-encuestas","title":"Concatenaci\u00f3n final de encuestas","text":"<p>Se filtran las tablas procesadas en <code>df_survey</code> que est\u00e1n presentes en la lista <code>tablesToConcat</code> y se almacenan en el diccionario <code>df_survey_2</code>. Luego, se concatenan todas las tablas seleccionadas en un \u00fanico dataframe <code>df_survey_final</code>, combinando los resultados y asegurando un \u00edndice continuo.</p> <pre><code>df_survey_2 = dict( filter(lambda item: item[0] in tablesToConcat, df_survey.items()) )\ndf_survey_final =  pd.concat( list(df_survey_2.values()) , ignore_index = True )\n</code></pre>"},{"location":"seccion/Bloque6/#consulta-y-combinacion-de-datos-de-afiliados","title":"Consulta y combinaci\u00f3n de datos de afiliados","text":"<p>Se realiza una consulta a la tabla <code>BD_Dim_Datos_Fijos</code> en la base DWH para obtener los campos <code>DOCUMENTO</code> y <code>CODDOC</code>. Luego, se hace un merge entre los datos de encuestas (<code>df_survey_final</code>) y la informaci\u00f3n obtenida de la tabla, para complementar los documentos y tipos de documento. Posteriormente, se eliminan las columnas innecesarias y se genera una nueva columna <code>ID_AFILIADO</code> concatenando <code>tipo_documento</code> y <code>documento</code>. Finalmente, se reordenan las columnas colocando <code>ID_AFILIADO</code> al inicio.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion3 = Conexion_dwh()\nmotor3 = create_engine(cadena_conexion3)\nlogger.info('CONEXION A BASE DWH')\n\n\n# Bloque 1: Consulta y carga de datos\nlogger.info(\"Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\")\nquery = \"SELECT DOCUMENTO, CODDOC FROM BD_Dim_Datos_Fijos\"\ndf_datos_fijos = pd.read_sql(query, con=motor3)\nlogger.info(f\"Consulta a BD_Dim_Datos_Fijos completada: {df_datos_fijos.shape[0]} filas cargadas.\")\n\n# Bloque 2: Merge y limpieza de columnas\nlogger.info(\"Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\")\ndf_survey_final = pd.merge(df_survey_final, df_datos_fijos, how='left', left_on='documento', right_on='DOCUMENTO')\ndf_survey_final = df_survey_final.drop(columns=['DOCUMENTO'])\ndf_survey_final['tipo_documento'] = df_survey_final['CODDOC'].fillna('CC')\ndf_survey_final = df_survey_final.drop(columns=['CODDOC'])\n\n# Bloque 3: Generaci\u00f3n de ID y reorganizaci\u00f3n de columnas\nlogger.info(\"Generando 'ID_AFILIADO' y reorganizando columnas.\")\ndf_survey_final['ID_AFILIADO'] = df_survey_final['tipo_documento'].astype(str) + df_survey_final['documento'].astype(str)\n\n# Renombrar columnas\ndf_survey_final = df_survey_final.rename(columns={\n    'submitdate': 'FECHA_ENCUESTA',\n    'documento': 'ID_DOCUMENTO',\n    'Pregunta': 'PREGUNTA',\n    'Respuesta': 'RESPUESTA',\n    'DetalleEncuesta': 'DETALLE_ENCUESTA',\n    'NIT:': 'NIT',\n    'tipo_documento': 'TIPO_DOCUMENTO',\n})\n\n# Reordenar las columnas colocando 'ID_AFILIADO' en la primera posici\u00f3n\ncolumnas = ['ID_AFILIADO'] + [col for col in df_survey_final.columns if col != 'ID']\ndf_survey_final = df_survey_final.reindex(columns=columnas)\n\n# Mostrar las columnas finales\nlogger.info(f\"Proceso completado. Columnas finales: {df_survey_final.columns.tolist()}\")\n</code></pre> <pre><code>2024-10-12 11:42:57,543 - INFO - CONEXION A BASE DWH\n2024-10-12 11:42:57,548 - INFO - Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\n2024-10-12 11:43:06,213 - INFO - Consulta a BD_Dim_Datos_Fijos completada: 975320 filas cargadas.\n2024-10-12 11:43:06,215 - INFO - Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\n2024-10-12 11:43:06,763 - INFO - Generando 'ID_AFILIADO' y reorganizando columnas.\n2024-10-12 11:43:06,916 - INFO - Proceso completado. Columnas finales: ['ID_AFILIADO', 'FECHA_ENCUESTA', 'FECHA:', 'ID_DOCUMENTO', 'PREGUNTA', 'RESPUESTA', 'DETALLE_ENCUESTA', 'NIT', 'TIPO_DOCUMENTO', 'ID_AFILIADO']\n</code></pre>"},{"location":"seccion/Bloque6/#conexion-a-la-base-de-datos-dwh_1","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion2 = Conexion_dwh()\nmotor2 = create_engine(cadena_conexion2)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-12 11:33:41,951 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/Bloque6/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>df_survey_final</code> en la tabla <code>BD_Fact_Encuestas</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>#Guardar en base dwh\nalmacenamiento_time = time.time()\ndf_survey_final.to_sql(name='BD_Fact_Encuestas', con=motor2, if_exists = 'replace', index=False)\nlogging.info(f'ALMACENAMIENTO --- {time.time() - almacenamiento_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-12 11:34:07,244 - INFO - ALMACENAMIENTO --- 25.28 seconds ---\n</code></pre>"},{"location":"seccion/IntroBodega1%20copy/","title":"Explicaci\u00f3n del Proceso ETL: Bloque1","text":"<p>Este documento tiene como objetivo explicar en detalle las diferentes partes del c\u00f3digo del archivo Bloque1, que representa un proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga). Este proceso ETL se encarga de gestionar los datos de afiliados, aplicando las transformaciones y preparaciones necesarias para su an\u00e1lisis posterior. Cada una de estas etapas del proceso es clave para garantizar la calidad y la integridad de los datos, asegurando as\u00ed su utilidad para los an\u00e1lisis empresariales y la toma de decisiones. A continuaci\u00f3n, se detalla cada secci\u00f3n del c\u00f3digo, explicando sus funcionalidades y la l\u00f3gica detr\u00e1s de cada paso.</p>"},{"location":"seccion/IntroBodega1%20copy/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de Librer\u00edas y Funciones","text":"<p>La primera parte del c\u00f3digo se centra en importar las librer\u00edas necesarias para realizar las operaciones requeridas en el proceso ETL. Las librer\u00edas importadas son las siguientes:</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport os\nimport logging\n</code></pre> <p>Estas librer\u00edas cumplen las siguientes funciones:</p> <ul> <li><code>sqlalchemy</code>: Se utiliza para conectarse y manipular bases de datos SQL. Esto es esencial para establecer conexiones confiables y realizar operaciones eficientes sobre las bases de datos involucradas en el proceso.</li> <li><code>pandas</code>: Librer\u00eda esencial para la manipulaci\u00f3n y transformaci\u00f3n de datos. <code>pandas</code> permite trabajar con estructuras de datos como DataFrames, que son fundamentales para llevar a cabo las operaciones de limpieza y transformaci\u00f3n de la informaci\u00f3n.</li> <li><code>numpy</code>: Se emplea para realizar operaciones matem\u00e1ticas y optimizaciones. <code>numpy</code> resulta muy \u00fatil para manejar c\u00e1lculos complejos, que suelen ser necesarios cuando se trabaja con grandes vol\u00famenes de datos.</li> <li><code>time</code>: Para monitorear el tiempo de ejecuci\u00f3n y controlar el rendimiento. Esto permite medir el tiempo que toma cada etapa del proceso ETL y as\u00ed optimizar la eficiencia del script.</li> <li><code>os</code>: Para interactuar con el sistema operativo (por ejemplo, manejar rutas de archivos y variables de entorno). Esto permite una mayor flexibilidad al manejar diferentes entornos de ejecuci\u00f3n.</li> <li><code>logging</code>: Para registrar eventos y errores durante la ejecuci\u00f3n del script. El uso de <code>logging</code> asegura que cualquier problema durante la ejecuci\u00f3n del proceso sea debidamente registrado, lo cual facilita la depuraci\u00f3n y mejora continua del proceso.</li> </ul> <p>Adem\u00e1s, se importa un m\u00f3dulo personalizado llamado <code>Funciones.e</code>, que contiene funciones espec\u00edficas desarrolladas para este proceso. Este m\u00f3dulo facilita la reutilizaci\u00f3n de c\u00f3digo y permite mantener el script principal m\u00e1s limpio y f\u00e1cil de entender.</p>"},{"location":"seccion/IntroBodega1%20copy/#creacion-de-conexiones-y-configuraciones-iniciales","title":"Creaci\u00f3n de Conexiones y Configuraciones Iniciales","text":"<p>El c\u00f3digo tambi\u00e9n define las configuraciones necesarias para conectarse a las bases de datos que se utilizar\u00e1n en el proceso. La funci\u00f3n <code>create_engine</code> de <code>sqlalchemy</code> se utiliza para crear una conexi\u00f3n con la base de datos, lo cual permite realizar consultas y operaciones sobre la misma.</p> <p>Ejemplo:</p> <pre><code>engine = create_engine('mysql+pymysql://user:password@host/dbname')\n</code></pre> <p>Esta l\u00ednea establece una conexi\u00f3n a una base de datos MySQL, proporcionando las credenciales necesarias. Esto es esencial para garantizar que la extracci\u00f3n de datos se realice de manera adecuada. Adem\u00e1s, el uso de <code>create_engine</code> permite mantener una conexi\u00f3n eficiente y confiable, lo cual es crucial cuando se trabaja con grandes vol\u00famenes de datos y se requiere una interacci\u00f3n frecuente con la base de datos.</p>"},{"location":"seccion/IntroBodega1%20copy/#extraccion-de-datos","title":"Extracci\u00f3n de Datos","text":"<p>La extracci\u00f3n de datos se lleva a cabo utilizando <code>pandas</code> para ejecutar consultas SQL a trav\u00e9s de la conexi\u00f3n establecida con la base de datos. El uso de <code>pd.read_sql()</code> permite extraer tablas completas o resultados de consultas espec\u00edficas, convirti\u00e9ndolos en DataFrames para su posterior manipulaci\u00f3n.</p> <p>Ejemplo:</p> <pre><code>df_afiliados = pd.read_sql('SELECT * FROM afiliados', con=engine)\n</code></pre> <p>En este caso, se extraen todos los registros de la tabla <code>afiliados</code> y se almacenan en un DataFrame de <code>pandas</code> para su manipulaci\u00f3n posterior. La extracci\u00f3n de datos es el primer paso en el proceso ETL, y su correcta implementaci\u00f3n es fundamental para asegurar la disponibilidad y calidad de los datos que ser\u00e1n transformados.</p> <p>Es importante destacar que, durante la extracci\u00f3n, es posible aplicar filtros o l\u00edmites para minimizar el volumen de datos y as\u00ed optimizar el rendimiento del proceso. Por ejemplo, se podr\u00edan filtrar solo aquellos registros que cumplan con ciertos criterios espec\u00edficos, lo cual ayuda a trabajar con un subconjunto m\u00e1s manejable de la informaci\u00f3n.</p>"},{"location":"seccion/IntroBodega1%20copy/#transformacion-de-datos","title":"Transformaci\u00f3n de Datos","text":"<p>Una vez extra\u00eddos los datos, se aplican diversas transformaciones para limpiar y estandarizar la informaci\u00f3n. Las transformaciones son una parte cr\u00edtica del proceso ETL, ya que aseguran que los datos sean consistentes, completos y adecuados para el an\u00e1lisis.</p> <p>Algunas de estas transformaciones incluyen:</p> <ul> <li>Eliminaci\u00f3n de valores nulos: Se identifican y eliminan los registros incompletos, o bien se imputan valores para llenar los datos faltantes. Esto es importante para evitar sesgos o errores en el an\u00e1lisis posterior.</li> <li>Estandarizaci\u00f3n de formatos: Se aplican cambios para que los datos cumplan con un formato uniforme (por ejemplo, transformar fechas a un formato est\u00e1ndar). Esto facilita el trabajo con los datos y asegura que no haya discrepancias debido a formatos diferentes.</li> <li>Generaci\u00f3n de nuevas columnas: Utilizando <code>numpy</code> y <code>pandas</code>, se crean columnas derivadas de los datos originales para facilitar el an\u00e1lisis posterior. Estas columnas derivadas pueden incluir c\u00e1lculos adicionales o combinaciones de informaci\u00f3n relevante que permitan extraer m\u00e1s valor de los datos.</li> </ul> <p>Ejemplo:</p> <pre><code>df_afiliados['nombre_completo'] = df_afiliados['nombre'] + ' ' + df_afiliados['apellido']\n</code></pre> <p>Esta l\u00ednea concatena las columnas <code>nombre</code> y <code>apellido</code> para crear una nueva columna llamada <code>nombre_completo</code>. Este tipo de transformaci\u00f3n permite trabajar de manera m\u00e1s sencilla con los datos al tener toda la informaci\u00f3n relevante en una \u00fanica columna.</p> <p>Adicionalmente, se pueden realizar transformaciones m\u00e1s complejas, como la normalizaci\u00f3n de variables num\u00e9ricas o la categorizaci\u00f3n de datos en diferentes grupos para facilitar el an\u00e1lisis. Cada transformaci\u00f3n aplicada est\u00e1 orientada a preparar los datos para que sean m\u00e1s \u00fatiles y f\u00e1ciles de interpretar en el contexto del an\u00e1lisis final.</p>"},{"location":"seccion/IntroBodega1%20copy/#carga-de-datos","title":"Carga de Datos","text":"<p>Despu\u00e9s de la transformaci\u00f3n, el proceso ETL finaliza con la carga de los datos preparados en una nueva base de datos o tabla. Esta etapa garantiza que los datos transformados est\u00e9n disponibles para su uso en sistemas posteriores o an\u00e1lisis.</p> <p>Esto se realiza utilizando la funci\u00f3n <code>to_sql()</code> de <code>pandas</code>:</p> <pre><code>df_afiliados.to_sql('afiliados_procesados', con=engine, if_exists='replace', index=False)\n</code></pre> <p>En este ejemplo, los datos transformados se cargan en una nueva tabla llamada <code>afiliados_procesados</code>. Si la tabla ya existe, ser\u00e1 reemplazada por los nuevos datos. Es importante destacar la opci\u00f3n <code>if_exists='replace'</code>, que permite asegurar que siempre se tenga la versi\u00f3n m\u00e1s reciente de los datos procesados.</p> <p>La carga de datos es una parte crucial del proceso ETL, ya que implica dejar disponibles los datos para su uso futuro. En muchos casos, los datos se cargan en un almac\u00e9n de datos centralizado, lo que facilita el acceso por parte de diferentes \u00e1reas de la empresa para sus respectivos an\u00e1lisis y reportes.</p>"},{"location":"seccion/IntroBodega1%20copy/#manejo-de-errores-y-registro-de-eventos","title":"Manejo de Errores y Registro de Eventos","text":"<p>Para asegurar la robustez del proceso, el c\u00f3digo utiliza <code>logging</code> para registrar eventos importantes y posibles errores. Esto facilita la identificaci\u00f3n de fallas y la mejora del proceso en futuras ejecuciones. Un registro detallado de los eventos tambi\u00e9n es \u00fatil para mantener un historial del rendimiento del proceso y para identificar patrones que podr\u00edan indicar problemas recurrentes.</p> <p>Ejemplo:</p> <pre><code>logging.basicConfig(filename='etl_proceso.log', level=logging.INFO)\nlogging.info('Extracci\u00f3n completada exitosamente')\n</code></pre> <p>Estas l\u00edneas configuran el registro de eventos en un archivo <code>etl_proceso.log</code> para hacer un seguimiento del progreso y registrar mensajes de \u00e9xito o error durante la ejecuci\u00f3n. El nivel de registro puede ajustarse (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>) dependiendo de la importancia del evento que se quiera registrar.</p> <p>El manejo de errores y el registro adecuado de eventos permiten que el proceso ETL sea m\u00e1s resiliente y f\u00e1cil de mantener. Ante cualquier fallo, los registros proveen la informaci\u00f3n necesaria para diagnosticar y corregir problemas, minimizando el impacto en la operaci\u00f3n.</p>"},{"location":"seccion/IntroBodega1%20copy/#diagrama-del-proceso-etl","title":"Diagrama del Proceso ETL","text":"<p>A continuaci\u00f3n se presenta un diagrama de procesos en Mermaid que ilustra cada una de las etapas del proceso ETL descrito anteriormente:</p> <pre><code>---\nconfig:\n  theme: default\n  look: neo\n  layout: elk\n---\ngraph TD\n    A[\ud83d\udce6 Importaci\u00f3n de Librer\u00edas y Funciones] --&gt; B((\ud83d\udd0c Creaci\u00f3n de Conexiones))\n    B --&gt; C([\ud83d\uddc4\ufe0f Extracci\u00f3n de Datos])\n    C --&gt; D[[\u2699\ufe0f Transformaci\u00f3n de Datos]]\n    D --&gt; E{\ud83d\udce4 Carga de Datos}\n    E --&gt; F[\ud83d\udea8 Manejo de Errores y Registro de Eventos]</code></pre> <p>Este diagrama muestra el flujo del proceso, comenzando con la importaci\u00f3n de las librer\u00edas necesarias, seguido de la creaci\u00f3n de conexiones, la extracci\u00f3n y transformaci\u00f3n de los datos, y finalmente la carga en la base de datos, con el manejo de errores como una etapa transversal que garantiza la robustez del proceso.</p>"},{"location":"seccion/IntroBodega1%20copy/#diagrama-de-entidades-relacion-er","title":"Diagrama de Entidades Relaci\u00f3n (ER)","text":"<p>Adem\u00e1s del diagrama de procesos, a continuaci\u00f3n se presenta un diagrama de entidades relaci\u00f3n en Mermaid que ilustra las relaciones entre las diferentes tablas de la base de datos involucradas en el proceso ETL:</p> <pre><code>erDiagram\n    SERVI20 {\n        int ID_SERVICIO PK\n        int ID_AFILIADO FK\n        int USOS\n        float CANTIDAD\n        float VALOR_SERVICIO\n        float VALOR_DESCUENTOS\n    }\n\n    SERVI21 {\n        int ID_SERVICIO PK\n        int ID_AFILIADO FK\n        float VALOR_TOTAL\n        date FECHA_SERVICIO\n    }\n\n    DIM_DATOS_FIJOS {\n        int ID_AFILIADO PK\n    }\n\n    SERVI20 }|..|{ DIM_DATOS_FIJOS : \"relacionado con afiliado\"\n    SERVI21 }|..|{ DIM_DATOS_FIJOS : \"relacionado con afiliado\"</code></pre> <p>Este diagrama ER muestra las relaciones entre las tablas <code>SERVI20</code>, <code>SERVI21</code> y <code>DIM_DATOS_FIJOS</code>. Las tablas <code>SERVI20</code> y <code>SERVI21</code> est\u00e1n relacionadas con la tabla <code>DIM_DATOS_FIJOS</code> a trav\u00e9s de la columna <code>ID_AFILIADO</code>, lo cual indica que ambas tablas de servicios est\u00e1n vinculadas a los datos de los afiliados. Esto ayuda a mantener la integridad de los datos y facilita la consulta de informaci\u00f3n relacionada.</p>"},{"location":"seccion/IntroBodega1%20copy/#conclusion","title":"Conclusi\u00f3n","text":"<p>Este proceso ETL de Bloque1 se encarga de extraer informaci\u00f3n de una base de datos de afiliados, transformarla para asegurar su consistencia y calidad, y cargarla en una nueva base de datos o tabla. Cada etapa est\u00e1 dise\u00f1ada cuidadosamente para garantizar que los datos sean precisos y \u00fatiles para an\u00e1lisis futuros. La documentaci\u00f3n detallada de cada paso permite al cliente comprender qu\u00e9 se hace en cada fase y c\u00f3mo se asegura la integridad de los datos.</p> <p>Adem\u00e1s, es importante resaltar que el proceso ETL es fundamental para transformar datos en informaci\u00f3n valiosa, permitiendo as\u00ed a las organizaciones tomar decisiones informadas basadas en datos de alta calidad. La automatizaci\u00f3n y el registro de eventos aseguran que el proceso sea repetible y confiable, lo cual es esencial en cualquier entorno de datos en el que la calidad y la disponibilidad de la informaci\u00f3n son cr\u00edticas para el \u00e9xito de la empresa.</p> <p>Cada etapa del proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) tiene un prop\u00f3sito claro y contribuye al objetivo global de convertir datos sin procesar en informaci\u00f3n lista para ser utilizada. El enfoque en la calidad de los datos y en la eficiencia de cada paso del proceso asegura que los resultados finales sean confiables y \u00fatiles para apoyar los objetivos estrat\u00e9gicos de la organizaci\u00f3n.</p>"},{"location":"seccion/IntroBodega1/","title":"Explicaci\u00f3n del Proceso ETL - Bloque1","text":"<p>Este documento proporciona una explicaci\u00f3n detallada del proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga de datos) realizado en el archivo Bloque1. El flujo se encuentra dividido en distintas sesiones:</p> <ul> <li>1.2 - EstandarAfiliados2.0</li> <li>1.3 - CalendarioMensual</li> <li>1.4 - Dimensiones</li> <li>1.5 - FactAfiliacion</li> <li>1.6 - FactDatosContacto</li> </ul> <p>Cada una de estas sesiones representa una etapa clave del procesamiento de datos, proporcionando una estructura clara para garantizar la eficiencia y la consistencia de la informaci\u00f3n tratada. A continuaci\u00f3n, se explica cada uno de los componentes.</p>"},{"location":"seccion/IntroBodega1/#12-estandarafiliados20","title":"1.2 - EstandarAfiliados2.0","text":""},{"location":"seccion/IntroBodega1/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este m\u00f3dulo se encarga de la estandarizaci\u00f3n de los datos de los afiliados. La estandarizaci\u00f3n incluye la limpieza de datos, la transformaci\u00f3n a formatos consistentes y la validaci\u00f3n para asegurar la calidad de los datos antes de su integraci\u00f3n en la base de datos.</p>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<p>Los datos se extraen de fuentes externas como archivos CSV o tablas SQL. Posteriormente, se transforman aplicando reglas de negocio para garantizar la coherencia. Finalmente, se cargan en una tabla de afiliados estandarizada.</p>"},{"location":"seccion/IntroBodega1/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<p>A continuaci\u00f3n, se presentan algunos fragmentos de c\u00f3digo que son fundamentales para la extracci\u00f3n, transformaci\u00f3n y carga de datos en esta sesi\u00f3n:</p> <pre><code>import pandas as pd\nimport sqlalchemy as sa\n\n# Creaci\u00f3n de la conexi\u00f3n con la base de datos\nengine = sa.create_engine('postgresql://usuario:password@host:puerto/basedatos')\n\n# Extracci\u00f3n de datos desde CSV\ndf_afiliados = pd.read_csv('afiliados.csv')\n\n# Limpieza y estandarizaci\u00f3n de datos\ndf_afiliados['fecha_nacimiento'] = pd.to_datetime(df_afiliados['fecha_nacimiento'], errors='coerce')\ndf_afiliados['nombre'] = df_afiliados['nombre'].str.upper()\n\n# Carga de datos a la base de datos\ndf_afiliados.to_sql('afiliados_estandarizados', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>---\nconfig:\n  theme: default\n  look: neo\n  layout: elk\n---\n\nflowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos]\n    D --&gt; E[Transformaci\u00f3n de Datos]\n    E --&gt; F[Carga de Datos]\n    F --&gt; G[Validaci\u00f3n y Manejo de Errores]\n    G --&gt; H[Fin]</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-er","title":"Diagrama ER","text":"<pre><code>---\nconfig:\n  theme: default\n  look: neo\n  layout: elk\n---\nerDiagram\n    Afiliados ||--o{ EstandarAfiliados : \"estandariza\"</code></pre>"},{"location":"seccion/IntroBodega1/#manejo-de-errores","title":"Manejo de Errores","text":"<p>Se configuran mecanismos de registro y manejo de errores para garantizar que los problemas encontrados durante el proceso sean registrados correctamente y el proceso pueda ser depurado en el futuro.</p>"},{"location":"seccion/IntroBodega1/#13-calendariomensual","title":"1.3 - CalendarioMensual","text":""},{"location":"seccion/IntroBodega1/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El m\u00f3dulo CalendarioMensual se encarga de definir y gestionar las actividades ETL mensuales, asegurando que cada proceso de carga se realice en el momento adecuado. Esto permite una planificaci\u00f3n eficiente del flujo de datos.</p>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<p>Se extraen datos de diferentes per\u00edodos para organizarlos en un calendario mensual. Las fechas se utilizan para definir la planificaci\u00f3n de la carga, facilitando an\u00e1lisis temporales consistentes.</p>"},{"location":"seccion/IntroBodega1/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<p>A continuaci\u00f3n, se muestran fragmentos de c\u00f3digo para la creaci\u00f3n y gesti\u00f3n del calendario mensual:</p> <pre><code>import pandas as pd\nimport datetime\n\n# Definir el rango de fechas para el calendario mensual\nfecha_inicio = '2023-01-01'\nfecha_fin = '2023-12-31'\n\n# Creaci\u00f3n del DataFrame del calendario\ndf_calendario = pd.date_range(start=fecha_inicio, end=fecha_fin, freq='M').to_frame(index=False, name='fecha')\n\n# A\u00f1adir columnas adicionales\ndf_calendario['mes'] = df_calendario['fecha'].dt.month\ndf_calendario['a\u00f1o'] = df_calendario['fecha'].dt.year\n\nprint(df_calendario)\n</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Configurar Calendario]\n    D --&gt; E[Extracci\u00f3n de Datos]\n    E --&gt; F[Organizaci\u00f3n por Fechas]\n    F --&gt; G[Carga de Datos]\n    G --&gt; H[Validaci\u00f3n y Manejo de Errores]\n    H --&gt; I[Fin]</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    Calendario ||--o{ FactAfiliacion : \"planifica\"</code></pre>"},{"location":"seccion/IntroBodega1/#manejo-de-errores_1","title":"Manejo de Errores","text":"<p>Se verifica que los datos sean consistentes en el calendario y se asegura la integridad de los mismos antes de la carga final.</p>"},{"location":"seccion/IntroBodega1/#14-dimensiones","title":"1.4 - Dimensiones","text":""},{"location":"seccion/IntroBodega1/#explicacion-del-proceso-etl_2","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este m\u00f3dulo se encarga de definir y gestionar las dimensiones del modelo de datos, tales como afiliados, productos, o cualquier otra entidad relevante. Las dimensiones son tablas descriptivas que proporcionan contexto a los datos transaccionales, enriqueciendo el an\u00e1lisis.</p>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga","text":"<p>Los datos se extraen, se procesan para definir dimensiones consistentes, y luego se cargan para ser utilizados en consultas y an\u00e1lisis posteriores.</p>"},{"location":"seccion/IntroBodega1/#fragmentos-de-codigo-importantes_2","title":"Fragmentos de C\u00f3digo Importantes","text":"<p>A continuaci\u00f3n, se presentan algunos fragmentos de c\u00f3digo relevantes para la creaci\u00f3n de las dimensiones:</p> <pre><code>import pandas as pd\nimport sqlalchemy as sa\n\n# Creaci\u00f3n de la conexi\u00f3n con la base de datos\nengine = sa.create_engine('postgresql://usuario:password@host:puerto/basedatos')\n\n# Extracci\u00f3n de datos desde la tabla de afiliados\ndf_afiliados = pd.read_sql('SELECT * FROM afiliados', con=engine)\n\n# Creaci\u00f3n de la dimensi\u00f3n afiliados\ndf_dim_afiliados = df_afiliados[['afiliado_id', 'nombre', 'edad', 'genero']]\n\n# Carga de la dimensi\u00f3n a la base de datos\ndf_dim_afiliados.to_sql('dim_afiliados', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-proceso_2","title":"Diagrama de Proceso","text":"<pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos]\n    D --&gt; E[Transformaci\u00f3n en Dimensiones]\n    E --&gt; F[Carga de Dimensiones]\n    F --&gt; G[Fin]</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-er_2","title":"Diagrama ER","text":"<pre><code>erDiagram\n    Dimensiones ||--o{ FactAfiliacion : \"describe\"</code></pre>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga","text":"<p>Los datos son extra\u00eddos, transformados en dimensiones bien definidas, y luego cargados para ser utilizados en el modelo dimensional.</p>"},{"location":"seccion/IntroBodega1/#15-factafiliacion","title":"1.5 - FactAfiliacion","text":""},{"location":"seccion/IntroBodega1/#explicacion-del-proceso-etl_3","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El m\u00f3dulo FactAfiliacion crea una tabla de hechos que almacena los datos transaccionales relacionados con la afiliaci\u00f3n. Esto permite realizar an\u00e1lisis hist\u00f3ricos sobre las afiliaciones, tales como el comportamiento de los afiliados a lo largo del tiempo.</p>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga-de-datos_2","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<p>Se extraen los datos de afiliaci\u00f3n, se transforman para adaptarse al esquema de la tabla de hechos y se cargan en la base de datos para an\u00e1lisis y generaci\u00f3n de reportes.</p>"},{"location":"seccion/IntroBodega1/#fragmentos-de-codigo-importantes_3","title":"Fragmentos de C\u00f3digo Importantes","text":"<p>A continuaci\u00f3n, algunos fragmentos de c\u00f3digo utilizados para la creaci\u00f3n de la tabla de hechos de afiliaci\u00f3n:</p> <pre><code>import pandas as pd\nimport sqlalchemy as sa\n\n# Creaci\u00f3n de la conexi\u00f3n con la base de datos\nengine = sa.create_engine('postgresql://usuario:password@host:puerto/basedatos')\n\n# Extracci\u00f3n de datos transaccionales\ndf_transacciones = pd.read_sql('SELECT * FROM transacciones_afiliacion', con=engine)\n\n# Transformaci\u00f3n de datos para adaptarse al esquema de hechos\ndf_fact_afiliacion = df_transacciones[['afiliado_id', 'fecha_afiliacion', 'tipo_afiliacion']]\ndf_fact_afiliacion['mes'] = pd.to_datetime(df_fact_afiliacion['fecha_afiliacion']).dt.month\n\n# Carga de la tabla de hechos\ndf_fact_afiliacion.to_sql('fact_afiliacion', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-proceso_3","title":"Diagrama de Proceso","text":"<pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos]\n    D --&gt; E[Transformaci\u00f3n de Datos]\n    E --&gt; F[Carga en Tabla de Hechos]\n    F --&gt; G[Validaci\u00f3n y Manejo de Errores]\n    G --&gt; H[Fin]</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-er_3","title":"Diagrama ER","text":"<pre><code>erDiagram\n    Afiliados ||--o{ FactAfiliacion : \"contiene\"\n    Calendario ||--o{ FactAfiliacion : \"establece\"\n    Dimensiones ||--o{ FactAfiliacion : \"describe\"</code></pre>"},{"location":"seccion/IntroBodega1/#extraccion-transformacion-y-carga-de-datos_3","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<p>Los datos se transforman para cumplir con el esquema de una tabla de hechos y se cargan en la base de datos para su posterior consulta.</p>"},{"location":"seccion/IntroBodega1/#16-factdatoscontacto","title":"1.6 - FactDatosContacto","text":""},{"location":"seccion/IntroBodega1/#explicacion-del-proceso-etl_4","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El m\u00f3dulo FactDatosContacto gestiona una tabla de hechos que contiene informaci\u00f3n sobre los datos de contacto de los afiliados. Estos datos incluyen tel\u00e9fonos, correos electr\u00f3nicos y direcciones, fundamentales para la comunicaci\u00f3n y seguimiento de los afiliados.</p>"},{"location":"seccion/IntroBodega1/#transformacion-y-carga","title":"Transformaci\u00f3n y Carga","text":"<p>Los datos se extraen, se limpian y se transforman para estandarizar la informaci\u00f3n de contacto. Luego, se cargan en la base de datos para que puedan ser utilizados en an\u00e1lisis y comunicaciones.</p>"},{"location":"seccion/IntroBodega1/#fragmentos-de-codigo-importantes_4","title":"Fragmentos de C\u00f3digo Importantes","text":"<p>A continuaci\u00f3n, se presentan algunos fragmentos de c\u00f3digo utilizados para manejar los datos de contacto:</p> <pre><code>import pandas as pd\nimport sqlalchemy as sa\n\n# Creaci\u00f3n de la conexi\u00f3n con la base de datos\nengine = sa.create_engine('postgresql://usuario:password@host:puerto/basedatos')\n\n# Extracci\u00f3n de datos de contacto\ndf_contacto = pd.read_csv('datos_contacto.csv')\n\n# Limpieza y estandarizaci\u00f3n de los datos de contacto\ndf_contacto['telefono'] = df_contacto['telefono'].str.replace('-', '').str.strip()\ndf_contacto['email'] = df_contacto['email'].str.lower()\n\n# Carga de los datos de contacto estandarizados\ndf_contacto.to_sql('fact_datos_contacto', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-proceso_4","title":"Diagrama de Proceso","text":"<pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos de Contacto]\n    D --&gt; E[Transformaci\u00f3n de Datos]\n    E --&gt; F[Carga de Datos]\n    F --&gt; G[Validaci\u00f3n y Manejo de Errores]\n    G --&gt; H[Fin]</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-er_4","title":"Diagrama ER","text":"<pre><code>erDiagram\n    Afiliados ||--o{ FactDatosContacto : \"contiene\"</code></pre>"},{"location":"seccion/IntroBodega1/#transformacion-y-carga_1","title":"Transformaci\u00f3n y Carga","text":"<p>La transformaci\u00f3n est\u00e1 orientada a estandarizar la informaci\u00f3n de contacto para garantizar la calidad y consistencia de los datos, mejorando su utilizaci\u00f3n en an\u00e1lisis y comunicaciones.</p>"},{"location":"seccion/IntroBodega1/#diagramas-de-cada-sesion-por-separado","title":"Diagramas de cada sesi\u00f3n por separado","text":""},{"location":"seccion/IntroBodega1/#diagrama-de-entidad-relacion-er","title":"Diagrama de Entidad-Relaci\u00f3n (ER)","text":"<p>El diagrama ER muestra la relaci\u00f3n entre las diferentes tablas utilizadas a lo largo del proceso ETL. Incluye las tablas de dimensiones y las tablas de hechos (FactAfiliacion y FactDatosContacto), as\u00ed como las relaciones entre estas.</p> <pre><code>erDiagram\n    Afiliados ||--o{ FactAfiliacion : \"contiene\"\n    Afiliados ||--o{ FactDatosContacto : \"contiene\"\n    Calendario ||--o{ FactAfiliacion : \"establece\"\n    Dimensiones ||--o{ FactAfiliacion : \"describe\"</code></pre>"},{"location":"seccion/IntroBodega1/#diagrama-de-procesos","title":"Diagrama de Procesos","text":"<p>El siguiente diagrama muestra las etapas principales del proceso ETL desde la extracci\u00f3n, pasando por la transformaci\u00f3n hasta la carga final en la base de datos.</p> <pre><code>flowchart TD\n    A[Inicio] --&gt; B[Importar Librer\u00edas]\n    B --&gt; C[Crear Conexiones]\n    C --&gt; D[Extracci\u00f3n de Datos]\n    D --&gt; E[Transformaci\u00f3n de Datos]\n    E --&gt; F[Carga de Datos]\n    F --&gt; G[Validaci\u00f3n y Manejo de Errores]\n    G --&gt; H[Fin]</code></pre> <p>Este documento proporciona una estructura clara y completa para explicar cada uno de los componentes del proceso ETL, permitiendo as\u00ed una mejor comprensi\u00f3n del flujo de datos para los clientes interesados.</p>"},{"location":"seccion/IntroBodega2/","title":"Explicaci\u00f3n del Proceso ETL - Bloque 2","text":""},{"location":"seccion/IntroBodega2/#21-factaportes","title":"2.1 - FactAportes","text":""},{"location":"seccion/IntroBodega2/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso tiene como objetivo gestionar los aportes de diferentes fuentes y preparar los datos para su posterior an\u00e1lisis y carga. Se asegura de estandarizar y transformar la informaci\u00f3n sobre los aportes para su correcta integraci\u00f3n dentro del flujo de datos principal.</p>"},{"location":"seccion/IntroBodega2/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de una base de datos relacional utilizando la librer\u00eda <code>sqlalchemy</code>.</li> <li>Transformaci\u00f3n: Se realizan varias transformaciones en los datos usando <code>pandas</code> y <code>numpy</code>, como limpieza de datos nulos y estandarizaci\u00f3n de formatos de fecha.</li> <li>Carga: Los datos transformados se cargan en la base de datos destino a trav\u00e9s de una conexi\u00f3n establecida con <code>sqlalchemy</code>.</li> </ul>"},{"location":"seccion/IntroBodega2/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport time\nfrom dateutil import parser\n\n# Creaci\u00f3n del motor de conexi\u00f3n\nengine = create_engine('mysql+pymysql://usuario:contrase\u00f1a@localhost/db_name')\n\n# Extracci\u00f3n de datos\nquery = 'SELECT * FROM aportes'\ndf = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf['fecha'] = df['fecha'].apply(parser.parse)\ndf.fillna(0, inplace=True)\n\n# Carga de datos\ndf.to_sql('aportes_limpios', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Datos] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos];</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    APORTES {\n        int id\n        string nombre\n        date fecha\n        float monto\n    }\n    APORTES ||--o| USUARIO : \"pertenece a\"\n    USUARIO {\n        int id\n        string nombre\n    }</code></pre>"},{"location":"seccion/IntroBodega2/#22-factentrega","title":"2.2 - FactEntrega","text":""},{"location":"seccion/IntroBodega2/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso gestiona la entrega de bienes o servicios y su registro en el sistema, asegurando que la informaci\u00f3n est\u00e9 consolidada y lista para el an\u00e1lisis.</p>"},{"location":"seccion/IntroBodega2/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se obtienen de un sistema de registros utilizando <code>sqlalchemy</code>.</li> <li>Transformaci\u00f3n: Se normalizan las columnas, se corrigen errores y se estandarizan los valores de las fechas y montos.</li> <li>Carga: Los datos se almacenan en una tabla preparada para auditor\u00edas.</li> </ul>"},{"location":"seccion/IntroBodega2/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos\nquery = 'SELECT * FROM entregas'\ndf_entregas = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_entregas['fecha_entrega'] = pd.to_datetime(df_entregas['fecha_entrega'])\ndf_entregas['monto'] = df_entregas['monto'].astype(float)\n\n# Carga de datos\ndf_entregas.to_sql('entregas_auditoria', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Entregas] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga para Auditor\u00eda];</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    ENTREGAS {\n        int id\n        date fecha_entrega\n        float monto\n    }\n    ENTREGAS ||--o| CLIENTE : \"entregado a\"\n    CLIENTE {\n        int id\n        string nombre\n    }</code></pre>"},{"location":"seccion/IntroBodega2/#23-factsubsidiovivienda","title":"2.3 - FactSubsidioVivienda","text":""},{"location":"seccion/IntroBodega2/#explicacion-del-proceso-etl_2","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso gestiona la informaci\u00f3n relacionada con los subsidios de vivienda, asegurando que se registren correctamente y est\u00e9n listos para an\u00e1lisis de pol\u00edticas sociales.</p>"},{"location":"seccion/IntroBodega2/#extraccion-transformacion-y-carga-de-datos_2","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos de subsidios se obtienen de diversas fuentes de registro.</li> <li>Transformaci\u00f3n: Se realizan ajustes para la estandarizaci\u00f3n de montos y normalizaci\u00f3n de los beneficiarios.</li> <li>Carga: Los datos se cargan en una base de datos destinada al an\u00e1lisis de impacto social.</li> </ul>"},{"location":"seccion/IntroBodega2/#fragmentos-de-codigo-importantes_2","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos\nquery = 'SELECT * FROM subsidios'\ndf_subsidios = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_subsidios['monto_subsidio'] = df_subsidios['monto_subsidio'].apply(lambda x: round(x, 2))\n\n# Carga de datos\ndf_subsidios.to_sql('subsidios_procesados', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-de-proceso_2","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Subsidios] --&gt; B[Transformaci\u00f3n de Montos];\n    B --&gt; C[Carga en Base de Datos];</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-er_2","title":"Diagrama ER","text":"<pre><code>erDiagram\n    SUBSIDIOS {\n        int id\n        string beneficiario\n        float monto_subsidio\n    }\n    SUBSIDIOS ||--o| BENEFICIARIO : \"otorgado a\"\n    BENEFICIARIO {\n        int id\n        string nombre\n    }</code></pre>"},{"location":"seccion/IntroBodega2/#24-crearfactfosfec","title":"2.4 - CrearFactFosfec","text":""},{"location":"seccion/IntroBodega2/#explicacion-del-proceso-etl_3","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El proceso de creaci\u00f3n de FactFosfec est\u00e1 orientado a consolidar las contribuciones y deducciones relacionadas con el fondo de solidaridad FOSFEC, asegurando una correcta trazabilidad.</p>"},{"location":"seccion/IntroBodega2/#extraccion-transformacion-y-carga-de-datos_3","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de bases internas y externas que aportan informaci\u00f3n sobre contribuciones.</li> <li>Transformaci\u00f3n: Se realizan verificaciones cruzadas para evitar duplicados y se calculan las deducciones necesarias.</li> <li>Carga: Los datos finales se cargan en una tabla espec\u00edfica para su an\u00e1lisis financiero.</li> </ul>"},{"location":"seccion/IntroBodega2/#fragmentos-de-codigo-importantes_3","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos\nquery = 'SELECT * FROM fosfec_contribuciones'\ndf_fosfec = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_fosfec.drop_duplicates(subset='id', keep='first', inplace=True)\n\n# Carga de datos\ndf_fosfec.to_sql('fosfec_final', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-de-proceso_3","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Contribuciones] --&gt; B[Transformaci\u00f3n y Verificaci\u00f3n];\n    B --&gt; C[Carga Final para An\u00e1lisis];</code></pre>"},{"location":"seccion/IntroBodega2/#diagrama-er_3","title":"Diagrama ER","text":"<pre><code>erDiagram\n    FOSFEC {\n        int id\n        string contribuyente\n        float monto_contribucion\n    }\n    FOSFEC ||--o| CONTRIBUYENTE : \"contribuci\u00f3n de\"\n    CONTRIBUYENTE {\n        int id\n        string nombre\n    }</code></pre>"},{"location":"seccion/IntroBodega3/","title":"Explicaci\u00f3n del Proceso ETL - FactTransaccionesVentas","text":""},{"location":"seccion/IntroBodega3/#31-facttransaccionesventas","title":"3.1 - FactTransaccionesVentas","text":""},{"location":"seccion/IntroBodega3/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El objetivo de este proceso es gestionar y procesar las transacciones de ventas realizadas, asegurando que los datos se registren adecuadamente y est\u00e9n listos para an\u00e1lisis futuros. Este proceso se encarga de consolidar la informaci\u00f3n de ventas desde m\u00faltiples fuentes, estandarizar formatos, y cargar los datos transformados en la base de datos principal para facilitar el an\u00e1lisis.</p>"},{"location":"seccion/IntroBodega3/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen desde una base de datos de ventas, utilizando la librer\u00eda <code>sqlalchemy</code> para conectarse y realizar consultas SQL que permiten obtener la informaci\u00f3n relevante de transacciones.</li> <li>Transformaci\u00f3n: Se utilizan librer\u00edas como <code>pandas</code> y <code>numpy</code> para limpiar y transformar los datos. Esto incluye la eliminaci\u00f3n de duplicados, el ajuste de los tipos de datos y la estandarizaci\u00f3n de valores monetarios y fechas.</li> <li>Carga: Los datos se cargan en una base de datos de destino, utilizando el motor de <code>sqlalchemy</code> para realizar la carga de manera eficiente y asegurar la disponibilidad de los datos para futuros an\u00e1lisis.</li> </ul>"},{"location":"seccion/IntroBodega3/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser\n\n# Creaci\u00f3n del motor de conexi\u00f3n a la base de datos\nengine = create_engine('mysql+pymysql://usuario:contrase\u00f1a@localhost/db_ventas')\n\n# Extracci\u00f3n de datos\nquery = 'SELECT * FROM transacciones_ventas'\ndf_ventas = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_ventas['fecha_venta'] = pd.to_datetime(df_ventas['fecha_venta'])\ndf_ventas['monto'] = df_ventas['monto'].astype(float)\ndf_ventas.drop_duplicates(subset='id_transaccion', keep='first', inplace=True)\n\n# Carga de datos transformados\ndf_ventas.to_sql('transacciones_procesadas', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega3/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Datos de Ventas] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos Transformados];</code></pre>"},{"location":"seccion/IntroBodega3/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    TRANSACCIONES_VENTAS {\n        int id_transaccion\n        date fecha_venta\n        float monto\n        string cliente_id\n    }\n    TRANSACCIONES_VENTAS ||--o| CLIENTE : \"realizada por\"\n    CLIENTE {\n        int cliente_id\n        string nombre_cliente\n        string email\n    }</code></pre>"},{"location":"seccion/IntroBodega3/#32-factresumenventas","title":"3.2 - FactResumenVentas","text":""},{"location":"seccion/IntroBodega3/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso est\u00e1 destinado a generar un resumen de las ventas realizadas, consolidando los datos por per\u00edodos de tiempo y categor\u00edas de producto, permitiendo un an\u00e1lisis m\u00e1s general de la evoluci\u00f3n de las ventas y el comportamiento del mercado.</p>"},{"location":"seccion/IntroBodega3/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Se extraen los datos agregados desde la tabla de transacciones ya procesadas para generar un resumen mensual.</li> <li>Transformaci\u00f3n: Se agrupan los datos por mes y se calculan m\u00e9tricas como el total de ventas y el n\u00famero de transacciones realizadas.</li> <li>Carga: El resumen de ventas se carga en una nueva tabla para reportes.</li> </ul>"},{"location":"seccion/IntroBodega3/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos procesados\nquery = 'SELECT * FROM transacciones_procesadas'\ndf_resumen = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_resumen['mes'] = df_resumen['fecha_venta'].dt.to_period('M')\ndf_agrupado = df_resumen.groupby('mes').agg(\n    total_ventas=('monto', 'sum'),\n    num_transacciones=('id_transaccion', 'count')\n).reset_index()\n\n# Carga del resumen de ventas\ndf_agrupado.to_sql('resumen_ventas', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega3/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Transacciones Procesadas] --&gt; B[Agregaci\u00f3n y Resumen de Datos];\n    B --&gt; C[Carga en Tabla de Resumen de Ventas];</code></pre>"},{"location":"seccion/IntroBodega3/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    RESUMEN_VENTAS {\n        string mes\n        float total_ventas\n        int num_transacciones\n    }\n    RESUMEN_VENTAS ||--o| TRANSACCIONES_VENTAS : \"resumido de\"\n    TRANSACCIONES_VENTAS {\n        int id_transaccion\n        date fecha_venta\n        float monto\n    }</code></pre>"},{"location":"seccion/IntroBodega4/","title":"Explicaci\u00f3n del Proceso ETL - Bloque 4","text":""},{"location":"seccion/IntroBodega4/#41-factcolegio","title":"4.1 - FactColegio","text":""},{"location":"seccion/IntroBodega4/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso tiene como objetivo procesar la informaci\u00f3n relacionada con los registros de colegios, garantizando que los datos se extraigan, transformen y carguen adecuadamente para facilitar el an\u00e1lisis de la gesti\u00f3n escolar. Se enfoca en estandarizar los registros de estudiantes y colegios para su posterior an\u00e1lisis.</p>"},{"location":"seccion/IntroBodega4/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de una base de datos que contiene la informaci\u00f3n de colegios y estudiantes, utilizando la librer\u00eda <code>sqlalchemy</code> para conectarse y realizar consultas SQL que obtengan los registros pertinentes.</li> <li>Transformaci\u00f3n: Se utilizan <code>pandas</code> y <code>numpy</code> para realizar diversas tareas, como la limpieza de datos nulos, la normalizaci\u00f3n de los nombres de colegios, y el ajuste de formatos de fechas.</li> <li>Carga: Los datos transformados se cargan en una base de datos destino, garantizando la integridad de los registros para futuros an\u00e1lisis de gesti\u00f3n.</li> </ul>"},{"location":"seccion/IntroBodega4/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser\n\n# Creaci\u00f3n del motor de conexi\u00f3n a la base de datos\nengine = create_engine('mysql+pymysql://usuario:contrase\u00f1a@localhost/db_colegios')\n\n# Extracci\u00f3n de datos\nquery = 'SELECT * FROM registros_colegios'\ndf_colegios = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_colegios['fecha_registro'] = pd.to_datetime(df_colegios['fecha_registro'])\ndf_colegios['nombre_colegio'] = df_colegios['nombre_colegio'].str.upper()\ndf_colegios.fillna({'telefono': 'No Disponible'}, inplace=True)\n\n# Carga de datos transformados\ndf_colegios.to_sql('colegios_procesados', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega4/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Datos de Colegios] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos Transformados];</code></pre>"},{"location":"seccion/IntroBodega4/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    COLEGIOS {\n        int id_colegio\n        string nombre_colegio\n        date fecha_registro\n        string telefono\n    }\n    COLEGIOS ||--o| ESTUDIANTE : \"registrado en\"\n    ESTUDIANTE {\n        int id_estudiante\n        string nombre_estudiante\n        date fecha_nacimiento\n    }</code></pre>"},{"location":"seccion/IntroBodega4/#42-factasistencia","title":"4.2 - FactAsistencia","text":""},{"location":"seccion/IntroBodega4/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso tiene como prop\u00f3sito gestionar y consolidar la informaci\u00f3n de asistencia de los estudiantes, asegurando que los datos se procesen adecuadamente para an\u00e1lisis de asistencia y deserci\u00f3n escolar.</p>"},{"location":"seccion/IntroBodega4/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Se obtienen los datos de asistencia desde registros digitales utilizando <code>sqlalchemy</code> para acceder a la base de datos y extraer la informaci\u00f3n necesaria.</li> <li>Transformaci\u00f3n: Se utilizan <code>pandas</code> para normalizar los registros de asistencia, eliminar duplicados y convertir las fechas de asistencia a un formato est\u00e1ndar.</li> <li>Carga: Los datos procesados se almacenan en una tabla espec\u00edfica para la generaci\u00f3n de reportes de asistencia.</li> </ul>"},{"location":"seccion/IntroBodega4/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos de asistencia\nquery = 'SELECT * FROM registros_asistencia'\ndf_asistencia = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_asistencia['fecha_asistencia'] = pd.to_datetime(df_asistencia['fecha_asistencia'])\ndf_asistencia.drop_duplicates(subset=['id_estudiante', 'fecha_asistencia'], keep='first', inplace=True)\n\n# Carga de datos transformados\ndf_asistencia.to_sql('asistencia_procesada', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega4/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Registros de Asistencia] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos Transformados];</code></pre>"},{"location":"seccion/IntroBodega4/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    ASISTENCIA {\n        int id_asistencia\n        int id_estudiante\n        date fecha_asistencia\n    }\n    ASISTENCIA ||--o| ESTUDIANTE : \"asistencia de\"\n    ESTUDIANTE {\n        int id_estudiante\n        string nombre_estudiante\n        date fecha_nacimiento\n    }</code></pre>"},{"location":"seccion/IntroBodega5/","title":"Explicaci\u00f3n del Proceso ETL - Bloque 5","text":""},{"location":"seccion/IntroBodega5/#51-factserviciosocial","title":"5.1 - FactServicioSocial","text":""},{"location":"seccion/IntroBodega5/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>El objetivo de este proceso es gestionar la informaci\u00f3n relacionada con los servicios sociales prestados, asegurando que los registros se extraigan, transformen y carguen adecuadamente para facilitar el an\u00e1lisis de la gesti\u00f3n del servicio social y su impacto. Este proceso se centra en la estandarizaci\u00f3n de datos de los beneficiarios del servicio social y el monitoreo de la prestaci\u00f3n de dichos servicios.</p>"},{"location":"seccion/IntroBodega5/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de una base de datos de registros de servicios sociales, utilizando <code>sqlalchemy</code> para conectarse y realizar consultas SQL que permitan obtener la informaci\u00f3n relevante.</li> <li>Transformaci\u00f3n: Se utilizan <code>pandas</code> y <code>numpy</code> para realizar transformaciones en los datos, tales como la normalizaci\u00f3n de nombres de beneficiarios, el ajuste de los tipos de datos, y la eliminaci\u00f3n de registros duplicados.</li> <li>Carga: Los datos transformados se cargan en una base de datos destino, garantizando su integridad y disponibilidad para futuros an\u00e1lisis.</li> </ul>"},{"location":"seccion/IntroBodega5/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser\n\n# Creaci\u00f3n del motor de conexi\u00f3n a la base de datos\nengine = create_engine('mysql+pymysql://usuario:contrase\u00f1a@localhost/db_servicios_sociales')\n\n# Extracci\u00f3n de datos\nquery = 'SELECT * FROM registros_servicio_social'\ndf_servicio_social = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_servicio_social['fecha_inicio'] = pd.to_datetime(df_servicio_social['fecha_inicio'])\ndf_servicio_social['nombre_beneficiario'] = df_servicio_social['nombre_beneficiario'].str.upper()\ndf_servicio_social.drop_duplicates(subset='id_servicio', keep='first', inplace=True)\n\n# Carga de datos transformados\ndf_servicio_social.to_sql('servicio_social_procesado', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega5/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Datos de Servicio Social] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos Transformados];</code></pre>"},{"location":"seccion/IntroBodega5/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    SERVICIO_SOCIAL {\n        int id_servicio\n        string nombre_beneficiario\n        date fecha_inicio\n        string tipo_servicio\n    }\n    SERVICIO_SOCIAL ||--o| BENEFICIARIO : \"prestado a\"\n    BENEFICIARIO {\n        int id_beneficiario\n        string nombre_beneficiario\n        string direccion\n    }</code></pre>"},{"location":"seccion/IntroBodega5/#52-factimpactoserviciosocial","title":"5.2 - FactImpactoServicioSocial","text":""},{"location":"seccion/IntroBodega5/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso est\u00e1 enfocado en evaluar el impacto del servicio social prestado, recopilando y procesando informaci\u00f3n sobre los beneficiarios y el alcance de los servicios. Se consolidan los datos para permitir el an\u00e1lisis del impacto en la comunidad y la efectividad del servicio social.</p>"},{"location":"seccion/IntroBodega5/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de la tabla de servicios sociales ya procesados, con el objetivo de calcular m\u00e9tricas de impacto.</li> <li>Transformaci\u00f3n: Se realiza una agrupaci\u00f3n de los datos por tipo de servicio y se calculan m\u00e9tricas como el n\u00famero total de beneficiarios y la duraci\u00f3n promedio del servicio.</li> <li>Carga: Los datos transformados se cargan en una tabla espec\u00edfica para an\u00e1lisis y reportes de impacto.</li> </ul>"},{"location":"seccion/IntroBodega5/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos procesados\nquery = 'SELECT * FROM servicio_social_procesado'\ndf_impacto = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_impacto['duracion'] = (pd.to_datetime(df_impacto['fecha_fin']) - df_impacto['fecha_inicio']).dt.days\ndf_agrupado = df_impacto.groupby('tipo_servicio').agg(\n    num_beneficiarios=('id_beneficiario', 'count'),\n    duracion_promedio=('duracion', 'mean')\n).reset_index()\n\n# Carga de los datos de impacto\ndf_agrupado.to_sql('impacto_servicio_social', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega5/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Servicio Social Procesado] --&gt; B[Transformaci\u00f3n y C\u00e1lculo de Impacto];\n    B --&gt; C[Carga en Tabla de Impacto de Servicio Social];</code></pre>"},{"location":"seccion/IntroBodega5/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    IMPACTO_SERVICIO_SOCIAL {\n        string tipo_servicio\n        int num_beneficiarios\n        float duracion_promedio\n    }\n    IMPACTO_SERVICIO_SOCIAL ||--o| SERVICIO_SOCIAL : \"evaluaci\u00f3n de\"\n    SERVICIO_SOCIAL {\n        int id_servicio\n        string tipo_servicio\n        date fecha_inicio\n    }</code></pre>"},{"location":"seccion/IntroBodega6/","title":"Explicaci\u00f3n del Proceso ETL - Bloque 6","text":""},{"location":"seccion/IntroBodega6/#61-mainencuestasv1","title":"6.1 - MainEncuestasV1","text":""},{"location":"seccion/IntroBodega6/#explicacion-del-proceso-etl","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso se encarga de gestionar la informaci\u00f3n relacionada con encuestas realizadas a diferentes grupos, con el prop\u00f3sito de consolidar los datos para su posterior an\u00e1lisis estad\u00edstico y evaluaci\u00f3n de tendencias. A trav\u00e9s de este proceso, se asegura la correcta extracci\u00f3n, transformaci\u00f3n y carga de los datos de encuestas, garantizando la calidad de los datos y su disponibilidad para an\u00e1lisis posteriores.</p>"},{"location":"seccion/IntroBodega6/#extraccion-transformacion-y-carga-de-datos","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Los datos se extraen de una base de datos de encuestas utilizando <code>sqlalchemy</code> para conectarse y realizar consultas SQL que permitan obtener los registros relevantes.</li> <li>Transformaci\u00f3n: Se utilizan <code>pandas</code> para realizar diversas tareas de transformaci\u00f3n, como la limpieza de valores nulos, la estandarizaci\u00f3n de respuestas, y la conversi\u00f3n de formatos de fecha.</li> <li>Carga: Los datos transformados se cargan en una base de datos destino para ser utilizados en an\u00e1lisis y reportes.</li> </ul>"},{"location":"seccion/IntroBodega6/#fragmentos-de-codigo-importantes","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser\n\n# Creaci\u00f3n del motor de conexi\u00f3n a la base de datos\nengine = create_engine('mysql+pymysql://usuario:contrase\u00f1a@localhost/db_encuestas')\n\n# Extracci\u00f3n de datos\nquery = 'SELECT * FROM encuestas_v1'\ndf_encuestas = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_encuestas['fecha_encuesta'] = pd.to_datetime(df_encuestas['fecha_encuesta'])\ndf_encuestas.fillna({'respuesta': 'No Respuesta'}, inplace=True)\ndf_encuestas['respuesta'] = df_encuestas['respuesta'].str.upper()\n\n# Carga de datos transformados\ndf_encuestas.to_sql('encuestas_procesadas', con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega6/#diagrama-de-proceso","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Datos de Encuestas] --&gt; B[Transformaci\u00f3n de Datos];\n    B --&gt; C[Carga de Datos Transformados];</code></pre>"},{"location":"seccion/IntroBodega6/#diagrama-er","title":"Diagrama ER","text":"<pre><code>erDiagram\n    ENCUESTAS {\n        int id_encuesta\n        string respuesta\n        date fecha_encuesta\n        string grupo_objetivo\n    }\n    ENCUESTAS ||--o| GRUPO : \"realizada a\"\n    GRUPO {\n        int id_grupo\n        string nombre_grupo\n    }</code></pre>"},{"location":"seccion/IntroBodega6/#62-resultadosencuestas","title":"6.2 - ResultadosEncuestas","text":""},{"location":"seccion/IntroBodega6/#explicacion-del-proceso-etl_1","title":"Explicaci\u00f3n del Proceso ETL","text":"<p>Este proceso tiene como objetivo consolidar los resultados de las encuestas para generar reportes y an\u00e1lisis que ayuden a identificar tendencias y preferencias de los diferentes grupos encuestados. La finalidad es asegurar que los datos sean consistentes y \u00fatiles para la toma de decisiones.</p>"},{"location":"seccion/IntroBodega6/#extraccion-transformacion-y-carga-de-datos_1","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos","text":"<ul> <li>Extracci\u00f3n: Se extraen los datos ya procesados desde la tabla de encuestas para generar un resumen de los resultados.</li> <li>Transformaci\u00f3n: Se agrupan las respuestas por grupo objetivo y se calculan m\u00e9tricas como la cantidad de respuestas y la distribuci\u00f3n porcentual de cada tipo de respuesta.</li> <li>Carga: Los datos consolidados se cargan en una nueva tabla para reportes de resultados y an\u00e1lisis de tendencias.</li> </ul>"},{"location":"seccion/IntroBodega6/#fragmentos-de-codigo-importantes_1","title":"Fragmentos de C\u00f3digo Importantes","text":"<pre><code># Extracci\u00f3n de datos procesados\nquery = 'SELECT * FROM encuestas_procesadas'\ndf_resultados = pd.read_sql(query, engine)\n\n# Transformaci\u00f3n de datos\ndf_agrupado = df_resultados.groupby(['grupo_objetivo', 'respuesta']).size().reset_index(name='cantidad_respuestas')\ndf_agrupado['porcentaje'] = (df_agrupado['cantidad_respuestas'] / df_agrupado.groupby('grupo_objetivo')['cantidad_respuestas'].transform('sum')) * 100\n\n# Carga de resultados\nresultados_tabla = 'resultados_encuestas'\ndf_agrupado.to_sql(resultados_tabla, con=engine, if_exists='replace', index=False)\n</code></pre>"},{"location":"seccion/IntroBodega6/#diagrama-de-proceso_1","title":"Diagrama de Proceso","text":"<pre><code>graph TD;\n    A[Extracci\u00f3n de Encuestas Procesadas] --&gt; B[Agrupaci\u00f3n y C\u00e1lculo de M\u00e9tricas];\n    B --&gt; C[Carga de Resultados Consolidados];</code></pre>"},{"location":"seccion/IntroBodega6/#diagrama-er_1","title":"Diagrama ER","text":"<pre><code>erDiagram\n    RESULTADOS_ENCUESTAS {\n        string grupo_objetivo\n        string respuesta\n        int cantidad_respuestas\n        float porcentaje\n    }\n    RESULTADOS_ENCUESTAS ||--o| ENCUESTAS : \"resumen de\"\n    ENCUESTAS {\n        int id_encuesta\n        string respuesta\n        date fecha_encuesta\n    }</code></pre>"}]}